---
title: "Variational inference for approximate reference priors using neural networks"
subtitle: ""
author:
  - name: Nils Baillie
    corresponding: true
date: last-modified
date-modified: last-modified
abstract: >+
  I
keywords: [Reference priors, Variational inference, Neural networks]
citation:
  type: article-journal
  container-title: "Computo"
  doi: "xxxx"
  url: https://computo.sfds.asso.fr/template-computo-quarto
  publisher: "Société Française de Statistique"
  issn: "2824-7795"
bibliography: references.bib
github-user: computorg
repo: "Computo_VA-RP"
draft: true # set to false once the build is running
published: false # will be set to true once accepted
format:
  computo-html: default
  computo-pdf: default
jupyter: python3
---


::: {.hidden}
\def\bX{\mathbf{X}}
\newcommand{\rD}{\mathrm{D}}
\newcommand{\aseq}[2][]{\overset{#1}{\underset{#2}{=}}}
\newcommand{\conv}[2][\ ]{\overset{#1}{\underset{#2}{\to}}}

::: 

```{python python-code}
import os
import sys
# base_dir = r'C:\Users\AV273338\Documents\GitHub\Computo_VA-RP'  # the path 
base_dir = r'/Users/antoinevanbiesbroeck/Documents/GitHub/Nils/Computo_VA-RP'
# {must be set on VA-RP
py_dir = os.path.join(base_dir, "python_files")
if py_dir not in sys.path:
    sys.path.append(py_dir)
#print(sys.path)
from aux_optimizers import *
from stat_models_torch import *
from neural_nets import *
from variational_approx import *
from div_metrics_torch import *
from constraints import *

# Packages used
import scipy.special as spc
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt
from matplotlib import rc
from matplotlib import gridspec
import pickle

rc('text', usetex=False) 
rc('lines', linewidth=2)
rougeCEA = "#b81420"
vertCEA = "#73be4b"

path_plot = os.path.join(base_dir, "plots_data")
probit_path = os.path.join(base_dir, "data_probit")
tirages_path = os.path.join(probit_path, "tirages_probit")
int_jeffreys_path = os.path.join(probit_path, "Multiseed_AJ")
varp_probit_path = os.path.join(probit_path, "Multiseed_VARP")

### Load parameter for each main computation
# If True, loads the different results without re-doing every computation
# If False, all computations will be done and saved (and possibly overwrite the data files !)


load_results_Probit_nocstr = False
load_results_Probit_cstr = False


```


## Probit model  {#sec-probit_model}

```{python python-code}

a_tab = np.linspace(10**-3, 15, num=100)

frag_curve = lambda a, theta : 1/2+1/2*spc.erf(np.log(a[np.newaxis]/theta[:,0,np.newaxis])/theta[:,1,np.newaxis])
theta_vrai = np.array([3.37610525, 0.43304097])
ref = 1/2+1/2*spc.erf(np.log(a_tab/theta_vrai[0])/theta_vrai[1])


def plot_frag_cred(theta_post, ax, color, label=None) :
    curves = frag_curve(a_tab, theta_post)
    q1 = np.quantile(curves, 1-0.05/2, axis=0)
    q2 = np.quantile(curves, 0.05/2, axis=0)
    # med = np.median(curves, axis=0)
    ax.fill_between(a_tab, q1, q2, color=color, alpha=0.6)
    if not label is None :
        ax.plot(a_tab, q1, color=color, alpha=0.6, label=label)
    # ax.plot(a_tab, med)
    ax.legend()
    return ax



```

```{python python-code}
#Parameters and classes
p = 50   
q = 2      
N = 50     
J = 500   
T = 50     
input_size = p  
output_size = q
low = 0.0001        
upp = 1 + low
mu_a, sigma2_a = 0, 1
#mu_a, sigma2_a = 8.7 * 10**-3, 1.03
Probit = torch_ProbitModel(use_log_normal=True, mu_a=mu_a, sigma2_a=sigma2_a, set_beta=None, alt_scaling=True)
n_samples_prior = 10**6
alpha = 0.5
name_file = 'Probit_results_unconstrained.pkl'
file_path = os.path.join(path_plot, name_file)

seed_all(0)
sqrt2 = np.sqrt(2)
act_bet = lambda x: torch.log(1- 0.5*(1+torch.erf(x/sqrt2)) )
NN = NetProbitSeparate(input_size, m1=0, s1=0.5, b1=0, pre_act=[nn.Identity(), act_bet], act1=[torch.exp, torch.exp])
# NN = DifferentActivations(NN, [torch.exp, nn.Softplus()])
# NN = AffineTransformation(NN, low, upp)
VA = VA_NeuralNet(neural_net=NN, model=Probit)
#print(f'Number of parameters : {VA.nb_param}')
Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=alpha, use_log_lik=True)
Div.penalize_norm_param=0
Div.save_params = True

with torch.no_grad():
    Thetas_ = Div.va.implicit_prior_sampler(n_samples_prior)
    theta_sample_init = Thetas_.numpy()
    Thetas_ = delete_nan_elements(Thetas_[-100:])
    # MI_list = np.array([Div.MI(theta, J, N).item() for theta in Thetas_])
    MI_list = Div.MI(Thetas_, J, N).numpy()
    MI_list = remove_nan_and_inf(MI_list)
    MI_current = np.mean(MI_list)

num_epochs = 7000
loss_fct = "LB_MI"
optimizer = torch_Adam
num_samples_MI = 10
freq_MI = 10
save_best_param = False
learning_rate = 0.001

if not load_results_Probit_nocstr : 
    MI, range_MI, lower_MI, upper_MI = Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, num_samples_MI, 
                                                            freq_MI, save_best_param, learning_rate,num_samples_grad=40, momentum=True)
    all_params = torch.cat([param.view(-1) for param in NN.parameters()])

    # with torch.no_grad() :
    #     NN.netbeta.singl.fc1.weight[0,0] += -NN.netbeta.singl.fc1.weight[0,0] +1/0.1
    #     NN.netbeta.singl.fc1.weight[0,1] += -NN.netbeta.singl.fc1.weight[0,1] +4
    
    seed_all(0)
    theta_sample = Div.va.implicit_prior_sampler(n_samples_prior)
    with torch.no_grad():
        theta_sample_prior = theta_sample.numpy()

    with open(os.path.join(tirages_path, 'tirages_data'), 'rb') as file:
        data = pickle.load(file)
    data_A, data_Z = data[0], data[1]
    theta_true = np.array([3.37610525, 0.43304097])
    N = 50  # non-degenerate
    i = 2

    seed_all(0)
    Xstack = np.stack((data_Z[:N, i],data_A[:N, i]),axis=1)
    X = torch.tensor(Xstack)
    D = X.unsqueeze(1)
    Probit.data = D
    n_samples_post = 5000
    T_mcmc = 10**5 + 1 
    sigma2_0 = torch.tensor(1.)
    eps_0 = torch.randn(p)
    #eps_0 = 10 * torch.ones(p)
    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept=0.4, adap=True, Cov=True, disable_tqdm=False)
    theta_MH = NN(eps_MH)
    with torch.no_grad():
        theta_MH = theta_MH.detach().numpy()
    theta_post_nocstr = theta_MH[-n_samples_post:,-n_samples_post:]
    
    # Saves all relevant quantities
    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI, 'weights':Div.keep_params}
    Prior = {'samples' : theta_sample_prior, 'jeffreys' : None, 'params' : all_params}
    Posterior = {'samples' : theta_post_nocstr, 'jeffreys' : None}
    Probit_results_nocstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}
    with open(file_path, 'wb') as file:
        pickle.dump(Probit_results_nocstr, file)


    torch.save(NN.state_dict(),   os.path.join(path_plot, 'Probit_results_unconstrained_torch_state'))
    
else :
    with open(file_path, 'rb') as file:
        res = pickle.load(file)
        MI_dic = res['MI']
        Prior = res['prior'] 
        Posterior = res['post']
        MI, range_MI, lower_MI, upper_MI = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper']
        theta_sample_prior, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']
        theta_post_nocstr, jeffreys_post = Posterior['samples'], Posterior['jeffreys']    
        weights_hist = MI_dic['weights']

with torch.no_grad():
    mult_w2 = NN.netbeta.singl.fc1.weight.detach().numpy()
w2_neg = 1/mult_w2[mult_w2<0]
w2_pos = 1/mult_w2[mult_w2>0]
print('multiplicative weights on beta:')
print('neagtives belong to [{},{}]'.format(w2_neg.min(), w2_neg.max()))
print('positives belong to [{},{}]'.format(w2_pos.min(), w2_pos.max()))

```

```{python stem-plot}
#| label: fig-probit_mi
#| fig-cap:  "Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 100 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. The learning rate used in the optimization is $0.001$."

plt.figure(figsize=(4, 3))
plt.plot(range_MI, MI, '-', color=rougeCEA)
plt.plot(range_MI, MI, '*', color=rougeCEA)
# for i in range(len(range_MI)):
#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')
plt.plot(range_MI, upper_MI, '-', color='lightgrey')
plt.plot(range_MI, lower_MI, '-', color='lightgrey')
plt.fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)
plt.xlabel(r"Epochs")
plt.ylabel(r"Generalized mutual information")
plt.grid()
#plt.yscale('log')
plt.tight_layout()
plt.show()

print('MI fistr value: {}'.format(MI_current))
# MI_list = np.concatenate( [Div.MI(torch.tensor(theta_post_nocstr[-100*(i+1):-100*(i)-1]), J, N).numpy()  for i in range(10)] ) 
with torch.no_grad():
    MI_list = Div.MI(torch.tensor(theta_post_nocstr[-100:]), J, N).numpy()
#  MI_list = remove_nan_and_inf(MI_list)
MI_last = np.nanmean(MI_list)
print('MI last value: {}'.format(MI_last))

plt.figure(figsize=(5, 4))
plt.hist(theta_sample_init[:,1], density=True, bins="rice", color="red", label=r"Initial prior in $\beta$", alpha=0.4)
plt.hist(theta_sample_prior[:,1][theta_sample_prior[:,1]<50], density=True, bins="rice", label=r"Fitted prior", alpha=0.4)
plt.legend()
plt.xlim(0,10)


target_w1 = lambda w1: np.linalg.norm(w1, axis=-1)-1
target_b1 = lambda b1: np.abs(b1)
target_w2_1 = lambda w2: np.max(1/w2 -1e10*(w2>0), axis=-1)
target_w2_2 = lambda w2: np.min(1/w2 +1e10*(w2<0), axis=-1)
plt.figure(figsize=(5, 4))
plt.plot(range_MI, target_w1(np.array(weights_hist['w1']).squeeze()), label=r'$\|w_1\|_2-1$')
plt.plot(range_MI, target_b1(weights_hist['b1']), label=r'$|b_1|$')
plt.plot(range_MI, target_w2_1(np.array(weights_hist['w2']).squeeze()), label=r'$\max(w_2^{-1}<0)$')
plt.plot(range_MI, target_w2_2(np.array(weights_hist['w2']).squeeze()), label=r'$\min(w_2^{-1}>0)$')

plt.legend()



```

In @fig-probit_mi is shown the evolution of the mutual information through the optimization of the VA-RP for the probit model. 
We perceive high mutual information values at the initialization, which we interpret as a result of the fact that the parametric prior on $\theta_1$ is already quite close to its target distribution.

### Probit constraints





```{python python-code}
kappa = 0.15
K_val = np.nanmean((theta_sample_prior[:,1]**kappa)**(1/alpha))
c_val = np.nanmean((theta_sample_prior[:,1]**kappa)**(1+1/alpha))
constr_val = 0.825
alpha_constr = np.mean((theta_sample_prior[:,0]**-kappa + 1)**-1)
print(f'Constraint value estimation : {K_val/c_val}')  # = 0.839594841003418 

# Parameters and classes
p = 50     # latent space dimension
q = 2      # parameter space dimension
N = 50    # number of data samples
J = 500    # nb samples for MC estimation in MI
T = 50     # nb samples MC marginal likelihood
alpha = 0.5
input_size = p  
output_size = q
low = 0.0001          # lower bound 
upp = 1 + low
n_samples_prior = 10**6

mu_a, sigma2_a = 0, 1
#mu_a, sigma2_a = 8.7 * 10**-3, 1.03
Probit = torch_ProbitModel(use_log_normal=True, mu_a=mu_a, sigma2_a=sigma2_a, set_beta=None, alt_scaling=True)

# Constraints
beta = torch.tensor([kappa])
b = torch.tensor([[1,constr_val]]) 
T_cstr = 10000
eta_augm = torch.tensor([[0.,1.]])
eta = torch.tensor([[0.,1.]])
name_file = 'Probit_results_constrained.pkl'
file_path = os.path.join(path_plot, name_file)

seed_all(0)
sqrt2 = np.sqrt(2)
act_bet = lambda x: torch.log(1- 0.5*(1+torch.erf(x/sqrt2)) )
NN_cst = NetProbitSeparate(input_size, m1=0, s1=0.1, b1=0, pre_act=[nn.Identity(), act_bet], act1=[torch.exp, torch.exp])
# NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Identity())
# NN = DifferentActivations(NN, [torch.exp, nn.Softplus()])
# NN = AffineTransformation(NN, low, upp)
VA = VA_NeuralNet(neural_net=NN_cst, model=Probit)
#print(f'Number of parameters : {VA.nb_param}')
Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=0.5, use_log_lik=True)
Constr = Constraints_NeuralNet(div=Div, betas=beta, b=b, T_cstr=T_cstr, 
                               objective='LB_MI', lag_method='augmented', eta_augm=eta_augm, rule='SGD')
Constr.penalize_norm_param = 0
Constr.save_params = True

with torch.no_grad():
    theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()

num_epochs = 5000
optimizer = torch_Adam
num_samples_MI = 10
freq_MI = 10
save_best_param = False
learning_rate = 0.001
freq_augm = 10

if not load_results_Probit_cstr :

    ### Training loop
    MI, constr_values, range_MI, lower_MI, upper_MI = Constr.Partial_autograd(J, N, eta, num_epochs, optimizer, num_samples_MI, 
                                                            freq_MI, save_best_param, learning_rate, 
                                                            num_samples_grad=40,
                                                            momentum=True,
                                                            freq_augm=freq_augm, max_violation=1, update_eta_augm=2.)

    constr_values = torch.stack(constr_values).numpy()
    all_params = torch.cat([param.view(-1) for param in NN_cst.parameters()])
    seed_all(0)
    with torch.no_grad():
        theta_sample = Constr.va.implicit_prior_sampler(n_samples_prior).numpy()

    print(f'Moment of order {beta.item()}, estimation : {np.mean(theta_sample**beta.item(),axis=0)}, wanted value : {b}')

    seed_all(0)
    with open(os.path.join(tirages_path,'tirages_data'), 'rb') as file:
        data = pickle.load(file)
    data_A, data_Z = data[0], data[1]
    N = 50  # non-degenerate
    i = 2

    Xstack = np.stack((data_Z[:N, i],data_A[:N, i]),axis=1)
    X = torch.tensor(Xstack)
    D = X.unsqueeze(1)
    Probit.data = D
    n_samples_post = 5000
    T_mcmc = 10**5 + 1 
    sigma2_0 = torch.tensor(1.)
    eps_0 = torch.randn(p)
    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept=0.4, adap=True, Cov=True, disable_tqdm=False)
    theta_MH = NN_cst(eps_MH)
    with torch.no_grad():
        theta_MH = theta_MH.detach().numpy()
    theta_post_cstr = theta_MH[-n_samples_post:,-n_samples_post:]

    # Saves all relevant quantities
    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI, 'constr' : constr_values}
    Prior = {'samples' : theta_sample_prior, 'jeffreys' : None, 'params' : all_params}
    Posterior = {'samples' : theta_post_cstr, 'jeffreys' : None}
    Probit_results_cstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}
    with open(file_path, 'wb') as file:
        pickle.dump(Probit_results_cstr, file)

    torch.save(NN_cst.state_dict(),   os.path.join(path_plot, 'Probit_results_constrained_torch_state'))
    
else :
    with open(file_path, 'rb') as file:
        res = pickle.load(file)
        MI_dic = res['MI']
        Prior = res['prior'] 
        Posterior = res['post']
        MI, range_MI, lower_MI, upper_MI, constr_values = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper'], MI_dic['constr']
        theta_sample_prior, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']
        theta_post_cstr, jeffreys_post = Posterior['samples'], Posterior['jeffreys']

with torch.no_grad():
    mult_w2 = NN_cst.netbeta.singl.fc1.weight.detach().numpy()
w2_neg = 1/mult_w2[mult_w2<0]
w2_pos = 1/mult_w2[mult_w2>0]
print('multiplicative weights on beta:')
print('neagtives belong to [{},{}]'.format(w2_neg.min(), w2_neg.max()))
print('positives belong to [{},{}]'.format(w2_pos.min(), w2_pos.max()))

```


```{python stem-plot}
#| label: fig-probit_constr_mi
#| fig-cap: "Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 100 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. The learning rate used in the optimization is $0.0005$."

plt.figure(figsize=(5, 3))
plt.plot(range_MI, MI, '-', color=rougeCEA)
plt.plot(range_MI, MI, '*', color=rougeCEA)
# for i in range(len(range_MI)):
#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')
plt.plot(range_MI, upper_MI, '-', color='lightgrey')
plt.plot(range_MI, lower_MI, '-', color='lightgrey')
plt.fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)
plt.xlabel(r"Epochs")
plt.ylabel(r"Generalized mutual information")
plt.grid()
#plt.yscale('log')
plt.tight_layout()
plt.show()


plt.figure(figsize=(5, 4))
plt.hist(theta_sample_init[:,1], density=True, bins="rice", color="red", label=r"Initial prior in $\beta$", alpha=0.4)
plt.hist(remove_nan_and_inf(theta_sample_prior[:,1]), density=True, bins="rice", label=r"Fitted prior", alpha=0.4)





target_w1 = lambda w1: np.linalg.norm(w1, axis=-1)-1
target_b1 = lambda b1: np.abs(b1)
target_w2_1 = lambda w2: np.max(1/w2 -1e10*(w2>0), axis=-1)
target_w2_2 = lambda w2: np.min(1/w2 +1e10*(w2<0), axis=-1)
plt.figure(figsize=(5, 4))
plt.plot(range_MI, target_w1(np.array(Constr.keep_params['w1']).squeeze()), label=r'$\|w_1\|_2-1$')
plt.plot(range_MI, target_b1(Constr.keep_params['b1']), label=r'$|b_1|$')
plt.plot(range_MI, target_w2_1(np.array(Constr.keep_params['w2']).squeeze()), label=r'$\max(w_2^{-1}<0)$')
plt.plot(range_MI, target_w2_2(np.array(Constr.keep_params['w2']).squeeze()), label=r'$\min(w_2^{-1}>0)$')

plt.legend()

```


```{python stem-plot}
#| label: fig-probit_constr_gap
#| fig-cap: "Evolution of the constraint value gap during training. It corresponds to the difference between the target and current values for the constraint (in absolute value)"

plt.figure(figsize=(5, 3))
plt.plot(range_MI, constr_values[:,0,1], '-', color=vertCEA)
plt.plot(range_MI, constr_values[:,0,1], '*', color=vertCEA)
plt.xlabel(r"Epochs")
plt.ylabel(r"Constraint gap")
plt.grid()
#plt.yscale('log')
plt.tight_layout()
plt.show()
```


For the posterior, we take as dataset $50$ samples from the probit model. For computational reasons, the Metropolis-Hastings algorithm is applied for only $5\cdot10^4$ iterations. An important remark is that if the size of the dataset is rather small, the probability that the data is degenerate is not negligible. By degenerate data, we refer to situations when the data points are partitioned into two disjoint subsets when classified according to $a$ values, the posterior becomes improper because the likelihood is constant (@van2024reference). In such cases, the convergence of the Markov chains is less apparent, the plots for this section are obtained with non-degenerate datasets.   


```{python stem-plot}
#| label: fig-probit_post_scatterhist
#| fig-cap: "Scatter histogram of the unconstrained fitted posterior and the Jeffreys posterior distributions obtained from $5000$ samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels."

N = 50  # non-degenerate
i = 2
file_path = os.path.join(int_jeffreys_path, f'model_J_{i}')
with open(file_path, 'rb') as file:
    model_J = pickle.load(file)
theta_J = model_J['logs']['post'][N]

x1 = theta_post_nocstr[:, 0]
y1 = theta_post_nocstr[:, 1]
x2 = theta_J[:, 0]
y2 = theta_J[:, 1]

kde_x1 = gaussian_kde(x1)
kde_y1 = gaussian_kde(y1)
kde_x2 = gaussian_kde(x2)
kde_y2 = gaussian_kde(y2)

# Create figure and gridspec layout
fig = plt.figure(figsize=(8, 6))
gs = gridspec.GridSpec(4, 5) 

# Main scatter plot 
ax_main = fig.add_subplot(gs[1:4, 0:3])
ax_main.scatter(x1, y1, alpha=0.5, s=20, marker='.', label='Fitted posterior',zorder=2)
ax_main.scatter(x2, y2, alpha=0.5, s=20, marker='.', label='Jeffreys posterior',zorder=1, color=vertCEA)
ax_main.set_xlabel(r'$\theta_1$')
ax_main.set_ylabel(r'$\theta_2$')
ax_main.legend(loc='upper left', fontsize=11)
ax_main.grid()

# Marginal histogram / KDE for alpha
ax_x_hist = fig.add_subplot(gs[0, 0:3], sharex=ax_main)
ax_x_hist.hist(x1, bins='rice', alpha=0.4, label='Fitted histogram',density=True)
ax_x_hist.hist(x2, bins='rice', alpha=0.4, label='Jeffreys histogram',density=True,color=vertCEA)
x_vals = np.linspace(min(x1.min(), x2.min()), max(x1.max(), x2.max()), 100)
ax_x_hist.plot(x_vals, kde_x1(x_vals), color='blue', lw=2, label='Fitted KDE')
ax_x_hist.plot(x_vals, kde_x2(x_vals), color='green', lw=2, label='Jeffreys KDE')
ax_x_hist.set_ylabel(r'Marginal $\theta_1$')
ax_x_hist.tick_params(axis='x', labelbottom=False)
ax_x_hist.legend()
ax_x_hist.grid()

# Marginal histogram / KDE for beta
ax_y_hist = fig.add_subplot(gs[1:4, 3:5], sharey=ax_main)
ax_y_hist.hist(y1, bins='rice', orientation='horizontal', alpha=0.4, label='Fitted histogram',density=True)
ax_y_hist.hist(y2, bins='rice', orientation='horizontal', alpha=0.4, label='Jeffreys histogram',density=True, color=vertCEA)
y_vals = np.linspace(min(y1.min(), y2.min()), max(y1.max(), y2.max()), 100)
ax_y_hist.plot(kde_y1(y_vals), y_vals, color='blue', lw=2, label='Fitted KDE')
ax_y_hist.plot(kde_y2(y_vals), y_vals, color='green', lw=2, label='Jeffreys KDE')
ax_y_hist.set_xlabel(r'Marginal $\theta_2$')
ax_y_hist.tick_params(axis='y', labelleft=False)
ax_y_hist.legend()
ax_y_hist.grid()

plt.tight_layout()
plt.show()



# curves

fig = plt.figure(figsize=(4,3))
ax = fig.add_subplot(111)

plot_frag_cred(theta_post_nocstr, ax, 'red', label='VA-RP')
plot_frag_cred(theta_J, ax, 'blue', label='AJ')


```

As @fig-probit_post_scatterhist shows, we obtain a relevant approximation of the true Jeffreys posterior especially on the variable $\theta_1$, whereas a small difference is present for the tail of the distribution on $\theta_2$. The latter remark was expected regarding the analytical study of the marginal distribution of $\pi_\lambda$ w.r.t. $\theta_2$  given the architecture considered for the VA-RP (see @eq-decay_rates_ptheta2).
It is interesting to see that the difference between the posteriors is harder to discern in the neighborhood of $\theta_2=0$. Indeed, in such case where the data are not degenerate, the likelihood provides a strong decay rate when $\theta_2\rightarrow0$ that makes the influence of the prior negligible (see @van2024reference):
$$
L_N(\bX\,|\,\theta) \aseq{\theta_2\rightarrow0} \theta_2^{\|\chi\|_2^2}\exp\left(-\frac{1}{2\theta_2^2}\sum_{i=1}^N\chi_i(\log a_i-\log\theta_1)^2 \right),
$$
where $\chi\in\{0,1\}^N$ is a non-null vector that depends on $\bX$.

When $\theta_2\rightarrow\infty$, however, the likelihood does not reduce the influence of the prior has it remains asymptotically constant: $L_N(\bX \,|\,\theta) \conv{\theta_2\rightarrow\infty}2^{-N}$.


```{python stem-plot}
#| label: fig-probit_post_constr_scatterhist
#| fig-cap: "Scatter histogram of the constrained fitted posterior and the target posterior distributions obtained from $5000$ samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels."

N = 50  # non-degenerate
i = 2
file_path = os.path.join(int_jeffreys_path, f'model_J_constraint_{i}')
with open(file_path, 'rb') as file:
    model_J = pickle.load(file)
theta_J = model_J['logs']['post'][N]

x1 = theta_post_cstr[:, 0]
y1 = theta_post_cstr[:, 1]
x2 = theta_J[:, 0]
y2 = theta_J[:, 1]
kde_x1 = gaussian_kde(x1)
kde_y1 = gaussian_kde(y1)
kde_x2 = gaussian_kde(x2)
kde_y2 = gaussian_kde(y2)

# Create figure and gridspec layout
fig = plt.figure(figsize=(8, 6))
gs = gridspec.GridSpec(4, 5) 

# Main scatter plot 
ax_main = fig.add_subplot(gs[1:4, 0:3])
ax_main.scatter(x1, y1, alpha=0.5, s=20, marker='.', label='Fitted posterior',zorder=2)
ax_main.scatter(x2, y2, alpha=0.5, s=20, marker='.', label='Jeffreys posterior',zorder=1, color=vertCEA)
ax_main.set_xlabel(r'$\theta_1$')
ax_main.set_ylabel(r'$\theta_2$')
ax_main.legend(loc='upper left',fontsize=11)
ax_main.grid()

# Marginal histogram / KDE for alpha
ax_x_hist = fig.add_subplot(gs[0, 0:3], sharex=ax_main)
ax_x_hist.hist(x1, bins='rice', alpha=0.4, label='Fitted histogram',density=True)
ax_x_hist.hist(x2, bins='rice', alpha=0.4, label='Jeffreys histogram',density=True,color=vertCEA)
x_vals = np.linspace(min(x1.min(), x2.min()), max(x1.max(), x2.max()), 100)
ax_x_hist.plot(x_vals, kde_x1(x_vals), color='blue', lw=2, label='Fitted KDE')
ax_x_hist.plot(x_vals, kde_x2(x_vals), color='green', lw=2, label='Jeffreys KDE')
ax_x_hist.set_ylabel(r'Marginal $\theta_1$')
ax_x_hist.tick_params(axis='x', labelbottom=False)
ax_x_hist.legend()
ax_x_hist.grid()

# Marginal histogram / KDE for beta
ax_y_hist = fig.add_subplot(gs[1:4, 3:5], sharey=ax_main)
ax_y_hist.hist(y1, bins='rice', orientation='horizontal', alpha=0.4, label='Fitted histogram',density=True)
ax_y_hist.hist(y2, bins='rice', orientation='horizontal', alpha=0.4, label='Jeffreys histogram',density=True, color=vertCEA)
y_vals = np.linspace(min(y1.min(), y2.min()), max(y1.max(), y2.max()), 100)
ax_y_hist.plot(kde_y1(y_vals), y_vals, color='blue', lw=2, label='Fitted KDE')
ax_y_hist.plot(kde_y2(y_vals), y_vals, color='green', lw=2, label='Jeffreys KDE')
ax_y_hist.set_xlabel(r'Marginal $\theta_2$')
ax_y_hist.tick_params(axis='y', labelleft=False)
ax_y_hist.legend()
ax_y_hist.grid()

plt.tight_layout()
plt.show()





# curves

fig = plt.figure(figsize=(4,3))
ax = fig.add_subplot(111)

plot_frag_cred(theta_post_cstr, ax, 'red', label='VA-RP')
plot_frag_cred(theta_J, ax, 'blue', label='AJ')




```

The result on the constrained case (@fig-probit_post_constr_scatterhist) is very similar to the unconstrained one. 

 Softplus activation function. The dimension of the latent variable $\varepsilon$ is $p=10$.

# Fragility curves




## Probit model and robustness

As mentioned in @sec-probit_model regarding the probit model, we present several additional results.

```{python stem-plot}
#| label: fig-quad_error_post
#| fig-cap: "Mean norm difference as a function of the size $N$ of the dataset for the unconstrained fitted posterior and the Jeffreys posterior. For each value of $N$, 10 different datasets are considered from which we obtain 95\\% confidence intervals."


num_M = 10
N_max = 200
N_init = 5
tab_N = np.arange(N_init, N_max+N_init, 5)
QE = np.zeros((len(tab_N), num_M))
file_name = os.path.join(varp_probit_path, 'Quadratic_errors/Quad_error_post')
first_file = file_name + '1.pkl'
for M in range(num_M):
    file = file_name + f'{M+1}.pkl'
    with open(file, 'rb') as file:
        QE[:,M] = pickle.load(file)

mean_vals = np.mean(QE, axis=1)
lower_quantile = np.quantile(QE, 0.025, axis=1)
upper_quantile = np.quantile(QE, 0.975, axis=1)

seed_all(1)
theta_vrai = np.array([3.37610525, 0.43304097])
N_max = 200
N_init = 5
tab_N2 = np.arange(N_init, N_max+N_init, 5)
num_MC = 5000
M = 10
num_MC = 5000
err_jeff = np.zeros((len(tab_N2), M))
j = 0
for N in tab_N2 : 
    resultats_theta_post_N = np.zeros((M, num_MC, 2))
    for i in tqdm(range(M), desc=f'Iterations for N={N}', disable=True): 
        file_path = os.path.join(int_jeffreys_path, f'model_J_{i}')
        with open(file_path, 'rb') as file:
            Jeffreys = pickle.load(file)
        resultats_theta_post_N[i] = Jeffreys['logs']['post'][N]
        err_jeff[j, i] = np.mean(np.linalg.norm(resultats_theta_post_N[i] - theta_vrai[np.newaxis], axis=1))
    j = j + 1

mean_vals2 = np.mean(err_jeff, axis=1)
lower_quantile2 = np.quantile(err_jeff, 0.025, axis=1)
upper_quantile2 = np.quantile(err_jeff, 0.975, axis=1)

plt.figure(figsize=(5, 3))
plt.plot(tab_N, mean_vals, label='Mean Approx')
plt.fill_between(tab_N, lower_quantile, upper_quantile, color='b', alpha=0.2, label='95% CI Approx')

plt.plot(tab_N2, mean_vals2, label='Mean Jeffreys')
plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color='orange', alpha=0.2, label='95% CI Jeffreys')
plt.xlabel('N')
plt.ylabel('Mean Norm Difference')
plt.legend(fontsize=12)
plt.grid()
plt.show()
```


```{python stem-plot}
#| label: fig-quad_error_post_constr
#| fig-cap: "Mean norm difference as a function of the size $N$ of the dataset for the constrained fitted posterior and the Jeffreys posterior. For each value of $N$, 10 different datasets are considered from which we obtain 95\\% confidence intervals."

num_M = 10
N_max = 200
N_init = 5
tab_N = np.arange(N_init, N_max+N_init, 5)
QE = np.zeros((len(tab_N), num_M))
file_name = os.path.join(varp_probit_path, 'Quadratic_errors/Quad_error_post_constr')
first_file = file_name + '1.pkl'
for M in range(num_M):
    file = file_name + f'{M+1}.pkl'
    with open(file, 'rb') as file:
        QE[:,M] = pickle.load(file)

mean_vals = np.mean(QE, axis=1)
lower_quantile = np.quantile(QE, 0.025, axis=1)
upper_quantile = np.quantile(QE, 0.975, axis=1)

seed_all(1)
theta_vrai = np.array([3.37610525, 0.43304097])
N_max = 200
N_init = 5
tab_N2 = np.arange(N_init, N_max+N_init, 5)
num_MC = 5000
M = 10
num_MC = 5000
err_jeff = np.zeros((len(tab_N2), M))
j = 0
for N in tab_N2 : 
    resultats_theta_post_N = np.zeros((M, num_MC, 2))
    for i in tqdm(range(M), desc=f'Iterations for N={N}', disable=True):
        file_path = os.path.join(int_jeffreys_path, f'model_J_constraint_{i}') 
        with open(file_path, 'rb') as file:
            Jeffreys = pickle.load(file)
        resultats_theta_post_N[i] = Jeffreys['logs']['post'][N]
        err_jeff[j, i] = np.mean(np.linalg.norm(resultats_theta_post_N[i] - theta_vrai[np.newaxis], axis=1))
    j = j + 1

mean_vals2 = np.mean(err_jeff, axis=1)
lower_quantile2 = np.quantile(err_jeff, 0.025, axis=1)
upper_quantile2 = np.quantile(err_jeff, 0.975, axis=1)

plt.figure(figsize=(5, 3))
plt.plot(tab_N, mean_vals, label='Mean Approx')
plt.fill_between(tab_N, lower_quantile, upper_quantile, color='b', alpha=0.2, label='95% CI Approx')

plt.plot(tab_N2, mean_vals2, label='Mean Jeffreys')
plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color='orange', alpha=0.2, label='95% CI Jeffreys')
plt.xlabel('N')
plt.ylabel('Mean Norm Difference')
plt.legend(fontsize=12)
plt.grid()
plt.show()
```


@fig-quad_error_post and @fig-quad_error_post_constr show the evolution of the posterior mean norm difference as the size $N$ of the dataset considered for the posterior distribution increases. For each value of $N$, 10 different datasets are used in order to quantify the variability of said error. The proportion of degenerate datasets is rather high when $N=5$ or $N=10$, the consequence is that the approximation tends to be more unstable. The main observation is that the error is decreasing in all cases when $N$ increases, also, the behaviour of the error for the fitted distributions on one hand, and the behaviour for the Jeffreys distribution on the other hand are quite similar in terms of mean value and confidence intervals.




```{python stem-plot}
#| label: fig-probit_ecdf
#| fig-cap: "Empirical cumulative distribution functions for the unconstrained fitted posterior and the Jeffreys posterior using $5000$ samples. The band is obtained by computing the ECDFs over $100$ different seeds and monitoring the maximum and minimum ECDF values for each $\\theta$."

n_exp = 100   # Number of experiments
model = os.path.join(varp_probit_path, 'probit_samples/probit')
q = 2
first_file = model + '_seed_1.pkl' 
 
n_samples_prior = 10**6
n_samples_post = 5000

# Concatenate results from all experiments

theta_prior = np.zeros((n_exp,n_samples_prior,q))
jeffreys_prior = np.zeros((n_exp,n_samples_prior,q))
theta_post = np.zeros((n_exp,n_samples_post,q))
jeffreys_post = np.zeros((n_exp,n_samples_post,q))

for k in range(n_exp):
    file_name = model + f'_seed{k+1}.pkl'
    with open(file_name, 'rb') as file:
        data = pickle.load(file)
        theta_prior[k,:,:] = data['prior']['VA'] 
        #jeffreys_prior[k,:,:] = data['prior']['Jeffreys']
        theta_post[k,:,:] = data['post']['VA']
        jeffreys_post[k,:,:] = data['post']['Jeffreys']

# Post CDF computations 
num_x = 5000
x_values_alpha = np.linspace(0., 8., num_x)
x_values_beta = np.linspace(0., 2., num_x)
cdf_post_alpha = np.zeros((n_exp, num_x))
cdf_post_beta = np.zeros((n_exp, num_x))
jeff_post_alpha = np.zeros((n_exp, num_x))
jeff_post_beta = np.zeros((n_exp, num_x))
for k in range(n_exp):
    cdf_post_alpha[k,:] = compute_ecdf(theta_post[k,:,0], x_values_alpha)
    cdf_post_beta[k,:] = compute_ecdf(theta_post[k,:,1], x_values_beta)
    jeff_post_alpha[k,:] = compute_ecdf(jeffreys_post[k,:,0], x_values_alpha)
    jeff_post_beta[k,:] = compute_ecdf(jeffreys_post[k,:,1], x_values_beta)


lower_bound_alpha = np.quantile(cdf_post_alpha, 0., axis=0)
upper_bound_alpha = np.quantile(cdf_post_alpha, 1.0, axis=0)
median_cdf_alpha = np.quantile(cdf_post_alpha, 0.5, axis=0)
median_jeff_alpha = np.quantile(jeff_post_alpha, 0.5, axis=0)

lower_bound_beta = np.quantile(cdf_post_beta, 0., axis=0)
upper_bound_beta = np.quantile(cdf_post_beta, 1., axis=0)
median_cdf_beta = np.quantile(cdf_post_beta, 0.5, axis=0)
median_jeff_beta = np.quantile(jeff_post_beta, 0.5, axis=0)

# Create subplots for alpha and beta
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))

# Plot for alpha
ax1.plot(x_values_alpha, median_cdf_alpha, label='Median fitted ECDF', color='blue', linewidth=2)
ax1.plot(x_values_alpha, median_jeff_alpha, label='Jeffreys ECDF', color='green', linewidth=2) 
ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color='lightblue', alpha=0.5, label='ECDF Band')
ax1.set_xlabel(r'$\theta_1$')
ax1.set_ylabel('CDF')
ax1.grid()
ax1.legend(loc='lower right', fontsize=12)

# Plot for beta
ax2.plot(x_values_beta, median_cdf_beta, label='Median fitted ECDF', color='blue', linewidth=2)
ax2.plot(x_values_beta, median_jeff_beta, label='Jeffreys ECDF', color='green', linewidth=2)  
ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color='lightblue', alpha=0.5, label='ECDF Band')
ax2.set_xlabel(r'$\theta_2$')
ax2.set_ylabel('CDF')
ax2.grid()
ax2.legend(loc='lower right', fontsize=12)
plt.tight_layout()  
plt.show()

```


```{python stem-plot}
#| label: fig-probit_ecdf_constr
#| fig-cap: "Empirical cumulative distribution functions for the constrained fitted posterior and the Jeffreys posterior using $5000$ samples. The band is obtained by computing the ECDFs over $100$ different seeds and monitoring the maximum and minimum ECDF values for each $\\theta$."

n_exp = 100   # Number of experiments
model = os.path.join(varp_probit_path, 'probit_samples/probit_constr')
q = 2
first_file = model + '_seed1.pkl' 
n_samples_prior = 10**6
n_samples_post = 5000

# Concatenate results from all experiments

theta_prior = np.zeros((n_exp,n_samples_prior,q))
jeffreys_prior = np.zeros((n_exp,n_samples_prior,q))
theta_post = np.zeros((n_exp,n_samples_post,q))
jeffreys_post = np.zeros((n_exp,n_samples_post,q))

for k in range(n_exp):
    file_name = model + f'_seed{k+1}.pkl'
    #file_name = model + f'_seed_lognormal{k+1}.pkl'
    with open(file_name, 'rb') as file:
        data = pickle.load(file)
        theta_prior[k,:,:] = data['prior']['VA'] 
        #jeffreys_prior[k,:,:] = data['prior']['Jeffreys']
        theta_post[k,:,:] = data['post']['VA']
        jeffreys_post[k,:,:] = data['post']['Jeffreys']

# Post CDF computations 
num_x = 5000
x_values_alpha = np.linspace(0., 8., num_x)
x_values_beta = np.linspace(0., 2., num_x)
cdf_post_alpha = np.zeros((n_exp, num_x))
cdf_post_beta = np.zeros((n_exp, num_x))
jeff_post_alpha = np.zeros((n_exp, num_x))
jeff_post_beta = np.zeros((n_exp, num_x))
for k in range(n_exp):
    cdf_post_alpha[k,:] = compute_ecdf(theta_post[k,:,0], x_values_alpha)
    cdf_post_beta[k,:] = compute_ecdf(theta_post[k,:,1], x_values_beta)
    jeff_post_alpha[k,:] = compute_ecdf(jeffreys_post[k,:,0], x_values_alpha)
    jeff_post_beta[k,:] = compute_ecdf(jeffreys_post[k,:,1], x_values_beta)


lower_bound_alpha = np.quantile(cdf_post_alpha, 0., axis=0)
upper_bound_alpha = np.quantile(cdf_post_alpha, 1.0, axis=0)
median_cdf_alpha = np.quantile(cdf_post_alpha, 0.5, axis=0)
median_jeff_alpha = np.quantile(jeff_post_alpha, 0.5, axis=0)

lower_bound_beta = np.quantile(cdf_post_beta, 0., axis=0)
upper_bound_beta = np.quantile(cdf_post_beta, 1.0, axis=0)
median_cdf_beta = np.quantile(cdf_post_beta, 0.5, axis=0)
median_jeff_beta = np.quantile(jeff_post_beta, 0.5, axis=0)

# Create subplots for alpha and beta
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))

# Plot for alpha
ax1.plot(x_values_alpha, median_cdf_alpha, label='Median fitted ECDF', color='blue', linewidth=2)
ax1.plot(x_values_alpha, median_jeff_alpha, label='Jeffreys ECDF', color='green', linewidth=2) 
ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color='lightblue', alpha=0.5, label='ECDF Band')
ax1.set_xlabel(r'$\theta_1$')
ax1.set_ylabel('CDF')
ax1.grid()
ax1.legend(loc='lower right', fontsize=12)

# Plot for beta
ax2.plot(x_values_beta, median_cdf_beta, label='Median fitted ECDF', color='blue', linewidth=2)
ax2.plot(x_values_beta, median_jeff_beta, label='Jeffreys ECDF', color='green', linewidth=2)  
ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color='lightblue', alpha=0.5, label='ECDF Band')
ax2.set_xlabel(r'$\theta_2$')
ax2.set_ylabel('CDF')
ax2.grid()
ax2.legend(loc='lower right', fontsize=12)
plt.tight_layout()  
plt.show()

```


@fig-probit_ecdf and @fig-probit_ecdf_constr compare the empirical distribution functions of the fitted posterior and the Jeffreys posterior. In the unconstrained case, one can observe that the ECDFs are very close for $\theta_1$, whereas the variability is slightly higher for $\theta_2$ although still reasonable. When imposing a constraint on $\theta_2$, one remarks that the variability of the result is higher. The Jeffreys ECDF is contained in the band when $\theta_2$ is close to zero, but not when $\theta_2$ increases ($\theta_2 > 0.5$). This is coherent with the previous scatter histograms where the Jeffreys posterior on $\theta_2$ tends to have a heavier tail than the variational approximation.

Altogether, despite the stochastic nature of the developed algorithm, we consider that the result tends to be reasonably robust to the RNG seed for the optimization part, and robust to the dataset used for the posterior distribution for the MCMC part.

# References {.unnumbered}

::: {#refs}
:::
