<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nils Baillie">
<meta name="author" content="Antoine Van Biesbroeck">
<meta name="author" content="Clément Gauchy">
<meta name="dcterms.date" content="2025-02-11">
<meta name="keywords" content="Reference priors, Variational inference, Neural networks">

<title>Variational inference for approximate reference priors using neural networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="main_nb_files/libs/clipboard/clipboard.min.js"></script>
<script src="main_nb_files/libs/quarto-html/quarto.js"></script>
<script src="main_nb_files/libs/quarto-html/popper.min.js"></script>
<script src="main_nb_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="main_nb_files/libs/quarto-html/anchor.min.js"></script>
<link href="main_nb_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="main_nb_files/libs/quarto-html/quarto-syntax-highlighting-86daaaaad7353f9cc0c554efc1dd6d94.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="main_nb_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="main_nb_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="main_nb_files/libs/bootstrap/bootstrap-f11e999d7f1636233620f09fa30e7671.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="main_nb_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="main_nb_files/libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>
<meta name="quarto:status" content="draft">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; Variational inference for approximate reference priors using neural networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        Nils Baillie 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université Paris-Saclay, CEA, Service d’Études Mécaniques et Thermiques, 91191 Gif-sur-Yvette, France
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Antoine Van Biesbroeck 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://cmap.ip-paris.fr/">
                  CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France
                  </a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        Clément Gauchy 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  Université Paris-Saclay, CEA, Service de Génie Logiciel pour la Simulation, 91191 Gif-sur-Yvette, France
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 11, 2025</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">February 11, 2025</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Reference priors, Variational inference, Neural networks</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <p class="date">draft</p>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>In Bayesian statistics, the choice of the prior can have an important influence on the posterior and the parameter estimation, especially when few data samples are available. To limit the added subjectivity from a priori information, one can use the framework of reference priors. However, computing such priors is a difficult task in general. We develop in this paper a flexible algorithm based on variational inference which computes approximations of reference priors from a set of parametric distributions using neural networks. We also show that our algorithm can retrieve reference priors when constraints are specified in the optimization problem to ensure the solution is proper. We propose a simple method to recover a relevant approximation of the parametric posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if the density function of the parametric prior is not known in general. Numerical experiments on several statistical models of increasing complexity are presented. We show the usefulness of this approach by recovering the target distribution. The performance of the algorithm is evaluated on the prior distributions as well as the posterior distributions, jointly using variational inference and MCMC sampling.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#sec-rp_theory" id="toc-sec-rp_theory" class="nav-link" data-scroll-target="#sec-rp_theory"><span class="header-section-number">2</span> Reference priors theory</a></li>
  <li><a href="#sec-VA-RP" id="toc-sec-VA-RP" class="nav-link" data-scroll-target="#sec-VA-RP"><span class="header-section-number">3</span> Variational approximation of the reference prior (VA-RP)</a>
  <ul class="collapse">
  <li><a href="#implicitly-defined-parametric-probability-distributions-using-neural-networks" id="toc-implicitly-defined-parametric-probability-distributions-using-neural-networks" class="nav-link" data-scroll-target="#implicitly-defined-parametric-probability-distributions-using-neural-networks"><span class="header-section-number">3.1</span> Implicitly defined parametric probability distributions using neural networks</a></li>
  <li><a href="#sec-sga" id="toc-sec-sga" class="nav-link" data-scroll-target="#sec-sga"><span class="header-section-number">3.2</span> Learning the VA-RP using stochastic gradient algorithm</a></li>
  <li><a href="#adaptation-for-the-constrained-va-rp" id="toc-adaptation-for-the-constrained-va-rp" class="nav-link" data-scroll-target="#adaptation-for-the-constrained-va-rp"><span class="header-section-number">3.3</span> Adaptation for the constrained VA-RP</a></li>
  <li><a href="#sec-posterio-sampling" id="toc-sec-posterio-sampling" class="nav-link" data-scroll-target="#sec-posterio-sampling"><span class="header-section-number">3.4</span> Posterior sampling using implicitly defined prior distributions</a></li>
  </ul></li>
  <li><a href="#sec-numexp" id="toc-sec-numexp" class="nav-link" data-scroll-target="#sec-numexp"><span class="header-section-number">4</span> Numerical experiments</a>
  <ul class="collapse">
  <li><a href="#multinomial-model" id="toc-multinomial-model" class="nav-link" data-scroll-target="#multinomial-model"><span class="header-section-number">4.1</span> Multinomial model</a></li>
  <li><a href="#sec-probit_model" id="toc-sec-probit_model" class="nav-link" data-scroll-target="#sec-probit_model"><span class="header-section-number">4.2</span> Probit model</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">5</span> Conclusion</a></li>
  <li><a href="#acknowledgement" id="toc-acknowledgement" class="nav-link" data-scroll-target="#acknowledgement">Acknowledgement</a></li>
  <li><a href="#sec-appendix" id="toc-sec-appendix" class="nav-link" data-scroll-target="#sec-appendix"><span class="header-section-number">6</span> Appendix</a>
  <ul class="collapse">
  <li><a href="#sec-grad_comp" id="toc-sec-grad_comp" class="nav-link" data-scroll-target="#sec-grad_comp"><span class="header-section-number">6.1</span> Gradient computation of the generalized mutual information</a></li>
  <li><a href="#gaussian-distribution-with-variance-parameter" id="toc-gaussian-distribution-with-variance-parameter" class="nav-link" data-scroll-target="#gaussian-distribution-with-variance-parameter"><span class="header-section-number">6.2</span> Gaussian distribution with variance parameter</a></li>
  <li><a href="#probit-model-and-robustness" id="toc-probit-model-and-robustness" class="nav-link" data-scroll-target="#probit-model-and-robustness"><span class="header-section-number">6.3</span> Probit model and robustness</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="main_nb.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="hidden">

</div>
<div id="102e9da8" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sys</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>base_dir <span class="op">=</span> os.getcwd()  <span class="co"># the path must be set on VA-RP</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>py_dir <span class="op">=</span> os.path.join(base_dir, <span class="st">"python_files"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> py_dir <span class="kw">not</span> <span class="kw">in</span> sys.path:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    sys.path.append(py_dir)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#print(sys.path)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> aux_optimizers <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stat_models_torch <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nets <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> variational_approx <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> div_metrics_torch <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> constraints <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Packages used</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> gaussian_kde</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> rc</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> gridspec</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">'text'</span>, usetex<span class="op">=</span><span class="va">False</span>) </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>rc(<span class="st">'lines'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>rougeCEA <span class="op">=</span> <span class="st">"#b81420"</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>vertCEA <span class="op">=</span> <span class="st">"#73be4b"</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>path_plot <span class="op">=</span> os.path.join(base_dir, <span class="st">"plots_data"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>probit_path <span class="op">=</span> os.path.join(base_dir, <span class="st">"data_probit"</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>tirages_path <span class="op">=</span> os.path.join(probit_path, <span class="st">"tirages_probit"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>int_jeffreys_path <span class="op">=</span> os.path.join(probit_path, <span class="st">"Multiseed_AJ"</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>varp_probit_path <span class="op">=</span> os.path.join(probit_path, <span class="st">"Multiseed_VARP"</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">### Load parameter for each main computation</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># If True, loads the different results without re-doing every computation</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># If False, all computations will be done and saved (and possibly overwrite the data files !)</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>load_results_Multinom <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>load_results_MultiMMD <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>load_results_Probit_nocstr <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>load_results_Probit_cstr <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>load_results_Normal_nocstr <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>load_results_Normal_cstr <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The Bayesian approach to statistical inference aims to produce estimations using the posterior distribution. The latter is derived by updating the prior distribution with the observed statistics thanks to Bayes’ theorem. However, the shape of the posterior can be heavily influenced by the prior choice when the amount of available data is limited or the prior distribution is highly informative. For this reason, selecting a prior remains a daunting task that must be handled carefully. Hence, systematic methods has been introduced by statisticians to help the choice of the prior distribution, both in cases where subjective knowledge is available or not <span class="citation" data-cites="Press2009">Press (<a href="#ref-Press2009" role="doc-biblioref">2009</a>)</span>. <span class="citation" data-cites="Kass1996">Kass and Wasserman (<a href="#ref-Kass1996" role="doc-biblioref">1996</a>)</span> propose different ways of defining the level of non-informativeness of a prior distribution. The most famous method is the Maximum Entropy (ME) prior distribution that has been popularized by <span class="citation" data-cites="Jaynes1957">Jaynes (<a href="#ref-Jaynes1957" role="doc-biblioref">1957</a>)</span>. In a Bayesian context, ME and Maximal Data Information (MDI) priors have been studied by <span class="citation" data-cites="Zellner1996">Zellner (<a href="#ref-Zellner1996" role="doc-biblioref">1996</a>)</span>, <span class="citation" data-cites="Soofi2000">Soofi (<a href="#ref-Soofi2000" role="doc-biblioref">2000</a>)</span>. Other candidates for objective priors are the so-called matching priors <span class="citation" data-cites="Reid2003">Reid, Mukerjee, and Fraser (<a href="#ref-Reid2003" role="doc-biblioref">2003</a>)</span>, which are priors such that the Bayesian posterior credible intervals correspond to confidence intervals of the sampling model. Moreover, when a simpler model is known, the Penalizing Complexity (PC) priors are yet another rationale of choosing an objective (or reference) prior distribution <span class="citation" data-cites="Simpson2017">Simpson et al. (<a href="#ref-Simpson2017" role="doc-biblioref">2017</a>)</span>.</p>
<p>In this paper, we will focus on the reference prior theory. First introduced in <span class="citation" data-cites="Bernardo1979a">Bernardo (<a href="#ref-Bernardo1979a" role="doc-biblioref">1979a</a>)</span> and further formalized in <span class="citation" data-cites="berger2009formal">Berger, Bernardo, and Sun (<a href="#ref-berger2009formal" role="doc-biblioref">2009</a>)</span>, the main rationale behind the reference prior theory is the maximization of the information brought by the data during Bayesian inference. Specifically, reference priors (RPs) are constructed to maximize the mutual information metric, which is defined as a divergence between itself and the posterior. In this way, it ensures that the data plays a dominant role in the Bayesian framework. This approach has been extensively studied (see e.g. <span class="citation" data-cites="bernardo1979">Bernardo (<a href="#ref-bernardo1979" role="doc-biblioref">1979b</a>)</span>, <span class="citation" data-cites="clarke1994jeffreys">Clarke and Barron (<a href="#ref-clarke1994jeffreys" role="doc-biblioref">1994</a>)</span>, <span class="citation" data-cites="van2023generalized">Van Biesbroeck (<a href="#ref-van2023generalized" role="doc-biblioref">2024a</a>)</span>) and applied to various statistical models, such as Gaussian process-based models <span class="citation" data-cites="Paulo2005">Paulo (<a href="#ref-Paulo2005" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="Gu2016">Gu and Berger (<a href="#ref-Gu2016" role="doc-biblioref">2016</a>)</span>, generalized linear models <span class="citation" data-cites="Natarajan2000">Natarajan and Kass (<a href="#ref-Natarajan2000" role="doc-biblioref">2000</a>)</span>, and even Bayesian Neural Networks <span class="citation" data-cites="gao2022deep">Gao, Ramesh, and Chaudhari (<a href="#ref-gao2022deep" role="doc-biblioref">2022</a>)</span>. The RPs are recognized for their objective nature in practical studies <span class="citation" data-cites="Dandrea2021">D’Andrea (<a href="#ref-Dandrea2021" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="Li2021">Li and Gu (<a href="#ref-Li2021" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span>, yet they suffer from their low computational feasibility. Indeed, the expression of the RPs often leads to an intricate theoretical expression, which necessitates a heavy numerical cost to be derived that becomes even more cumbersome as the dimensionality of the problem increases. Moreover, in many applications, a posteriori estimates are obtained using Markov Chain Monte Carlo (MCMC) methods, which require a large number of prior evaluations, further compounding the computational burden.</p>
<p>In general, when we look for sampling or approximating a probability distribution, several approaches arise and may be used within a Bayesian framework. In this work, we focus on variational inference methods. Variational inference seeks to approximate a complex target distribution <span class="math inline">p</span>, —e.g.&nbsp;a posterior— by optimizing over a family of simpler parameterized distributions <span class="math inline">q_{\lambda}</span>. The goal then is to find the distribution <span class="math inline">q_{\lambda^\ast}</span> that is the best approximation of <span class="math inline">p</span> by minimizing a divergence, such as the Kullback-Leibler (KL) divergence. Variational inference methods have been widely adopted in various contexts, including popular models such as Variational Autoencoders (VAEs) <span class="citation" data-cites="kingma2019introduction">Kingma and Welling (<a href="#ref-kingma2019introduction" role="doc-biblioref">2019</a>)</span>, which are a class of generative models where one wants to learn the underlying distribution of data samples. We can also mention normalizing flows <span class="citation" data-cites="papamakarios2021nf">Papamakarios et al. (<a href="#ref-papamakarios2021nf" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="kobyzev_flows">Kobyzev, Prince, and Brubaker (<a href="#ref-kobyzev_flows" role="doc-biblioref">2021</a>)</span>, which consider diffeomorphism transformations to recover the density of the approximated distribution from the simpler one taken as input.</p>
<p>When it resorts to approximate RPs, it is possible to leverage the optimal characteristic of the reference prior (that is, it maximizes the mutual information metric) instead of directly maximizing a divergence between a target and an output. Indeed, the mutual information metric does not depend on the target distribution that we want to reach so iterative derivations of the theoretical RP are not necessary. In <span class="citation" data-cites="nalisnick2017learning">Nalisnick and Smyth (<a href="#ref-nalisnick2017learning" role="doc-biblioref">2017</a>)</span>, the authors propose a variational inference procedure to approximate the RP using a lower bound of the mutual information as an optimization criterion. In <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>, a variational inference procedure is proposed using stochastic gradient descent of the mutual information criterion and illustrated on simple statistical models.</p>
<p>By building on these foundations, this paper proposes a novel variational inference algorithm to compute RPs. As in <span class="citation" data-cites="nalisnick2017learning">Nalisnick and Smyth (<a href="#ref-nalisnick2017learning" role="doc-biblioref">2017</a>)</span> and <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>, the RP is approximated in a parametric family of probability distributions implicitly defined by the push-forward probability distribution through a nonlinear function (see e.g. <span class="citation" data-cites="papamakarios2021nf">Papamakarios et al. (<a href="#ref-papamakarios2021nf" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="Marzouk2016">Marzouk et al. (<a href="#ref-Marzouk2016" role="doc-biblioref">2016</a>)</span>). We will focus in this paper to push-forward probability measures through neural networks. In comparison with the previous works, we benchmark extensively our algorithm on statistical models of different complexity and nature to ensure its robustness. We also extend our algorithm to handle a more general definition of RPs <span class="citation" data-cites="van2023generalized">Van Biesbroeck (<a href="#ref-van2023generalized" role="doc-biblioref">2024a</a>)</span>, where a generalized mutual information criterion is defined using <span class="math inline">f</span>-divergences. In this paper, we restrict the different benchmarks to <span class="math inline">\alpha</span>-divergences. Additionally, we extend the framework to allow the integration of linear constraints on the prior in the pipeline. That last feature permits handling situations where the RP may be improper (i.e.&nbsp;it integrates to infinity). Improper priors pose a challenge because (i) one can not sample from the a priori distribution, and (ii) they do not ensure that the posterior is proper, jeopardizing a posteriori inference. Recent work detailed in <span class="citation" data-cites="van2024constr">Van Biesbroeck (<a href="#ref-van2024constr" role="doc-biblioref">2024b</a>)</span> introduces linear constraints that ensure the proper aspects of RPs. Our algorithm incorporates these constraints, providing a principled way to address improper priors and ensuring that the resulting posterior distributions are well-defined and suitable for practical use.</p>
<p>First, we will introduce the reference prior theory of <span class="citation" data-cites="bernardo1979">Bernardo (<a href="#ref-bernardo1979" role="doc-biblioref">1979b</a>)</span> and the recent developments around generalized reference priors made by <span class="citation" data-cites="van2023generalized">Van Biesbroeck (<a href="#ref-van2023generalized" role="doc-biblioref">2024a</a>)</span> in <a href="#sec-rp_theory" class="quarto-xref">Section&nbsp;2</a>. Next, the variational approximation of the reference prior (VA-RP) methodology is detailed in <a href="#sec-VA-RP" class="quarto-xref">Section&nbsp;3</a>. A stochastic gradient algorithm is proposed, as well as an augmented Lagrangian algorithm for the constrained optimization problem, for learning the parameters of an implicitly defined probability density function that will approximate the reference prior. Moreover, a mindful trick to sample from the posterior distribution by MCMC using the implicitly defined prior distribution is proposed. In <a href="#sec-numexp" class="quarto-xref">Section&nbsp;4</a>, different numerical experiments from various test cases are carried out in order to benchmark the VA-RP. Analytical statistical models where the true asymptotic RP is known are tested to allow comparison between the VA-RP and the true asymptotic RP.</p>
</section>
<section id="sec-rp_theory" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Reference priors theory</h1>
<p>The reference prior theory fits into the usual framework of statistical inference. The situation is the following: we observe i.i.d data samples <span class="math inline">\mathbf{X}= (X_1,..., X_N) \in \mathcal{X}^N</span> with <span class="math inline">\mathcal{X} \subset \mathbb{R}^d</span>. We suppose that the likelihood function <span class="math inline">L_N(\mathbf{X} \, | \, \theta) = \prod_{i=1}^N L(X_i \, | \, \theta)</span> is known and <span class="math inline">\theta \in \Theta \subset \mathbb{R}^q</span> is the parameter we want to infer. Since we use the Bayesian framework, <span class="math inline">\theta</span> is considered to be a random variable with a prior distribution <span class="math inline">\pi</span>. We also define the marginal likelihood <span class="math inline">p_{\pi, N}(\mathbf{X}) = \int_{\Theta}\pi(\theta)L_N(\mathbf{X}\, | \, \theta)d\theta</span> associated to the marginal probability measure <span class="math inline">\mathbb{P}_{\pi, N}</span>. The non-asymptotic RP, first introduced in <span class="citation" data-cites="Bernardo1979a">Bernardo (<a href="#ref-Bernardo1979a" role="doc-biblioref">1979a</a>)</span> and formalized in <span class="citation" data-cites="berger2009formal">Berger, Bernardo, and Sun (<a href="#ref-berger2009formal" role="doc-biblioref">2009</a>)</span>, is defined to be one of the priors verifying:</p>
<p><span id="eq-RPdef"><span class="math display">
    \pi^\ast \in \underset{\pi \in \mathcal{P}}{\operatorname{argmax}} \, I(\pi; L_N) \ ,
\tag{1}</span></span> where <span class="math inline">\mathcal{P}</span> is a class of admissible probability distributions and <span class="math inline">I(\pi; L_N)</span> is the mutual information for the prior <span class="math inline">\pi</span> and the likelihood <span class="math inline">L_N</span> between the random variable of the parameters <span class="math inline">\theta \sim \pi</span> and the random variable of the data <span class="math inline">\mathbf{X}\sim \mathbb{P}_{\pi, N}</span>: <span id="eq-mutualinfoberger"><span class="math display">
    I(\pi; L_N) = \int_{\mathcal{X}^N}{\rm KL}(\pi(\cdot \,|\, \mathbf{X}) \, || \,  \pi) p_{\pi,N}(\mathbf{X})d\mathbf{X}
\tag{2}</span></span> Hence, <span class="math inline">\pi^\ast</span> is a prior that maximizes the Kullback-Leibler divergence between itself and its posterior averaged by the marginal distribution of datasets. The Kullback-Leibler divergence between two probability measures of density <span class="math inline">p</span> and <span class="math inline">q</span> defined on a generic set <span class="math inline">\Omega</span> writes: <span class="math display">
    {\rm KL}(p\,||\,q) = \int_\Omega \log\left(\frac{p(\omega)}{q(\omega)}\right) p(\omega)d\omega.
</span> Thus, <span class="math inline">\pi^\ast</span> is the prior that maximises the influence of the data on the posterior distribution, justifying its reference (or objective) properties. The RP <span class="math inline">\pi^\ast</span> can also be interpreted using channel coding and information theory <span class="citation" data-cites="MacKay2003">MacKay (<a href="#ref-MacKay2003" role="doc-biblioref">2003</a>)</span> (chapter 9). Indeed, remark that <span class="math inline">I(\pi; L_N)</span> corresponds to the mutual information <span class="math inline">I(\theta, \mathbf{X})</span> between the random variable <span class="math inline">\theta \sim \pi</span> and the data <span class="math inline">\mathbf{X}\sim \mathbb{P}_{\pi, N}</span>, it measures the information that conveys the data <span class="math inline">\mathbf{X}</span> about the parameters <span class="math inline">\theta</span>. The maximal value of this mutual information is defined as the channel’s capacity. The RP thus corresponds to the prior distribution that maximizes the information about <span class="math inline">\theta</span> conveyed by the data <span class="math inline">\mathbf{X}</span>.</p>
<p>Using Fubini’s theorem and Bayes’ theorem, we can derive an alternative and more practical expression for the mutual information : <span id="eq-mutualinfo"><span class="math display">
    I(\pi; L_N) = \int_{\Theta}{\rm KL}(L_N(\cdot \, | \, \theta)||p_{\pi,N}) \pi(\theta)d\theta.
\tag{3}</span></span> A more generalized definition of RPs has been proposed in <span class="citation" data-cites="van2023generalized">Van Biesbroeck (<a href="#ref-van2023generalized" role="doc-biblioref">2024a</a>)</span> using <span class="math inline">f</span>-divergences. The <span class="math inline">f</span>-divergence mutual information is defined by <span id="eq-mutual-info-Df"><span class="math display">
    I_{\mathrm{D}_f}(\pi; L_N) = \int_{\Theta}{\mathrm{D}_f}(p_{\pi,N}||L_N(\cdot \, | \, \theta)) \pi(\theta)d\theta \ ,
\tag{4}</span></span> with</p>
<p><span class="math display">
    \mathrm{D}_f(p\,||\,q) = \int_{\Omega} f\left(\frac{p(\omega)}{q(\omega)}\right) q(\omega)d\omega. \ ,
</span> where <span class="math inline">f</span> is usually chosen to be a convex function mapping 1 to 0. Remark that the classical mutual information is obtained by choosing <span class="math inline">f = -\log</span>, indeed, <span class="math inline">\mathrm{D}_{-\log}(p\,||\,q) = {\rm KL}(q\,||\,p)</span>. The formal RP is defined as <span class="math inline">N</span> goes to infinity, but since we want to develop an algorithm to approximate the distribution of the RP, we are restricted to the case where <span class="math inline">N</span> takes a finite value. However, the limit case <span class="math inline">N \rightarrow +\infty</span> is relevant because it has been shown in <span class="citation" data-cites="clarke1994jeffreys">Clarke and Barron (<a href="#ref-clarke1994jeffreys" role="doc-biblioref">1994</a>)</span>, <span class="citation" data-cites="van2023generalized">Van Biesbroeck (<a href="#ref-van2023generalized" role="doc-biblioref">2024a</a>)</span> that the solution of this asymptotic problem is the Jeffreys prior when the mutual information is expressed as in <a href="#eq-mutualinfoberger" class="quarto-xref">Equation&nbsp;2</a>, or when it is defined using an <span class="math inline">\alpha</span>-divergence, as in <a href="#eq-mutual-info-Df" class="quarto-xref">Equation&nbsp;4</a> with <span class="math inline">f=f_\alpha</span>, where:</p>
<p><span id="eq-falpha"><span class="math display">
    f_\alpha(x) = \frac{x^{\alpha}-\alpha x -(1-\alpha)}{\alpha(\alpha-1)}, \quad \alpha\in(0,1).
\tag{5}</span></span> The Jeffreys prior, denoted by <span class="math inline">J</span>, is defined as follows: <span class="math display">
J(\theta) \propto \det (\mathcal{I}(\theta))^{1/2} \quad \text{with} \quad \mathcal{I}(\theta) = - \int_{\mathcal{X}^N} \frac{\partial^2 \log L_N}{\partial \theta^2}(\mathbf{X}\, | \, \theta)\cdot L_N(\mathbf{X}\, | \, \theta) \, d\mathbf{X}.
</span> We suppose that the likelihood function is smooth such that the Fisher information matrix <span class="math inline">\mathcal I</span> is well-defined. The Jeffreys prior and the RP have the relevant property to be ``invariant by reparametrization’’: <span class="math display">
\forall \varphi \,\, \text{diffeomorphism} ,  \quad J(\theta) = \left| \frac{\partial \varphi}{\partial \theta} \right| \cdot J(\varphi(\theta)) .
</span> This property expresses non-information in the sense that if there is no information on <span class="math inline">\theta</span>, there should not be more information on <span class="math inline">\varphi(\theta)</span> when <span class="math inline">\varphi</span> is a diffeomorphism: an invertible and differentiable transformation.</p>
<p>Actually, the historical definition of RPs involves the KL-divergence in the definition of the mutual information. Yet the use of <span class="math inline">\alpha</span>-divergences instead is relevant because they can be seen as a continuous path between the KL-divergence and the Reverse-KL-divergence when <span class="math inline">\alpha</span> varies from <span class="math inline">0</span> to <span class="math inline">1</span>. We can also mention that for <span class="math inline">\alpha = 1/2</span>, the <span class="math inline">\alpha</span>-divergence is the squared Hellinger distance whose square root is a metric since it is symmetric and verifies the triangle inequality.</p>
<p>Technically, the formal RP is constructed such that its projection on every compact subset (or open subset in <span class="citation" data-cites="mure2018objective">Muré (<a href="#ref-mure2018objective" role="doc-biblioref">2018</a>)</span>) of <span class="math inline">\Theta</span> maximizes asymptotically the mutual information, which allows for improper distributions to be RPs in some cases. The Jeffreys prior is itself often improper.</p>
<p>In our algorithm we consider probability distributions defined on the space <span class="math inline">\Theta</span> and not on sequences of subsets. A consequence of this statement is that our algorithm may tend to approximate improper priors in some cases. Indeed, any given sample by our algorithm results, by construction, from a proper distribution, which is expected to be a good approximation of the solution of the optimization problem expressed in <a href="#eq-RPdef" class="quarto-xref">Equation&nbsp;1</a>. If <span class="math inline">N</span> is large enough, the latter should be close to the —potentially improper— theoretical RP. This approach is justified to some extent since in the context of Q-vague convergence defined in <span class="citation" data-cites="Bioche2016">Bioche and Druilhet (<a href="#ref-Bioche2016" role="doc-biblioref">2016</a>)</span> for instance, improper priors can be the limit of sequences of proper priors. Although this theoretical notion of convergence is defined, no concrete metric is given, making quantification of the difference between proper and improper priors infeasible in practice. Furthermore, as mentioned in the introduction, improper priors can also compromise the validity of {a posteriori} estimates in some cases. To address this issue, we adapted our algorithm to handle the developments made in <span class="citation" data-cites="van2024constr">Van Biesbroeck (<a href="#ref-van2024constr" role="doc-biblioref">2024b</a>)</span>, which suggest a method to define proper RPs by simply resolving a constrained version of the initial optimization problem: <span id="eq-def_const_RP"><span class="math display">
    \tilde\pi^* \in \underset{\substack{\pi \, \text{prior}\\ \text{s.t.}\,\mathcal{C}(\pi)&lt;\infty}}{\operatorname{argmax}} I_{\mathrm{D}_{f_\alpha}}(\pi; L_N),
\tag{6}</span></span> where <span class="math inline">\mathcal{C}(\pi)</span> defines a constraint of the form <span class="math inline">\int_\Theta a(\theta)\pi(\theta)d\theta</span>, <span class="math inline">a</span> being a positive function. When the mutual information in the above optimization problem is defined from an <span class="math inline">\alpha</span>-divergence, and when <span class="math inline">a</span> verifies <span id="eq-condtitions_a"><span class="math display">
    \int_\Theta J(\theta)a(\theta)^{1/\alpha}d\theta&lt;\infty\quad \text{and}\quad \int_\Theta J(\theta)a(\theta)^{1+1/\alpha}d\theta&lt;\infty,
\tag{7}</span></span> the author has proven that the constrained RP <span class="math inline">\tilde\pi^\ast</span> asymptotically takes the following form:</p>
<p><span class="math display">
    \tilde\pi^\ast(\theta) \propto J(\theta)a(\theta)^{1/\alpha},
</span> which is proper.</p>
</section>
<section id="sec-VA-RP" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Variational approximation of the reference prior (VA-RP)</h1>
<section id="implicitly-defined-parametric-probability-distributions-using-neural-networks" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="implicitly-defined-parametric-probability-distributions-using-neural-networks"><span class="header-section-number">3.1</span> Implicitly defined parametric probability distributions using neural networks</h2>
<p>Variational inference refers to techniques that aim to approximate a probability distribution by solving an optimization problem —that often takes a variational form, such as maximizing evidence lower bound (ELBO) <span class="citation" data-cites="Kingma2014">Kingma and Welling (<a href="#ref-Kingma2014" role="doc-biblioref">2014</a>)</span>. It is thus relevant to consider them for approximating RPs, as the goal is to maximize, w.r.t. the prior, the mutual information defined in <a href="#eq-mutualinfo" class="quarto-xref">Equation&nbsp;3</a>.</p>
<p>We restrict the set of priors to a parametric space <span class="math inline">\{\pi_\lambda,\,\lambda\in\Lambda\}</span>, <span class="math inline">\Lambda\subset\mathbb{R}^L</span>, reducing the original optimization problem into a finite-dimensional one. The optimization problem in <a href="#eq-RPdef" class="quarto-xref">Equation&nbsp;1</a> or <a href="#eq-def_const_RP" class="quarto-xref">Equation&nbsp;6</a> becomes finding <span class="math inline">\underset{\lambda\in\Lambda}{\operatorname{argmax}} \, I_{\mathrm{D}_f}(\pi_\lambda; L_N)</span>. Our approach is to define the set of priors <span class="math inline">\pi_\lambda</span> implicitly, as in <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>: <span class="math display">
\theta \sim \pi_{\lambda} \iff \theta = g(\lambda,\varepsilon) \quad \text{and} \quad \varepsilon \sim \mathbb{P}_{\varepsilon}.
</span> Here, <span class="math inline">g</span> is a measurable function parameterized by <span class="math inline">\lambda</span>, typically a neural network with <span class="math inline">\lambda</span> corresponding to its weights and biases, and we impose that <span class="math inline">g</span> is differentiable with respect to <span class="math inline">\lambda</span>. The variable <span class="math inline">\varepsilon</span> can be seen as a latent variable. It has an easy-to-sample distribution <span class="math inline">\mathbb{P}_{\varepsilon}</span> with a simple density function. In practice we use the centered multivariate Gaussian <span class="math inline">\mathcal{N}(0,\mathbb{I}_{p\times p})</span>. The construction described above allows the consideration of a vast family of priors. However, except in very simple cases, the density of <span class="math inline">\pi_\lambda</span> is not known and cannot be evaluated. Only samples of <span class="math inline">\theta\sim\pi_\lambda</span> can be obtained.</p>
<p>In the work of <span class="citation" data-cites="nalisnick2017learning">Nalisnick and Smyth (<a href="#ref-nalisnick2017learning" role="doc-biblioref">2017</a>)</span>, this implicit sampling method is compared to several other algorithms used to learn RPs in the case of one-dimensional models. Among these methods, we can mention an algorithm proposed by <span class="citation" data-cites="berger2009formal">Berger, Bernardo, and Sun (<a href="#ref-berger2009formal" role="doc-biblioref">2009</a>)</span> which does not sample from the RP but only evaluates it for specific points, or an MCMC-based approach by <span class="citation" data-cites="lafferty2013iterative">Lafferty and Wasserman (<a href="#ref-lafferty2013iterative" role="doc-biblioref">2001</a>)</span>, which is inspired from the previous one but can sample from the RP.</p>
<p>According to this comparison, implicit sampling is, in the worst case, competitive with the other methods, but achieves state-of-the-art results in the best case. Hence, computing the variational approximation of the RP, which we will refer to as the VA-RP, seems to be a promising technique.</p>
<p>The situations presented by <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span> and <span class="citation" data-cites="nalisnick2017learning">Nalisnick and Smyth (<a href="#ref-nalisnick2017learning" role="doc-biblioref">2017</a>)</span> are in dimension one and use the Kullback-Leibler divergence within the definition of the mutual information.</p>
<p>The construction of the algorithm that we propose in the following accommodates multi-dimensional modeling. It is also compatible with the extended form of the mutual information, as defined in <a href="#eq-mutualinfo" class="quarto-xref">Equation&nbsp;3</a> from an <span class="math inline">f</span>-divergence.</p>
<p>The choice of the neural network is up to the user, we will showcase in our numerical applications simple networks, composed of one fully connected linear layer and one activation function. However, the method can be used with deeper networks, such as normalizing flows <span class="citation" data-cites="papamakarios2021nf">Papamakarios et al. (<a href="#ref-papamakarios2021nf" role="doc-biblioref">2021</a>)</span>, or larger networks obtained through a mixture model of smaller networks utilizing the ``Gumbel-Softmax trick’’ <span class="citation" data-cites="jang2017categorical">Jang, Gu, and Poole (<a href="#ref-jang2017categorical" role="doc-biblioref">2017</a>)</span> for example. Such choices lead to more flexible parametric distributions, but increase the difficulty of fine-tuning hyperparameters.</p>
</section>
<section id="sec-sga" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-sga"><span class="header-section-number">3.2</span> Learning the VA-RP using stochastic gradient algorithm</h2>
<p>The VA-RP is formulated as the solution to the following optimization problem: <span id="eq-opti_pb_pilambda"><span class="math display">
    \pi_{\lambda^\ast}=\underset{\lambda\in\Lambda}{\operatorname{argmax}} \, \mathcal{O}_{\mathrm{D}_f}(\pi; L_N),
\tag{8}</span></span> where <span class="math inline">\pi_\lambda</span> is parameterized through the relation between a latent variable <span class="math inline">\varepsilon</span> and the parameter <span class="math inline">\theta</span>, as outlined in the preceding Section. The function <span class="math inline">\mathcal{O}_{\mathrm{D}_f}</span> is called the objective function, it is maximized using stochastic gradient optimization, following the approach described in <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>.</p>
<p>It is intuitive to fix <span class="math inline">\mathcal{O}_{\mathrm{D}_f}</span> to equal <span class="math inline">I_{\mathrm{D}_f}</span>, in order to maximize the mutual information of interest. In this Section, we suggest alternative objective functions that can be considered to compute the VA-RP. Our method is adaptable to any objective function <span class="math inline">\mathcal{O}_{\mathrm{D}_f}</span> admitting a gradient w.r.t. <span class="math inline">\lambda=(\lambda_1,\dots,\lambda_L)</span> that takes the form <span id="eq-compatible_objective_function"><span class="math display">
    \frac{\partial \mathcal{O}_{\mathrm{D}_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{\mathcal{O}}_{\mathrm{D}_f}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right]
\tag{9}</span></span> for any <span class="math inline">l\in\{1,\dots,L\}</span>, where <span class="math inline">\tilde{\mathcal O}_{\mathrm{D}_f}</span> is independent of <span class="math inline">\lambda</span>. This framework allows for flexible implementation, as it permits the separation of sampling and differentiation operations:</p>
<ul>
<li>The gradient of <span class="math inline">\tilde{\mathcal{O}}_{\mathrm{D}_f}</span> mostly relies on random sampling and depends only on the likelihood <span class="math inline">L_N</span> and the function <span class="math inline">f</span>.</li>
<li>The gradient of <span class="math inline">g</span> is computed independently. In practice, it is possible to leverage usual differentiation techniques for the neural network. In our work, we rely on PyTorch’s automatic differentiation feature ``autograd’’ <span class="citation" data-cites="torch2019">Paszke et al. (<a href="#ref-torch2019" role="doc-biblioref">2019</a>)</span>.</li>
</ul>
<p>This separation is advantageous as automatic differentiation tools —such as autograd— are well-suited to differentiating complex networks but struggle with functions incorporating randomness.</p>
<p>This way, the optimization problem can be addressed using stochastic gradient optimization, approximating at each step the gradient in <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a> via Monte Carlo estimates. In our experiments, the implementation of the algorithm is done with the popular Adam optimizer (<span class="citation" data-cites="kingma2017adam">Kingma and Ba (<a href="#ref-kingma2017adam" role="doc-biblioref">2015</a>)</span>), with its default hyperparameters, <span class="math inline">\beta_1=0.9</span> and <span class="math inline">\beta_2=0.999</span>. The learning rate is tuned more specifically for each numerical benchmark.</p>
<p>Concerning the choice of objective function, we verify that <span class="math inline">I_{\mathrm{D}_f}</span> is compatible with our method by computing its gradient: <span id="eq-gradientIdf"><span class="math display">
\begin{aligned}
\frac{\partial I_{\mathrm{D}_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) &amp; = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{I}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right] \\
&amp; + \mathbb{E}_{\theta \sim \pi_{\lambda}}\left[ \mathbb{E}_{\mathbf{X}\sim L_N(\cdot |\theta)}\left[ \frac{1}{L_N(\mathbf{X}\,|\,\theta)} \frac{\partial p_{\lambda}}{\partial \lambda_l}(\mathbf{X})f'\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right)\right]  \right],
\end{aligned}
\tag{10}</span></span> where: <span class="math display">
    \frac{\partial \tilde{I}}{\partial \theta_j}(\theta) = \mathbb{E}_{\mathbf{X}\sim L_N(\cdot \,|\,\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\mathbf{X}\,| \,\theta)F \left(\frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)} \right)  \right],
</span> with <span class="math inline">F(x) = f(x)-xf'(x)</span> and <span class="math inline">p_{\lambda}</span> is a shortcut notation for <span class="math inline">p_{\pi_{\lambda}, N}</span> being the marginal distribution under <span class="math inline">\pi_{\lambda}</span>. In <a href="#sec-grad_comp" class="quarto-xref">Section&nbsp;6.1</a>, we provide a detailed derivation of the above equation and show that the second term can be developed to align with the form of <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a>. Remark that only the case <span class="math inline">f=-\log</span> is considered by <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>, but it leads to a simplification of the gradient since the second term vanishes. Each term in the above equations is approximated as follows: <span id="eq-MCgradI"><span class="math display">
\begin{cases} \displaystyle
     p_{\lambda}(\mathbf{X}) = \mathbb{E}_{\theta \sim \pi_{\lambda}}[L_N(\mathbf{X}\, | \, \theta)] \approx \frac{1}{T} \sum_{t=1}^T L_N(\mathbf{X}\, | \, g(\lambda, \varepsilon_t)) \quad \text{where} \quad \varepsilon_1,...\,, \varepsilon_T \sim \mathbb{P}_{\varepsilon} \\
\displaystyle
     \frac{\partial \tilde{I}}{\partial \theta_j}(\theta) \approx \frac{1}{U} \sum_{u=1}^{U} \frac{\partial \log L_N}{\partial \theta_j}(\mathbf{X}^u \,| \,\theta)F \left(\frac{p_{\lambda}(\mathbf{X}^u)}{L_N(\mathbf{X}^u \,|\, \theta)} \right) \quad \text{where} \quad \mathbf{X}^1,...\,,\mathbf{X}^{U} \sim \mathbb{P}_{\mathbf{X}|\theta}
.\end{cases}  
\tag{11}</span></span></p>
<p>In their work, <span class="citation" data-cites="nalisnick2017learning">Nalisnick and Smyth (<a href="#ref-nalisnick2017learning" role="doc-biblioref">2017</a>)</span> propose an alternative objective function to optimize, that we call <span class="math inline">B_{\mathrm{D}_f}</span>.</p>
<p>This function corresponds to a lower bound of the mutual information. It is derived from an upper bound on the marginal distribution and relies on maximizing the likelihood. Their approach is only presented for <span class="math inline">f=-\log</span>, we generalize the lower bound for any decreasing function <span class="math inline">f</span>: <span id="eq-Bdf"><span class="math display">
    B_{\mathrm{D}_f}(\pi; L_N) = \displaystyle \int_{\Theta}\int_{\mathcal{X}^N}f\left(\frac{L_N(\mathbf{X}\, |\, \hat{\theta}_{MLE})}{L_N(\mathbf{X}\, |\, \theta)}\right)\pi(\theta)L_N(\mathbf{X}\, | \, \theta)d\mathbf{X}d\theta,
\tag{12}</span></span> where <span class="math inline">\hat{\theta}_{MLE}</span> being the maximum likelihood estimator (MLE). It only depends on the likelihood and not on <span class="math inline">\lambda</span> which simplifies the gradient computation: <span class="math display">
\frac{\partial B_{\mathrm{D}_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{B}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right],
</span> where: <span class="math display">
\frac{\partial \tilde{B}}{\partial \theta_j}(\theta) = \mathbb{E}_{\mathbf{X}\sim L_N(\cdot \,|\,\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\mathbf{X}\,|\, \theta)F \left(\frac{L_N(\mathbf{X}\,| \,\hat{\theta}_{MLE})}{L_N(\mathbf{X}\,|\,\theta)} \right)  \right].
</span> Its form corresponds to the one expressed in <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a>.</p>
<p>Given that <span class="math inline">p_{\lambda}(\mathbf{X}) \leq \max_{\theta' \in \Theta} L_N(\mathbf{X}\,|\, \theta') = L_N(\mathbf{X}\,|\,\hat{\theta}_{MLE})</span> for all <span class="math inline">\lambda</span>, we have <span class="math inline">B_{\mathrm{D}_f}(\pi_{\lambda}; L_N) \leq I_{\mathrm{D}_f}(\pi_{\lambda};L_N)</span>.</p>
<p>Since <span class="math inline">f_\alpha</span>, used in <span class="math inline">\alpha</span>-divergence (<a href="#eq-falpha" class="quarto-xref">Equation&nbsp;5</a>), is not decreasing, we replace it with <span class="math inline">\hat{f}_\alpha</span> defined hereafter, because <span class="math inline">\mathrm{D}_{f_\alpha}=\mathrm{D}_{\hat{f}_\alpha}</span>:</p>
<p><span class="math display">
  \hat{f}_{\alpha}(x) = \frac{x^{\alpha}-1}{\alpha(\alpha-1)} = f_{\alpha}(x) + \frac{1}{\alpha-1}(x-1) .
</span> The use of this function results in a more stable computation overall. Moreover, one argument for the use of <span class="math inline">\alpha</span>-divergences rather that the KL-divergence, is that we have an universal and explicit upper bound on the mutual information: <span class="math display">
I_{\mathrm{D}_{f_\alpha}}(\pi; L_N) = I_{\mathrm{D}_{\hat{f}_{\alpha}}}(\pi ; L_N) \leq \hat{f}_{\alpha}(0) = \frac{1}{\alpha(1-\alpha)} .  
</span> This bound can be an indicator on how well the mutual information is optimized, although there is no guarantee that it can be attained in general.</p>
<p>The gradient of the objective function <span class="math inline">B_{\mathrm{D}_f}</span> can be approximated via Monte Carlo, in the same manner as in <a href="#eq-MCgradI" class="quarto-xref">Equation&nbsp;11</a>.</p>
<p>It requires to compute the MLE, which can also be done using samples of <span class="math inline">\varepsilon</span>: <span class="math display">
L_N(\mathbf{X}\, | \, \hat{\theta}_{MLE}) \approx \max_{t\in\{1,\dots,T\}} L_N(\mathbf{X}\, | \, g(\lambda, \varepsilon_t)) \quad \text{where} \quad \varepsilon_1,..., \varepsilon_T \sim \mathbb{P}_{\varepsilon}.
</span></p>
</section>
<section id="adaptation-for-the-constrained-va-rp" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="adaptation-for-the-constrained-va-rp"><span class="header-section-number">3.3</span> Adaptation for the constrained VA-RP</h2>
<p>Reference priors are often criticized, because it can lead to improper posteriors. However, the variational optimization problem defined in <a href="#eq-opti_pb_pilambda" class="quarto-xref">Equation&nbsp;8</a> can be adapted to incorporate simple constraints on the prior. As mentioned in <a href="#sec-rp_theory" class="quarto-xref">Section&nbsp;2</a>, there exist specific constraints that would make the theoretical solution proper.</p>
<p>This is also a way to incorporate expert knowledge to some extent. We consider <span class="math inline">K</span> constraints of the form: <span class="math display">
\forall \, k \in \{1,\ldots,K\} \text{,}\, \,   \, \mathcal{C}_k(\pi_{\lambda}) = \mathbb{E}_{\theta \sim \pi_{\lambda}} \left[ a_k(\theta) \right] - b_k,
</span> with <span class="math inline">a_k</span> : <span class="math inline">\Theta \mapsto \mathbb{R}^+</span> integrable and linearly independent functions, and <span class="math inline">b_k \in \mathbb{R}</span>. We then adapt the optimization problem in <a href="#eq-opti_pb_pilambda" class="quarto-xref">Equation&nbsp;8</a> to propose the following constrained optimization problem: <span class="math display">
\begin{aligned}
&amp; \pi^C_{\lambda^\ast} \in \underset{\lambda \in \Lambda}{\operatorname{argmax}} \, \mathcal{O}_{\mathrm{D}_f}(\pi_{\lambda}; L_N) \\
&amp; \text{subject to} \quad \forall \,k \in \{1, \ldots, K\}, \, \,   \, \mathcal{C}_k(\pi_{\lambda}) = 0,
\end{aligned}
</span> where <span class="math inline">\pi^C_{\lambda^\ast}</span> is the constrained VA-RP. The optimization problem with the mutual information has an explicit asymptotic solution for proper priors verifying the previous conditions:</p>
<ul>
<li><p>In the case of the KL-divergence (<span class="citation" data-cites="bernardo2005reference">Bernardo (<a href="#ref-bernardo2005reference" role="doc-biblioref">2005</a>)</span>): <span class="math display">
\pi^C(\theta) \propto J(\theta) \exp \left( 1 + \sum_{k=1}^K \nu_k a_k(\theta)  \right) .  
</span></p></li>
<li><p>In the case of <span class="math inline">\alpha</span>-divergences (<span class="citation" data-cites="van2024constr">Van Biesbroeck (<a href="#ref-van2024constr" role="doc-biblioref">2024b</a>)</span>): <span class="math display">
\pi^C(\theta) \propto J(\theta)  \left( 1 + \sum_{k=1}^K \nu_k a_k(\theta)  \right)^{1/\alpha} .
</span></p></li>
</ul>
<p>where <span class="math inline">\nu_1,\dots, \nu_K \in \mathbb{R}</span> are constants determined by the constraints.</p>
<p>Recent work by <span class="citation" data-cites="van2024constr">Van Biesbroeck (<a href="#ref-van2024constr" role="doc-biblioref">2024b</a>)</span> makes it possible to build a proper reference prior under a relevant constraint function with <span class="math inline">\alpha</span>-divergence. The theorem considers <span class="math inline">a:\Theta\mapsto\mathbb{R}^+</span> which verifies the conditions expressed in <a href="#eq-condtitions_a" class="quarto-xref">Equation&nbsp;7</a>. Letting <span class="math inline">\mathcal{P}_a</span> be the set of priors <span class="math inline">\pi</span> on <span class="math inline">\Theta</span> such that <span class="math inline">\pi\cdot a\in L^1</span>, the reference prior <span class="math inline">\tilde\pi^\ast</span> under the constraint <span class="math inline">\tilde\pi^\ast\in\mathcal{P}_a</span> is:</p>
<p><span class="math display">
  \tilde\pi^\ast(\theta) \propto J(\theta)a(\theta)^{1/\alpha} .
</span></p>
<p>We propose the following general method to approximate the VA-RP under such constraints:</p>
<ul>
<li><p>Compute the VA-RP <span class="math inline">\pi_{\lambda} \approx J</span>, in the same manner as for the unconstrained case.</p></li>
<li><p>Estimate the constants <span class="math inline">\mathcal{K}</span> and <span class="math inline">c</span> using Monte Carlo samples from the VA-RP, as: <span class="math display">
\mathcal{K}_{\lambda} = \int_{\Theta} \pi_{\lambda}(\theta)a(\theta)^{1/\alpha}d\theta   \approx \int_{\Theta} J(\theta)a(\theta)^{1/\alpha}d\theta = \mathcal{K},
</span> <span class="math display">
c_{\lambda} =  \int_{\Theta} \pi_{\lambda}(\theta)a(\theta)^{1+(1/\alpha)}d\theta \approx \int_{\Theta} J(\theta)a(\theta)^{1+(1/\alpha)}d\theta = c .
</span></p></li>
<li><p>Since we have the equality: <span class="math display">
\mathbb{E}_{\theta \sim \tilde\pi^\ast}[a(\theta)] = \int_{\Theta} \tilde\pi^\ast(\theta)a(\theta)d\theta = \frac{1}{\mathcal{K}}\int_{\Theta}J(\theta)a(\theta)^{1+(1/\alpha)}d\theta  = \frac{c}{\mathcal{K}},  
</span></p></li>
</ul>
<p>we compute the constrained VA-RP using the constraint : <span class="math inline">\mathbb{E}_{\theta \sim \pi_{\lambda'}}[a(\theta)] = c_{\lambda} / \mathcal{K}_{\lambda}</span> to approximate <span class="math inline">\pi_{\lambda'} \approx \tilde\pi^\ast</span>.</p>
<p>One might use different variational approximations for <span class="math inline">\pi_{\lambda}</span> and <span class="math inline">\pi_{\lambda'}</span> because <span class="math inline">J</span> and <span class="math inline">\tilde\pi^\ast</span> could have very different forms depending on the function <span class="math inline">a</span>.</p>
<p>The idea is to solve the constrained optimization problem as an unconstrained problem but with a Lagrangian as the objective function. We take the work of <span class="citation" data-cites="nocedal2006penalty">Nocedal and Wright (<a href="#ref-nocedal2006penalty" role="doc-biblioref">2006</a>)</span> as support.</p>
<p>We denote <span class="math inline">\eta</span> the Lagrange multiplier. Instead of using the usual Lagrangian function, <span class="citation" data-cites="nocedal2006penalty">Nocedal and Wright (<a href="#ref-nocedal2006penalty" role="doc-biblioref">2006</a>)</span> suggest adding a term defined with <span class="math inline">\tilde{\eta}</span>, a vector with positive components which serve as penalization coefficients, and <span class="math inline">\eta'</span> which can be thought of a prior estimate of <span class="math inline">\eta</span>, although not in a Bayesian sense. The objective is to find a saddle point <span class="math inline">(\lambda^*, \eta^*)</span> which is a solution of the updated optimization problem: <span class="math display">
\max_{\lambda} \, \left(\min_{\eta} \,  \mathcal{O}_{\mathrm{D}_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k \mathcal{C}_k(\pi_{\lambda})  + \sum_{k=1}^K \frac{1}{2\tilde{\eta}_k} ( \eta_k - \eta_k')^2 \right)   .  
</span> One can see that the third term serves as a penalization for large deviations from <span class="math inline">\eta'</span>. The minimization on <span class="math inline">\eta</span> is feasible because it is a convex quadratic, and we get <span class="math inline">\eta = \eta' - \tilde{\eta} \cdot \mathcal{C}(\pi_\lambda)</span>. Replacing <span class="math inline">\eta</span> by its expression leads to the resolution of the problem: <span class="math display">
  \max_{\lambda} \, \mathcal{O}_{\mathrm{D}_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k' \mathcal{C}_k(\pi_{\lambda}) - \sum_{k=1}^K \frac{\tilde{\eta}_k}{2} \mathcal{C}_k(\pi_{\lambda})^2 .
  </span> This motivates the definition of the augmented Lagrangian: <span class="math display">  
\mathcal{L}_A(\lambda, \eta, \tilde{\eta}) = \mathcal{O}_{\mathrm{D}_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k \mathcal{C}_k(\pi_{\lambda}) - \sum_{k=1}^K \frac{\tilde{\eta}_k}{2} \mathcal{C}_k(\pi_{\lambda})^2 .
</span> Its gradient has a form that which is compatible with our algorithm, as depicted in <a href="#sec-sga" class="quarto-xref">Section&nbsp;3.2</a> (see <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a>):</p>
<p><span class="math display">
\begin{aligned}
\frac{\partial \mathcal{L}_A}{\partial \lambda}(\lambda, \eta, \tilde{\eta}) &amp; = \frac{\partial \mathcal{O}_{\mathrm{D}_f}}{\partial \lambda}(\pi_{\lambda}; L_N) +  \mathbb{E}_{\varepsilon}\left[ \left(\sum_{k=1}^K  \frac{\partial a_k}{\partial \theta}(g(\lambda, \varepsilon))(\eta_k - \tilde{\eta}_k\mathcal{C}_k(\pi_{\lambda}))\right)\frac{\partial g}{\partial \lambda}(\lambda,\varepsilon)\right] \\
&amp; = \mathbb{E}_{\varepsilon}\left[ \left( \frac{\partial \tilde{\mathcal{O}}}{\partial \theta}(g(\lambda,\varepsilon))  + \sum_{k=1}^K  \frac{\partial a_k}{\partial \theta}(g(\lambda, \varepsilon))(\eta_k - \tilde{\eta}_k\mathcal{C}_k(\pi_{\lambda}))\right)\frac{\partial g}{\partial \lambda}(\lambda,\varepsilon)  \right] .
\end{aligned}
</span> In practice, the augmented Lagrangian algorithm is of the form: <span class="math display">
   \begin{cases}
    \lambda^{t+1} = \underset{\lambda}{\operatorname{argmax}} \, \mathcal{L}_A(\lambda, \eta^t, \tilde{\eta}) \\
    \forall k \in \{1, \ldots, K\}, \, \eta_k^{t+1} = \eta_k^t - \tilde{\eta}_k\cdot \mathcal{C}_k(\pi_{\lambda^{t+1}}).
\end{cases}
</span> In our implementation, <span class="math inline">\eta</span> is updated every <span class="math inline">100</span> epochs. The penalty parameter <span class="math inline">\tilde{\eta}</span> can be interpreted as the learning rate of <span class="math inline">\eta</span>, we use an adaptive scheme inspired by <span class="citation" data-cites="basir2023adaptive">Basir and Senocak (<a href="#ref-basir2023adaptive" role="doc-biblioref">2023</a>)</span> where we check if the largest constraint value <span class="math inline">|| \mathcal{C}(\pi_{\lambda}) ||_{\infty}</span> is higher than a specified threshold <span class="math inline">M</span> or not. If <span class="math inline">|| \mathcal{C}(\pi_{\lambda}) ||_{\infty} &gt; M</span>, we multiply <span class="math inline">\tilde{\eta}</span> by <span class="math inline">v</span>, otherwise we divide by <span class="math inline">v</span>. We also impose a maximum value <span class="math inline">\tilde{\eta}_{max}</span>.</p>
</section>
<section id="sec-posterio-sampling" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-posterio-sampling"><span class="header-section-number">3.4</span> Posterior sampling using implicitly defined prior distributions</h2>
<p>Although our main object of study is the prior distribution, one needs to find the posterior distribution given an observed dataset <span class="math inline">\mathbf{X}</span> in order to do the inference on <span class="math inline">\theta</span>. The posterior is of the form : <span class="math display">
\pi_{\lambda}(\theta \, | \, \mathbf{X}) = \frac{\pi_{\lambda}(\theta)L_N(\mathbf{X}\, | \, \theta)}{p_{\lambda}(\mathbf{X})} .
</span> As discussed in the introduction, one can approximate the posterior distribution when knowing the prior either by using MCMC or variational inference. In both cases, knowing the marginal distribution is not required. Indeed, MCMC samplers inspired by the Metropolis-Hastings algorithm can be applied, even if the posterior distribution is only known up to a multiplicative constant. The same can be said for variational approximation since the ELBO can be expressed without the marginal.</p>
<p>The issue here is that the density function <span class="math inline">\pi_{\lambda}(\theta)</span> is not explicit and can not be evaluated, except for very simple cases. However, we imposed that the distribution of the variable <span class="math inline">\varepsilon</span> is simple enough so one is able to evaluate its density. We propose to use <span class="math inline">\varepsilon</span> as the variable of interest instead of <span class="math inline">\theta</span> because it lets us circumvent this issue. In practice, the idea is to reverse the order of operations : instead of sampling <span class="math inline">\varepsilon</span>, then transforming <span class="math inline">\varepsilon</span> into <span class="math inline">\theta</span>, which defines the prior on <span class="math inline">\theta</span>, and finally sampling posterior samples of <span class="math inline">\theta</span> given <span class="math inline">X</span>, one can proceed as follows :</p>
<ul>
<li><p>Define the posterior distribution on <span class="math inline">\varepsilon</span> : <span class="math display">
\pi_{\varepsilon,\lambda}(\varepsilon \, | \, \mathbf{X}) = \frac{p_{\varepsilon}(\varepsilon)L_N(\mathbf{X}\, | \, g(\lambda, \varepsilon))}{p_{\lambda}(\mathbf{X})} \ ,
</span> where <span class="math inline">p_{\varepsilon}</span> is the probability density function of <span class="math inline">\varepsilon</span>. <span class="math inline">\pi_{\varepsilon,\lambda}(\varepsilon \, | \, \mathbf{X})</span> is known up to a multiplicative constant since the marginal <span class="math inline">p_{\lambda}</span> is intractable in general. It is indeed a probability distribution on <span class="math inline">\mathbb{R}^p</span> because : <span class="math display">
p_{\lambda}(\mathbf{X}) = \int_{\Theta} \pi_{\lambda}(\theta)L_N(\mathbf{X}\, | \, \theta)d\theta = \int_{\mathbb{R}^p} L_N(\mathbf{X}\, | \, g(\lambda, \varepsilon)) d\mathbb{P}_{\varepsilon}
</span></p></li>
<li><p>Sample posterior <span class="math inline">\varepsilon</span> samples from the previous distribution, approximated by MCMC or variational inference.</p></li>
<li><p>Apply the transformation <span class="math inline">\theta = g(\lambda, \varepsilon)</span>, and one gets posterior <span class="math inline">\theta</span> samples : <span class="math inline">\theta \sim \pi_{\lambda}(\cdot \, | \, \mathbf{X})</span>.</p></li>
</ul>
<p>More precisely, we denote for a fixed dataset <span class="math inline">\mathbf{X}</span> : <span class="math display">
\theta \sim  \tilde{\pi}_{\lambda}(\cdot \, | \, \mathbf{X}) \iff \theta = g(\lambda, \varepsilon) \quad \text{with} \quad \varepsilon \sim \pi_{\varepsilon,\lambda}(\cdot \, | \, \mathbf{X}).
</span></p>
<p>The previous approach is valid because <span class="math inline">\pi_{\lambda}(\cdot \, | \, \mathbf{X})</span> and <span class="math inline">\tilde{\pi}_{\lambda}(\cdot \, | \, \mathbf{X})</span> lead to the same distribution, as proven by the following derivation : let <span class="math inline">\varphi</span> be a bounded and measurable function on <span class="math inline">\Theta</span>.</p>
<p>Using the definitions of the different distributions, we have that: <span class="math display">
\begin{aligned}
    \int_{\Theta} \varphi(\theta) \tilde{\pi}_{\lambda}(\theta \, | \, \mathbf{X}) d\theta &amp; = \int_{\mathbb{R}^p} \varphi(g(\lambda, \varepsilon)) \pi_{\varepsilon,\lambda}(\varepsilon \, | \, \mathbf{X}) d\varepsilon \\
    &amp; = \int_{\mathbb{R}^p} \varphi(g(\lambda, \varepsilon)) \frac{p_{\varepsilon}(\varepsilon)L_N(X\, | \, g(\lambda, \varepsilon))}{p_{\lambda}(\mathbf{X})} d\varepsilon \\
    &amp; = \int_{\Theta} \varphi(\theta)\pi_{\lambda}(\theta) \frac{L_N(\mathbf{X}\, | \, \theta)}{p_{\lambda}(\mathbf{X})}
     d\theta \\
    &amp; = \int_{\Theta} \varphi(\theta) \pi_{\lambda}(\theta \, | \, \mathbf{X}) d\theta.
\end{aligned}
</span> As mentioned in the last Section, when the RP is improper, we compare the posterior distributions, namely, the exact reference posterior when available and the posterior obtained from the VA-RP using the previous method. Altogether, we are able to sample <span class="math inline">\theta</span> from the posterior even if the density of the parametric prior <span class="math inline">\pi_{\lambda}</span> on <span class="math inline">\theta</span> is unavailable due to an implicit definition of the prior distribution.</p>
<p>For our computations, we choose MCMC sampling, namely an adaptive Metropolis-Hastings sampler with a multivariate Gaussian as the proposition distribution. The adaptation scheme is the following: for each batch of iterations, we monitor the acceptance rate and we adapt the variance parameter of the Gaussian proposition in order to have an acceptance rate close to 40%, which is the advised value <span class="citation" data-cites="gelman2013">Gelman et al. (<a href="#ref-gelman2013" role="doc-biblioref">2013</a>)</span> for models in small dimensions. We refer to this algorithm as MH(<span class="math inline">\varepsilon</span>). Because we apply MCMC sampling on variable <span class="math inline">\varepsilon \in \mathbb{R}^p</span> with a reasonable value for <span class="math inline">p</span>, we expect this step of the algorithm to be fast compared to the computation of the VA-RP.</p>
<p>One could also use classic variational inference on <span class="math inline">\varepsilon</span> instead, but the parametric set of distributions must be chosen wisely. In VAEs for instance, multivariate Gaussian are often considered since it simplifies the KL-divergence term in the ELBO. However, this might be too simplistic in our case since we must apply the neural network <span class="math inline">g</span> to recover <span class="math inline">\theta</span> samples. This means that the approximated posterior on <span class="math inline">\theta</span> belongs to a very similar set of distributions to those used for the VA-RP, since we already used a multivariate Gaussian for the prior on <span class="math inline">\varepsilon</span>. On the other hand, applying once again the implicit sampling approach does not exploit the additional information we have on <span class="math inline">\pi_{\varepsilon, \lambda}(\varepsilon \, | \, \mathbf{X})</span> compared to <span class="math inline">\pi_{\lambda}(\theta)</span>, specifically, that its density function is known up to a multiplicative constant. Hence, we argue that using a Metropolis-Hastings sampler is more straightforward in this situation.</p>
</section>
</section>
<section id="sec-numexp" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Numerical experiments</h1>
<p>We want to apply our algorithm to different statistical models, the first one is the multinomial model, which is the simplest in the sense that the target distributions —the Jeffreys prior and posterior— have explicit expressions and are part of a usual parametric family of proper distributions. The second model —the probit model— will be highlighted with supplementary computations, in regards to the assessment of the stability of our stochastic algorithm, and also with the addition of a moment constraint.</p>
<p>The one-dimensional statistical model of the Gaussian distribution with variance parameter is also presented in <a href="#sec-appendix" class="quarto-xref">Section&nbsp;6</a>.</p>
<p>Since we only have to compute quotients of the likelihood or the gradient of the log-likelihood, we can omit the multiplicative constant which does not depend on <span class="math inline">\theta</span>.</p>
<p>As for the output of the neural networks, the activation function just before the output is different for each statistical model, the same can be said for the learning rate. In some cases, we apply an affine transformation on the variable <span class="math inline">\theta</span> to avoid divisions by zero during training. In every test case, we consider simple networks for an easier fine-tuning of the hyperparameters and also because the precise computation of the loss function is an important bottleneck.</p>
<p>For the initialization of the neural networks, biases are set to zero and weights are randomly sampled from a Gaussian distribution. As for the several hyperparameters, we take <span class="math inline">N=10</span>, <span class="math inline">T=50</span> and <span class="math inline">U=1000</span> unless stated otherwise. We take a latent space of dimension <span class="math inline">p=50</span>. For the posterior calculations, we keep the last <span class="math inline">5\cdot 10^4</span> samples from the Markov chain over a total of <span class="math inline">10^5</span> Metropolis-Hastings iterations. Increasing <span class="math inline">N</span> is advised in order to get closer to the asymptotic case for the optimization problem, and increasing <span class="math inline">U</span> and <span class="math inline">T</span> is relevant for the precision of the Monte Carlo estimates. Nevertheless, this increases computation times and we have to do a trade-off between the former and the latter. As for the constrained optimization, we use <span class="math inline">v=2</span>, <span class="math inline">M=0.005</span> and <span class="math inline">\tilde{\eta}_{max} = 10^4</span>.</p>
<section id="multinomial-model" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="multinomial-model"><span class="header-section-number">4.1</span> Multinomial model</h2>
<p>The multinomial distribution can be interpreted as the generalization of the binomial distribution for higher dimensions. We denote : <span class="math inline">X_i \sim \text{Multinomial}(n,(\theta_1,...,\theta_q))</span> with <span class="math inline">n \in \mathbb{N}^*</span>, <span class="math inline">\mathbf{X}\in \mathcal{X}^N</span> and <span class="math inline">\theta \in \Theta</span>, with : <span class="math inline">\mathcal{X} = \{ X \in \{0,...,n\}^q \, | \, \sum_{j = 1}^q X^j = n \}</span> and <span class="math inline">\Theta = \{ \theta \in (0,1)^q \, | \, \sum_{j = 1}^q \theta_j = 1  \}</span>. We use <span class="math inline">n=10</span> and <span class="math inline">q=\text{dim}(\theta)=4</span>.</p>
<p>The likelihood function and the gradient of its logarithm are: <span class="math display">
L_N(\mathbf{X}\,|\,\theta) = \prod_{i=1}^N \frac{n!}{X_i^1 ! \cdot ... \cdot X_i^q !} \prod_{j=1}^q \theta_{j}^{X_i^j} \propto  \prod_{i=1}^N \prod_{j=1}^q \theta_{j}^{X_i^j}
</span></p>
<p><span class="math display">
\forall (i,j), \,  \frac{\partial \log L}{\partial \theta_j}(X_i\,|\,\theta) = \frac{X_i^j}{\theta_j}.
</span></p>
<p>The MLE is available : <span class="math inline">\forall j, \, \hat{\theta}_{MLE}(j) = \frac{1}{nN}\sum_{i=1}^N X_i^j</span> and the Jeffreys prior is the <span class="math inline">\text{Dir}_q \left(\frac{1}{2}, ... , \frac{1}{2} \right)</span> distribution, which is proper. The Jeffreys posterior is a conjugate Dirichlet distribution: <span class="math display">
J_{post}(\theta \, | \, \mathbf{X}) = \text{Dir}_q(\theta; \gamma) \quad \text{with} \quad \gamma_j = \frac{1}{2} + \sum_{i=1}^N X_i^j .
</span> We recall that the probability density function of a Dirichlet distribution is the following: <span class="math display">
\text{Dir}_q(x; \gamma) = \frac{\Gamma(\sum_{j=1}^q \gamma_j)}{\prod_{j=1}^q \Gamma(\gamma_j)} \prod_{j=1}^q x_j^{\gamma_j - 1}.  
</span> For approximating the RP, we opt for a simple neural network with one linear layer and a Softmax activation function assuring that all components are positive and sum to 1. Explicitly, we have that: <span class="math display">
\theta = \text{Softmax}(W^{\top}\varepsilon + b),  
</span></p>
<p>with <span class="math inline">W \in \mathbb{R}^{p,4}</span> the weight matrix and <span class="math inline">b \in \mathbb{R}^4</span> the bias vector. The density function of <span class="math inline">\theta</span> does not have a closed expression. The following results are obtained with <span class="math inline">\alpha=0.5</span> for the divergence and the lower bound is used as the objective function.</p>
<div id="bddf3f53" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters and classes</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">50</span>     <span class="co"># latent space dimension</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">4</span>      <span class="co"># parameter space dimension</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span>     <span class="co"># multinomial parameter</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10</span>     <span class="co"># number of data samples</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> <span class="dv">1000</span>   <span class="co"># nb samples for MC estimation in MI</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">50</span>     <span class="co"># nb samples MC marginal likelihood</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>Multinom <span class="op">=</span> torch_MultinomialModel(n<span class="op">=</span>n, q<span class="op">=</span>q)  </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> p  </span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> q</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Multinomial_results.pkl'</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>low <span class="op">=</span> <span class="fl">0.001</span>          <span class="co"># lower bound (must verifiy q * low &lt; 1)</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>upp <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> low<span class="op">*</span>(q<span class="op">-</span><span class="dv">1</span>)  <span class="co"># upper bound (forced value with low and q)</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">0</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> AffineTransformation(NN, m_low<span class="op">=</span>low, M_upp<span class="op">=</span>upp)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Multinom)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, use_log_lik<span class="op">=</span><span class="va">True</span>, use_baseline<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    theta_sample_init <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>loss_fct <span class="op">=</span> <span class="st">"LB_MI"</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>num_samples_MI <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>freq_MI <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>save_best_param <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0025</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_Multinom :</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training loop</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    MI, range_MI, lower_MI, upper_MI <span class="op">=</span> Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>                                                            num_samples_MI, freq_MI, save_best_param, </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>                                                            learning_rate, momentum<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        theta_sample <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        jeffreys_sample <span class="op">=</span> Div.model.sample_Jeffreys(n_samples<span class="op">=</span>n_samples_prior).numpy()</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    all_params <span class="op">=</span> torch.cat([param.view(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> param <span class="kw">in</span> NN.parameters()])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample data from 'true' parameter</span></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>    theta_true <span class="op">=</span> torch.tensor(q<span class="op">*</span>[<span class="dv">1</span><span class="op">/</span>q])  </span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> Multinom.sample(theta_true, N, <span class="dv">1</span>)</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> D[:, <span class="dv">0</span>]</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior samples using Jeffreys prior</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>    n_samples_post <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    jeffreys_post <span class="op">=</span> Multinom.sample_post_Jeffreys(X, n_samples_post)</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>    T_mcmc <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>    sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>    eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> theta_MH.numpy()</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>        jeffreys_post <span class="op">=</span> jeffreys_post.numpy()</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    theta_post <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:,:]</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saves all relevant quantities</span></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>    Mutual_Info <span class="op">=</span> {<span class="st">'values'</span> : MI, <span class="st">'range'</span> : range_MI, <span class="st">'lower'</span> : lower_MI, <span class="st">'upper'</span> : upper_MI}</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>    Prior <span class="op">=</span> {<span class="st">'samples'</span> : theta_sample, <span class="st">'jeffreys'</span> : jeffreys_sample, <span class="st">'params'</span> : all_params}</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>    Posterior <span class="op">=</span> {<span class="st">'samples'</span> : theta_post, <span class="st">'jeffreys'</span> : jeffreys_post}</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a>    Multinom_results <span class="op">=</span> {<span class="st">'MI'</span> : Mutual_Info, <span class="st">'prior'</span> : Prior, <span class="st">'post'</span> : Posterior}</span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>        pickle.dump(Multinom_results, <span class="bu">file</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>        MI_dic <span class="op">=</span> res[<span class="st">'MI'</span>]</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>        Prior <span class="op">=</span> res[<span class="st">'prior'</span>] </span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>        Posterior <span class="op">=</span> res[<span class="st">'post'</span>]</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI <span class="op">=</span> MI_dic[<span class="st">'values'</span>], MI_dic[<span class="st">'range'</span>], MI_dic[<span class="st">'lower'</span>], MI_dic[<span class="st">'upper'</span>]</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>        theta_sample, jeffreys_sample, all_params <span class="op">=</span> Prior[<span class="st">'samples'</span>], Prior[<span class="st">'jeffreys'</span>], Prior[<span class="st">'params'</span>]</span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>        theta_post, jeffreys_post <span class="op">=</span> Posterior[<span class="st">'samples'</span>], Posterior[<span class="st">'jeffreys'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-multi_mi" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'-'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'*'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in range(len(range_MI)):</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='lightgrey')</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, upper_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, lower_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>plt.fill_between(range_MI, lower_MI, upper_MI, color<span class="op">=</span><span class="st">'lightgrey'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Epochs"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Generalized mutual information"</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.yscale('log')</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-multi_mi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multi_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-multi_mi-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Monte Carlo estimation of the generalized mutual information with <span class="math inline">\alpha=0.5</span> (from 200 samples) for <span class="math inline">\pi_{\lambda_e}</span> where <span class="math inline">\lambda_e</span> is the parameter of the neural network at epoch <span class="math inline">e</span>. The red curve is the mean value and the gray zone is the 95% confidence interval. The learning rate used in the optimization is <span class="math inline">0.0025</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-multi_prior" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, q, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))  </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(q):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">#axs[i].hist(theta_sample_init[:, i], density=True, bins="rice", color="red", label=r"Initial prior", alpha=0.4)</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co">#axs[i].hist(theta_sample[:,i], density=True, bins="rice", label=r"Fitted prior", alpha=0.8, color='red', linewidth=1.2)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    axs[i].hist(theta_sample[:,i], density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Fitted prior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#axs[i].hist(jeffreys_sample[:,i], density=True, bins="rice", label="Jeffreys", alpha=0.6, color='blue', linewidth=1.2)</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    axs[i].hist(jeffreys_sample[:,i], density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="st">"Jeffreys"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    axs[i].set_xlabel(<span class="vs">r"$\theta_</span><span class="sc">{}</span><span class="vs">$"</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>))  </span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    axs[i].grid()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    axs[i].legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-multi_prior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multi_prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-multi_prior-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi_prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Histograms of the fitted prior and the Jeffreys prior Dir<span class="math inline">(\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2})</span> for each dimension of <span class="math inline">\theta</span>, each one is obtained from <span class="math inline">10^5</span> samples.
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the posterior distribution, we sample 10 times from the Multinomial distribution using <span class="math inline">\theta_{true} = (\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})</span>. The covariance matrix in the proposition distribution of the Metropolis-Hastings algorithm is not diagonal, since we have a relation between the different components of <span class="math inline">\theta</span>, we introduce non-zero covariances. We also verified that the auto-correlation between the successive remaining samples of the Markov chain decreases rapidly on each component.</p>
<div id="cell-fig-multi_post" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, q, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">4</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(q): </span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    axs[i].hist(jeffreys_post[:,i], density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Fitted post"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    axs[i].hist(theta_post[:,i], density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Jeffreys"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>,color<span class="op">=</span>vertCEA)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    axs[i].grid()</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    axs[i].legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    axs[i].set_xlabel(<span class="vs">r"$\theta_</span><span class="sc">{}</span><span class="vs">$"</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-multi_post" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-multi_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-multi_post-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-multi_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Histograms of the fitted posterior and the Jeffreys posterior for each dimension of <span class="math inline">\theta</span>, each one is obtained from <span class="math inline">5\cdot10^4</span> samples.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We notice (<a href="#fig-multi_mi" class="quarto-xref">Figure&nbsp;1</a>) that the mutual information lies between <span class="math inline">0</span> and <span class="math inline">1/\alpha(1-\alpha) = 4</span>, which is coherent with the theory, the confidence interval is rather large, but the mean value has an increasing trend.</p>
<p>Although the shape of the fitted prior resembles the one of the Jeffreys prior, one can notice that it tends to put more weight towards the extremities of the interval (<a href="#fig-multi_prior" class="quarto-xref">Figure&nbsp;2</a>). The posterior distribution however is quite similar to the target Jeffreys posterior on every component (<a href="#fig-multi_post" class="quarto-xref">Figure&nbsp;3</a>).</p>
<p>Since the multinomial model is simple and computationally practical, we would like to quantify the effect of the divergence with different <span class="math inline">\alpha</span> values on the output of the algorithm. In order to do so, we utilize the maximum mean discrepancy (MMD) defined as : <span class="math display">
   \text{MMD}(p,q) = || \mu_{p} - \mu_{q} ||_{\mathcal{H}},  
</span></p>
<p>where <span class="math inline">\mu_{p}</span> and <span class="math inline">\mu_{q}</span> are respectively the kernel mean embeddings of distributions <span class="math inline">p</span> and <span class="math inline">q</span> in a reproducible kernel Hilbert space (RKHS) <span class="math inline">(\mathcal{H}, || \cdot ||_{\mathcal{H}})</span>, meaning : <span class="math inline">\mu_p(\theta') = \mathbb{E}_{\theta \sim p }[K(\theta,\theta')]</span> for all <span class="math inline">\theta' \in \Theta</span> and <span class="math inline">K</span> being the kernel. The MMD is used for instance in the context of two-sample tests <span class="citation" data-cites="gretton_mmd">Gretton et al. (<a href="#ref-gretton_mmd" role="doc-biblioref">2012</a>)</span>, whose purpose is to compare distributions. We use in our computations the Gaussian or RBF kernel : <span class="math display">
  K(\theta, \theta') = \exp(-0.5 \cdot ||\theta - \theta'||_2^2),
</span> for which the MMD is a metric, this means that the following implication: <span class="math display">
\text{MMD}(p,q) = 0 \implies p = q
</span> is verified with the other axioms. In practice, we consider an unbiased estimator of the MMD<span class="math inline">^2</span> given by: <span class="math display">
\widehat{\text{MMD}^2}(p,q) =  \frac{1}{m(m-1)} \sum_{i \neq j}K(x_i,x_j) +  \frac{1}{n(n-1)} \sum_{i \neq j}K(y_i,y_j) - \frac{2}{mn} \sum_{i,j} K(x_i,y_j),
</span></p>
<p>where <span class="math inline">(x_1,...,x_m)</span> and <span class="math inline">(y_1,...,y_n)</span> are samples from <span class="math inline">p</span> and <span class="math inline">q</span> respectively. In our case, <span class="math inline">p</span> is the distribution obtained through variational inference and <span class="math inline">q</span> is the target Jeffreys distribution. Since the MMD can be time-consuming or memory inefficient to compute in practice for very large samples, we consider only the last <span class="math inline">2 \cdot 10^4</span> entries of our priors and posterior samples.</p>
<div id="f36cd3f5" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>compute_autocorr <span class="op">=</span> <span class="va">False</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> compute_autocorr :</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> autocorrelation(x, lag):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Compute the autocorrelation of a 1D array at a given lag."""</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> np.asarray(x)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        x_mean <span class="op">=</span> np.mean(x)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        x_var <span class="op">=</span> np.var(x)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x_var <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="dv">0</span> </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.correlate(x <span class="op">-</span> x_mean, x <span class="op">-</span> x_mean, mode<span class="op">=</span><span class="st">"full"</span>)[n <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> lag] <span class="op">/</span> (n <span class="op">*</span> x_var)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    lags <span class="op">=</span> <span class="dv">100</span> </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    autocorrs <span class="op">=</span> {i: [autocorrelation(theta_post[:, i], lag) <span class="cf">for</span> lag <span class="kw">in</span> <span class="bu">range</span>(lags <span class="op">+</span> <span class="dv">1</span>)] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(theta_post.shape[<span class="dv">1</span>])}</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, ac <span class="kw">in</span> autocorrs.items():</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        plt.plot(ac, label<span class="op">=</span><span class="ss">f'Component </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Lag'</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Autocorrelation'</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="95933ff5" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Multinomial_MMD.pkl'</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>len_alpha <span class="op">=</span> <span class="bu">len</span>(alphas)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>nb_samples_mmd <span class="op">=</span> <span class="dv">2</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_MultiMMD :</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    MMDvalues <span class="op">=</span> np.zeros((len_alpha,<span class="dv">2</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> index_alpha <span class="kw">in</span> <span class="bu">range</span>(len_alpha):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> alphas[index_alpha]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'alpha value = </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Parameters and classes</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> <span class="dv">50</span>     <span class="co"># latent space dimension</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="dv">4</span>      <span class="co"># parameter space dimension</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="dv">10</span>     <span class="co"># multinomial parameter</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> <span class="dv">10</span>     <span class="co"># number of data samples</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        J <span class="op">=</span> <span class="dv">1000</span>   <span class="co"># nb samples for MC estimation in MI</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        T <span class="op">=</span> <span class="dv">50</span>     <span class="co"># nb samples MC marginal likelihood</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        Multinom <span class="op">=</span> torch_MultinomialModel(n<span class="op">=</span>n, q<span class="op">=</span>q)  </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        input_size <span class="op">=</span> p  </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        output_size <span class="op">=</span> q</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        low <span class="op">=</span> <span class="fl">0.001</span>           <span class="co"># lower bound (must verifiy q * low &lt; 1)</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        upp <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> low<span class="op">*</span>(q<span class="op">-</span><span class="dv">1</span>)  <span class="co"># upper bound (forced value with low and q)</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">##### Prior #####</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        seed_all(<span class="dv">0</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>nn.Softmax(dim<span class="op">=-</span><span class="dv">1</span>))</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        NN <span class="op">=</span> AffineTransformation(NN, m_low<span class="op">=</span>low, M_upp<span class="op">=</span>upp)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Multinom)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span>alpha, use_log_lik<span class="op">=</span><span class="va">True</span>, use_baseline<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            theta_sample_init <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        loss_fct <span class="op">=</span> <span class="st">"LB_MI"</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        num_samples_MI <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        freq_MI <span class="op">=</span> <span class="dv">200</span>  </span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        save_best_param <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        learning_rate <span class="op">=</span> <span class="fl">0.0025</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        <span class="co">### Training loop</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI <span class="op">=</span> Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>                                                                num_samples_MI, freq_MI, save_best_param, learning_rate,</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>                                                                momentum<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        theta_sample_torch <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior)</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>            theta_sample <span class="op">=</span> theta_sample_torch.numpy()</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        jeffreys_sample <span class="op">=</span> Div.model.sample_Jeffreys(n_samples<span class="op">=</span>n_samples_prior)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">#MMDvalues[index_alpha, 0] = np.sqrt(compute_MMD2(theta_sample, jeffreys_sample))</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        MMDvalues[index_alpha, <span class="dv">0</span>] <span class="op">=</span> np.sqrt(MMD2_rbf(theta_sample[<span class="op">-</span>nb_samples_mmd:,:], jeffreys_sample[<span class="op">-</span>nb_samples_mmd:,:], gamma<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">##### Posterior ####</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        seed_all(<span class="dv">0</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sample data from 'true' parameter</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        theta_true <span class="op">=</span> torch.tensor(q<span class="op">*</span>[<span class="dv">1</span><span class="op">/</span>q])  </span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        D <span class="op">=</span> Multinom.sample(theta_true, N, <span class="dv">1</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> D[:, <span class="dv">0</span>]</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Posterior samples using Jeffreys prior</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        n_samples_post <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        jeffreys_post <span class="op">=</span> Multinom.sample_post_Jeffreys(X, n_samples_post)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>        T_mcmc <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>        eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>        eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>            theta_MH <span class="op">=</span> theta_MH.numpy()</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>            jeffreys_post <span class="op">=</span> jeffreys_post.numpy()</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>        theta_post <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:,:]</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>        <span class="co">#MMDvalues[index_alpha, 1] = np.sqrt(compute_MMD2(theta_post, jeffreys_post))</span></span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>        MMDvalues[index_alpha, <span class="dv">1</span>] <span class="op">=</span> np.sqrt(MMD2_rbf(theta_post[<span class="op">-</span>nb_samples_mmd:,:], jeffreys_post[<span class="op">-</span>nb_samples_mmd:,:], gamma<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>        pickle.dump(MMDvalues, <span class="bu">file</span>)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>        MMDvalues <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-mmd_multinom" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.column_stack((alphas, MMDvalues))</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#headers = ["α", "Prior", "Posterior"]</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>headers <span class="op">=</span> [<span class="vs">r"$\alpha$"</span>, <span class="st">"Prior"</span>, <span class="st">"Posterior"</span>]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> format_scientific(value, decimals<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Format a float in LaTeX-style scientific notation."""</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    formatted <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>value<span class="sc">:.</span>{decimals}e<span class="sc">}</span><span class="ss">"</span>  </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    base, exp <span class="op">=</span> formatted.split(<span class="st">"e"</span>)  </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"$</span><span class="sc">{</span>base<span class="sc">}</span><span class="ss"> </span><span class="ch">\\</span><span class="ss">times 10^</span><span class="ch">{{</span><span class="sc">{</span><span class="bu">int</span>(exp)<span class="sc">}</span><span class="ch">}}</span><span class="ss">$"</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>formatted_data <span class="op">=</span> [</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    [<span class="ss">f"$</span><span class="sc">{</span>row[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">$"</span>, format_scientific(row[<span class="dv">1</span>]), format_scientific(row[<span class="dv">2</span>])]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> data</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># formatted_data = [</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">#     [f"{row[0]:.2f}", f"{row[1]:.2e}", f"{row[2]:.3e}"] for row in data</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ]</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>table_data <span class="op">=</span> [headers] <span class="op">+</span> formatted_data</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>ax.axis(<span class="st">"off"</span>)  </span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> ax.table(</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    cellText<span class="op">=</span>table_data,  </span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    colLabels<span class="op">=</span><span class="va">None</span>,      </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    loc<span class="op">=</span><span class="st">"center"</span>,        </span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    cellLoc<span class="op">=</span><span class="st">"center"</span>,   </span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i, key) <span class="kw">in</span> <span class="bu">enumerate</span>(headers):</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    cell <span class="op">=</span> table[<span class="dv">0</span>, i]  </span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    cell.set_text_props(fontweight<span class="op">=</span><span class="st">"bold"</span>)  </span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    cell.set_facecolor(<span class="st">"lightgray"</span>)  </span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>table.scale(<span class="dv">1</span>, <span class="fl">2.5</span>)  </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>table.auto_set_font_size(<span class="va">False</span>)</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>table.set_fontsize(<span class="dv">12</span>)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-mmd_multinom" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mmd_multinom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-mmd_multinom-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mmd_multinom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: MMD values for different <span class="math inline">\alpha</span>-divergences at prior and posterior levels. As a reference on the prior level, when computing the criterion between two independent Dirichlet Dir<span class="math inline">(\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2})</span> distributions (ie the Jeffreys prior) on <span class="math inline">2 \cdot 10^4</span> samples, we obtain an order of magnitude of <span class="math inline">10^{-3}</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>According to <a href="#fig-mmd_multinom" class="quarto-xref">Figure&nbsp;4</a>, the difference between <span class="math inline">\alpha</span> values in terms of the MMD criterion is essentially inconsequential. One remark is that the mutual information tends to be more unstable as <span class="math inline">\alpha</span> gets closer to <span class="math inline">1</span>. The explanation is that when <span class="math inline">\alpha</span> tends to <span class="math inline">1</span>, we have the approximation : <span class="math display">
\hat{f}_{\alpha}(x) \approx \frac{x-1}{\alpha(\alpha-1)} + \frac{x\log(x)}{\alpha},    
</span></p>
<p>which diverges for all <span class="math inline">x</span> because of the first term. Hence, we advise the user to avoid <span class="math inline">\alpha</span> values that are too close to <span class="math inline">1</span>. In the following, we use <span class="math inline">\alpha = 0.5</span> for the divergence.</p>
<div id="0e62ef5f" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>compute_mmd_ref_value <span class="op">=</span> <span class="va">False</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> compute_mmd_ref_value : </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    dir1 <span class="op">=</span> Multinom.sample_Jeffreys(n_samples<span class="op">=</span>nb_samples_mmd)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    dir2 <span class="op">=</span> Multinom.sample_Jeffreys(n_samples<span class="op">=</span>nb_samples_mmd)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'MMD reference value : </span><span class="sc">{</span>np<span class="sc">.</span>sqrt(MMD2_rbf(dir1, dir2, gamma<span class="op">=</span><span class="fl">0.5</span>))<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-probit_model" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-probit_model"><span class="header-section-number">4.2</span> Probit model</h2>
<p>We present in this section the probit model used to estimate seismic fragility curves, which was introduced by <span class="citation" data-cites="kennedy1980">Kennedy et al. (<a href="#ref-kennedy1980" role="doc-biblioref">1980</a>)</span>, it is also referred as the log-normal model in the literature. A seismic fragility curve is the probability of failure <span class="math inline">P_f(a)</span> of a mechanical structure subjected to a seism as a function of a scalar value <span class="math inline">a</span> derived from the seismic ground motion. The properties of the Jeffreys prior for this model are highlighted by <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span>.</p>
<p>The model is defined by the observation of an i.i.d. sample <span class="math inline">\mathbf{X}=(X_1,\dots,X_N)</span> where for any <span class="math inline">i</span>, <span class="math inline">X_i\sim(Z,a)\in\mathcal{X}=\{0,1\}\times(0,\infty)</span>. The distribution of the r.v. <span class="math inline">(Z,a)</span> is parameterized by <span class="math inline">\theta=(\theta_1,\theta_2)\in(0,\infty)^2</span> as:</p>
<p><span class="math display">    
\begin{cases}
   \displaystyle a \sim \text{Log}\text{-}\mathcal{N}(\mu_a, \sigma^2_a) \\
    P_f(a) = \displaystyle \Phi \left( \frac{\log a - \log \theta_1}{\theta_2} \right) \\
    Z \sim \text{Bernoulli}(P_f(a)),
\end{cases}
</span></p>
<p>where <span class="math inline">\Phi</span> is the cumulative distribution function of the standard Gaussian. The probit function is the inverse of <span class="math inline">\Phi</span>. The likelihood is of the form : <span class="math display">
\begin{cases}
    L_N(\mathbf{X}\, | \, \theta) = \displaystyle \prod_{i=1}^N p(a_i) \prod_{i=1}^N P_f(a_i)^{Z_i} (1-P_f(a_i))^{1-Z_i} \propto \prod_{i=1}^N  P_f(a_i)^{Z_i} (1-P_f(a_i))^{1-Z_i} \\
    p(a_i) = \displaystyle \frac{1}{a_i \sqrt{2\pi \sigma^2_a}} \exp \left( - \frac{1}{2\sigma_a^2}(\log a_i - \mu_a)^2 \right).
\end{cases}
</span></p>
<p>For simplicity, we denote : <span class="math inline">\gamma_i = \displaystyle  \frac{\log a_i - \log \theta_1}{\theta_2} = \Phi^{-1}(P_f(a_i)) = \text{probit}(P_f(a_i))</span>, the gradient of the log-likelihood is the following :</p>
<p><span class="math display">
\begin{cases}
\displaystyle \frac{\partial \log L_N}{\partial \theta_1}(\mathbf{X}\,|\,\theta)  = \sum_{i=1}^N  \frac{1}{\theta_1 \theta_2} \left( (-Z_i)\frac{\Phi'(\gamma_i)}{\Phi(\gamma_i)} + (1-Z_i)\frac{\Phi'(\gamma_i)}{1-\Phi(\gamma_i)}  \right) \\
\displaystyle \frac{\partial \log L_N}{\partial \theta_2}(\mathbf{X}\,|\,\theta)  =   \sum_{i=1}^N \frac{\gamma_i}{\theta_2} \left( (-Z_i)\frac{\Phi'(\gamma_i)}{\Phi(\gamma_i)} + (1-Z_i)\frac{\Phi'(\gamma_i)}{1-\Phi(\gamma_i)}  \right).
\end{cases}
</span></p>
<p>There is no explicit formula for the MLE, so it has to be approximated using samples. This statistical model is a more difficult case than the previous one, since no explicit formula for the Jeffreys prior is available either but it has been shown by <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span> that it is improper in <span class="math inline">\theta_2</span> and some asymptotic rates where derived. More precisely, when <span class="math inline">\theta_1 &gt; 0</span> is fixed, <span class="math display">
\begin{cases} \displaystyle
J(\theta) \propto 1/\theta_2 \quad \text{as} \quad  \theta_2 \longrightarrow 0 \\
J(\theta) \propto 1/\theta_2^3 \quad \text{as} \quad  \theta_2 \longrightarrow +\infty.
\end{cases}
</span></p>
<p>If we fix <span class="math inline">\theta_2 &gt; 0</span>, the prior is proper for the variable <span class="math inline">\theta_1</span> : <span class="math display">
  J(\theta) \propto \frac{|\log \theta_1|}{\theta_1} \exp \left( -\frac{(\log \theta_1 - \mu_a)^2}{2\theta_2 + 2\sigma_a^2} \right)  \quad \text{when} \quad  |\log \theta_1| \longrightarrow +\infty.
  </span></p>
<p>which resembles a log-normal distribution except for the <span class="math inline">|\log \theta_1|</span> factor. Since the density of the Jeffreys prior is not explicit and can not be computed directly, the Fisher information matrix is computed in <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span> using numerical integration with Simpson’s rule on a specific grid and then an interpolation is applied. We use this computation as the reference to evaluate the quality of the output of our algorithm. In the mentioned article, the posterior distribution is also computed with an adaptive Metropolis-Hastings algorithm on the variable <span class="math inline">\theta</span>, we refer to this algorithm as MH(<span class="math inline">\theta</span>) since it is different from the one mentioned in <a href="#sec-posterio-sampling" class="quarto-xref">Section&nbsp;3.4</a>. More details on MH(<span class="math inline">\theta</span>) are given in <span class="citation" data-cites="gauchy_thesis">Gauchy (<a href="#ref-gauchy_thesis" role="doc-biblioref">2022</a>)</span>. We take <span class="math inline">\mu_a = 0</span>, <span class="math inline">\sigma^2_a = 1</span>, <span class="math inline">N=500</span> and <span class="math inline">U=500</span> for the computation of the prior.</p>
<p>As for the neural network, we use a one-layer network with an <span class="math inline">\exp</span> activation for <span class="math inline">\theta_1</span> and a Softplus activation for <span class="math inline">\theta_2</span>. We have that : <span class="math display">
\theta = \begin{pmatrix}
           \theta_1 \\
           \theta_2 \\
         \end{pmatrix} =
\begin{pmatrix}
           \exp(w_1^{\top}\varepsilon + b_1) \\
           \log\left(1 + \exp(w_2^{\top} \varepsilon + b_2)\right) \\
         \end{pmatrix},  
</span></p>
<p>with <span class="math inline">w_1, w_2 \in \mathbb{R}^p</span> the weight vectors and <span class="math inline">b_1, b_2 \in \mathbb{R}</span> the biases, thus we have <span class="math inline">\lambda = (w_1, w_2, b_1,b_2)</span>. Because this architecture remains simple, it is possible to elucidate the resulting marginal distributions of <span class="math inline">\theta_1</span> and <span class="math inline">\theta_2</span>. The first component <span class="math inline">\theta_1</span> follows a <span class="math inline">\text{Log-}\mathcal{N}(b_1, ||w_1||_2^2)</span> distribution and <span class="math inline">\theta_2</span> has an explicit density function : <span class="math display">\begin{equation*}
    p(\theta_2) = \frac{1}{\sqrt{2\pi||w_2||_2^2}(1-e^{-\theta_2})} \exp \left(-\frac{1}{2||w_2||_2^2}\left(\log(e^{\theta_2}-1)-b_2 \right)^2 \right).
\end{equation*}</span></p>
<p>These expressions describe the parameterized set <span class="math inline">\mathcal{P}_\Lambda</span> of priors considered in the optimization problem. This set is restrictive, so that the resulting VA-RP must be interpreted as the most objective —according to the mutual information criterion— prior among the ones in <span class="math inline">\mathcal{P}_\Lambda</span>. Since we do not know any explicit expression of the Jeffreys prior for this prior, we cannot provide a precise comparison between the parameterized VA-RP elucidated above and the target. However, the form of the distribution of <span class="math inline">\theta_1</span> qualitatively resembles its theoretical target. In the case of <span class="math inline">\theta_2</span>, the asymptotic decay rates of its density function can be derived: <span id="eq-decay_rates_ptheta2"><span class="math display">
\begin{cases}
    p(\theta_2) \overset{}{\underset{\theta_2\rightarrow 0}{=}} \frac{1}{\theta_2\sqrt{2\pi}\|w_2\|_2}\exp\left(-\frac{(\log\theta_2-b_2)^2}{2\|w_2\|_2^2}\right); \\
    p(\theta_2) \overset{}{\underset{\theta_2\rightarrow\infty}{=}} \frac{1}{\sqrt{2\pi}\|w_2\|_2}\exp\left(-\frac{(\theta_2-b_2)^2}{2\|w_2\|_2^2} \right).
\end{cases}
\tag{13}</span></span></p>
<p>While <span class="math inline">\|w_2\|_2</span> does not tend toward <span class="math inline">\infty</span>, these decay rates strongly differ from the ones of the Jeffreys prior w.r.t. <span class="math inline">\theta_2</span>. Otherwise, the decay rates resemble to something proportional to <span class="math inline">(\theta_2+1)^{-1}</span> in both directions. In our numerical computations, the optimization process yielded a VA-RP with parameters <span class="math inline">w_2</span> and <span class="math inline">b_2</span> that did not diverge to extreme values.</p>
<div id="5ad2592e" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters and classes</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">50</span>   </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">2</span>      </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">500</span>     </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> <span class="dv">500</span>   </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">50</span>     </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> p  </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> q</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>low <span class="op">=</span> <span class="fl">0.0001</span>        </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>upp <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> low</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>mu_a, sigma2_a <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="co">#mu_a, sigma2_a = 8.7 * 10**-3, 1.03</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>Probit <span class="op">=</span> torch_ProbitModel(use_log_normal<span class="op">=</span><span class="va">True</span>, mu_a<span class="op">=</span>mu_a, sigma2_a<span class="op">=</span>sigma2_a, set_beta<span class="op">=</span><span class="va">None</span>, alt_scaling<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Probit_results_unconstrained.pkl'</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">0</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>nn.Identity())</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> DifferentActivations(NN, [torch.exp, nn.Softplus()])</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> AffineTransformation(NN, low, upp)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Probit)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span>alpha, use_log_lik<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    theta_sample_init <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>loss_fct <span class="op">=</span> <span class="st">"LB_MI"</span></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>num_samples_MI <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>freq_MI <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>save_best_param <span class="op">=</span> <span class="va">True</span></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_Probit_nocstr : </span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>    MI, range_MI, lower_MI, upper_MI <span class="op">=</span> Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, num_samples_MI, </span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>                                                            freq_MI, save_best_param, learning_rate, momentum<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    all_params <span class="op">=</span> torch.cat([param.view(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> param <span class="kw">in</span> NN.parameters()])</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>    theta_sample <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        theta_sample_prior <span class="op">=</span> theta_sample.numpy()</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(tirages_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>    data_A, data_Z <span class="op">=</span> data[<span class="dv">0</span>], data[<span class="dv">1</span>]</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>    theta_true <span class="op">=</span> np.array([<span class="fl">3.37610525</span>, <span class="fl">0.43304097</span>])</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">50</span>  <span class="co"># non-degenerate</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    Xstack <span class="op">=</span> np.stack((data_Z[:N, i],data_A[:N, i]),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.tensor(Xstack)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> X.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    Probit.data <span class="op">=</span> D</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    n_samples_post <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    T_mcmc <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span> <span class="op">+</span> <span class="dv">1</span> </span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    <span class="co">#eps_0 = 10 * torch.ones(p)</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>    eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept<span class="op">=</span><span class="fl">0.4</span>, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">True</span>, disable_tqdm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>    theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> theta_MH.detach().numpy()</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    theta_post_nocstr <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:,<span class="op">-</span>n_samples_post:]</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saves all relevant quantities</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    Mutual_Info <span class="op">=</span> {<span class="st">'values'</span> : MI, <span class="st">'range'</span> : range_MI, <span class="st">'lower'</span> : lower_MI, <span class="st">'upper'</span> : upper_MI}</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    Prior <span class="op">=</span> {<span class="st">'samples'</span> : theta_sample_prior, <span class="st">'jeffreys'</span> : <span class="va">None</span>, <span class="st">'params'</span> : all_params}</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    Posterior <span class="op">=</span> {<span class="st">'samples'</span> : theta_post_nocstr, <span class="st">'jeffreys'</span> : <span class="va">None</span>}</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    Probit_results_nocstr <span class="op">=</span> {<span class="st">'MI'</span> : Mutual_Info, <span class="st">'prior'</span> : Prior, <span class="st">'post'</span> : Posterior}</span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>        pickle.dump(Probit_results_nocstr, <span class="bu">file</span>)</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>        MI_dic <span class="op">=</span> res[<span class="st">'MI'</span>]</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>        Prior <span class="op">=</span> res[<span class="st">'prior'</span>] </span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>        Posterior <span class="op">=</span> res[<span class="st">'post'</span>]</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI <span class="op">=</span> MI_dic[<span class="st">'values'</span>], MI_dic[<span class="st">'range'</span>], MI_dic[<span class="st">'lower'</span>], MI_dic[<span class="st">'upper'</span>]</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>        theta_sample_prior, jeffreys_sample, all_params <span class="op">=</span> Prior[<span class="st">'samples'</span>], Prior[<span class="st">'jeffreys'</span>], Prior[<span class="st">'params'</span>]</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>        theta_post_nocstr, jeffreys_post <span class="op">=</span> Posterior[<span class="st">'samples'</span>], Posterior[<span class="st">'jeffreys'</span>]    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-probit_mi" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'-'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'*'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in range(len(range_MI)):</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, upper_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, lower_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.fill_between(range_MI, lower_MI, upper_MI, color<span class="op">=</span><span class="st">'lightgrey'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Epochs"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Generalized mutual information"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.yscale('log')</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_mi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_mi-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Monte Carlo estimation of the generalized mutual information with <span class="math inline">\alpha=0.5</span> (from 100 samples) for <span class="math inline">\pi_{\lambda_e}</span> where <span class="math inline">\lambda_e</span> is the parameter of the neural network at epoch <span class="math inline">e</span>. The red curve is the mean value and the gray zone is the 95% confidence interval. The learning rate used in the optimization is <span class="math inline">0.001</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-probit_mi" class="quarto-xref">Figure&nbsp;5</a> is shown the evolution of the mutual information through the optimization of the VA-RP for the probit model. We perceive high mutual information values at the initialization, which we interpret as a result of the fact that the parametric prior on <span class="math inline">\theta_1</span> is already quite close to its target distribution.</p>
<p>With <span class="math inline">\alpha</span>-divergences, using a moment constraint of the form <span class="math inline">a(\theta_2) =\theta_2^{\kappa}</span> for the second component is relevant here as long as <span class="math inline">\kappa \in \left(0, \frac{2}{1+1/\alpha}\right)</span>, to ensure that the resulting constrained prior is indeed proper. With <span class="math inline">\alpha=0.5</span>, we take the value <span class="math inline">\kappa=1/8</span> and we use the same neural network. The evolution of the mutual information through the optimization of the constrained VA-RP is proposed in <a href="#fig-probit_constr_mi" class="quarto-xref">Figure&nbsp;6</a>. In <a href="#fig-probit_constr_gap" class="quarto-xref">Figure&nbsp;7</a> is presented the evolution of the constrained gap: the difference between the target and current values for the constraint.</p>
<div id="c350f8c5" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>kappa <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">8</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>K_val <span class="op">=</span> np.mean((theta_sample_prior[:,<span class="dv">1</span>]<span class="op">**</span>kappa)<span class="op">**</span>(<span class="dv">1</span><span class="op">/</span>alpha))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>c_val <span class="op">=</span> np.mean((theta_sample_prior[:,<span class="dv">1</span>]<span class="op">**</span>kappa)<span class="op">**</span>(<span class="dv">1</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span>alpha))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>constr_val <span class="op">=</span> c_val <span class="op">/</span> K_val</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>alpha_constr <span class="op">=</span> np.mean((theta_sample_prior[:,<span class="dv">0</span>]<span class="op">**-</span>kappa <span class="op">+</span> <span class="dv">1</span>)<span class="op">**-</span><span class="dv">1</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f'Constraint value estimation : {constr_val}')  # = 0.839594841003418 </span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters and classes</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">50</span>     <span class="co"># latent space dimension</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">2</span>      <span class="co"># parameter space dimension</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">500</span>    <span class="co"># number of data samples</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> <span class="dv">500</span>    <span class="co"># nb samples for MC estimation in MI</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">50</span>     <span class="co"># nb samples MC marginal likelihood</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> p  </span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> q</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>low <span class="op">=</span> <span class="fl">0.0001</span>          <span class="co"># lower bound </span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>upp <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> low</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>mu_a, sigma2_a <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co">#mu_a, sigma2_a = 8.7 * 10**-3, 1.03</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>Probit <span class="op">=</span> torch_ProbitModel(use_log_normal<span class="op">=</span><span class="va">True</span>, mu_a<span class="op">=</span>mu_a, sigma2_a<span class="op">=</span>sigma2_a, set_beta<span class="op">=</span><span class="va">None</span>, alt_scaling<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Constraints</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> torch.tensor([kappa])</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor([[alpha_constr,constr_val]]) </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>T_cstr <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>eta_augm <span class="op">=</span> torch.tensor([[<span class="fl">0.</span>,<span class="fl">1.</span>]])</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> torch.tensor([[<span class="fl">0.</span>,<span class="fl">1.</span>]])</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Probit_results_constrained.pkl'</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">0</span>)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>nn.Identity())</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> DifferentActivations(NN, [torch.exp, nn.Softplus()])</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> AffineTransformation(NN, low, upp)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Probit)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, use_log_lik<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>Constr <span class="op">=</span> Constraints_NeuralNet(div<span class="op">=</span>Div, betas<span class="op">=</span>beta, b<span class="op">=</span>b, T_cstr<span class="op">=</span>T_cstr, </span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>                               objective<span class="op">=</span><span class="st">'LB_MI'</span>, lag_method<span class="op">=</span><span class="st">'augmented'</span>, eta_augm<span class="op">=</span>eta_augm, rule<span class="op">=</span><span class="st">'SGD'</span>)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    theta_sample_init <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>num_samples_MI <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>freq_MI <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>save_best_param <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0005</span></span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>freq_augm <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_Probit_cstr :</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training loop</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    MI, constr_values, range_MI, lower_MI, upper_MI <span class="op">=</span> Constr.Partial_autograd(J, N, eta, num_epochs, optimizer, num_samples_MI, </span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>                                                            freq_MI, save_best_param, learning_rate, momentum<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>                                                            freq_augm<span class="op">=</span>freq_augm, max_violation<span class="op">=</span><span class="fl">0.005</span>, update_eta_augm<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>    constr_values <span class="op">=</span> torch.stack(constr_values).numpy()</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>    all_params <span class="op">=</span> torch.cat([param.view(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> param <span class="kw">in</span> NN.parameters()])</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>        theta_sample <span class="op">=</span> Constr.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Moment of order </span><span class="sc">{</span>beta<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">, estimation : </span><span class="sc">{</span>np<span class="sc">.</span>mean(theta_sample<span class="op">**</span>beta.item(),axis<span class="op">=</span><span class="dv">0</span>)<span class="sc">}</span><span class="ss">, wanted value : </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(tirages_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>    data_A, data_Z <span class="op">=</span> data[<span class="dv">0</span>], data[<span class="dv">1</span>]</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">50</span>  <span class="co"># non-degenerate</span></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a>    Xstack <span class="op">=</span> np.stack((data_Z[:N, i],data_A[:N, i]),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.tensor(Xstack)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> X.unsqueeze(<span class="dv">1</span>)</span>
<span id="cb12-80"><a href="#cb12-80" aria-hidden="true" tabindex="-1"></a>    Probit.data <span class="op">=</span> D</span>
<span id="cb12-81"><a href="#cb12-81" aria-hidden="true" tabindex="-1"></a>    n_samples_post <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb12-82"><a href="#cb12-82" aria-hidden="true" tabindex="-1"></a>    T_mcmc <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span> <span class="op">+</span> <span class="dv">1</span> </span>
<span id="cb12-83"><a href="#cb12-83" aria-hidden="true" tabindex="-1"></a>    sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb12-84"><a href="#cb12-84" aria-hidden="true" tabindex="-1"></a>    eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb12-85"><a href="#cb12-85" aria-hidden="true" tabindex="-1"></a>    eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept<span class="op">=</span><span class="fl">0.4</span>, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">True</span>, disable_tqdm<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-86"><a href="#cb12-86" aria-hidden="true" tabindex="-1"></a>    theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb12-87"><a href="#cb12-87" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-88"><a href="#cb12-88" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> theta_MH.detach().numpy()</span>
<span id="cb12-89"><a href="#cb12-89" aria-hidden="true" tabindex="-1"></a>    theta_post_cstr <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:,<span class="op">-</span>n_samples_post:]</span>
<span id="cb12-90"><a href="#cb12-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-91"><a href="#cb12-91" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saves all relevant quantities</span></span>
<span id="cb12-92"><a href="#cb12-92" aria-hidden="true" tabindex="-1"></a>    Mutual_Info <span class="op">=</span> {<span class="st">'values'</span> : MI, <span class="st">'range'</span> : range_MI, <span class="st">'lower'</span> : lower_MI, <span class="st">'upper'</span> : upper_MI, <span class="st">'constr'</span> : constr_values}</span>
<span id="cb12-93"><a href="#cb12-93" aria-hidden="true" tabindex="-1"></a>    Prior <span class="op">=</span> {<span class="st">'samples'</span> : theta_sample_prior, <span class="st">'jeffreys'</span> : <span class="va">None</span>, <span class="st">'params'</span> : all_params}</span>
<span id="cb12-94"><a href="#cb12-94" aria-hidden="true" tabindex="-1"></a>    Posterior <span class="op">=</span> {<span class="st">'samples'</span> : theta_post_cstr, <span class="st">'jeffreys'</span> : <span class="va">None</span>}</span>
<span id="cb12-95"><a href="#cb12-95" aria-hidden="true" tabindex="-1"></a>    Probit_results_cstr <span class="op">=</span> {<span class="st">'MI'</span> : Mutual_Info, <span class="st">'prior'</span> : Prior, <span class="st">'post'</span> : Posterior}</span>
<span id="cb12-96"><a href="#cb12-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb12-97"><a href="#cb12-97" aria-hidden="true" tabindex="-1"></a>        pickle.dump(Probit_results_cstr, <span class="bu">file</span>)</span>
<span id="cb12-98"><a href="#cb12-98" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-99"><a href="#cb12-99" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb12-100"><a href="#cb12-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb12-101"><a href="#cb12-101" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb12-102"><a href="#cb12-102" aria-hidden="true" tabindex="-1"></a>        MI_dic <span class="op">=</span> res[<span class="st">'MI'</span>]</span>
<span id="cb12-103"><a href="#cb12-103" aria-hidden="true" tabindex="-1"></a>        Prior <span class="op">=</span> res[<span class="st">'prior'</span>] </span>
<span id="cb12-104"><a href="#cb12-104" aria-hidden="true" tabindex="-1"></a>        Posterior <span class="op">=</span> res[<span class="st">'post'</span>]</span>
<span id="cb12-105"><a href="#cb12-105" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI, constr_values <span class="op">=</span> MI_dic[<span class="st">'values'</span>], MI_dic[<span class="st">'range'</span>], MI_dic[<span class="st">'lower'</span>], MI_dic[<span class="st">'upper'</span>], MI_dic[<span class="st">'constr'</span>]</span>
<span id="cb12-106"><a href="#cb12-106" aria-hidden="true" tabindex="-1"></a>        theta_sample_prior, jeffreys_sample, all_params <span class="op">=</span> Prior[<span class="st">'samples'</span>], Prior[<span class="st">'jeffreys'</span>], Prior[<span class="st">'params'</span>]</span>
<span id="cb12-107"><a href="#cb12-107" aria-hidden="true" tabindex="-1"></a>        theta_post_cstr, jeffreys_post <span class="op">=</span> Posterior[<span class="st">'samples'</span>], Posterior[<span class="st">'jeffreys'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-probit_constr_mi" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'-'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, MI, <span class="st">'*'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for i in range(len(range_MI)):</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, upper_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, lower_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>plt.fill_between(range_MI, lower_MI, upper_MI, color<span class="op">=</span><span class="st">'lightgrey'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Epochs"</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Generalized mutual information"</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.yscale('log')</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_constr_mi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_constr_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_constr_mi-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_constr_mi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Monte Carlo estimation of the generalized mutual information with <span class="math inline">\alpha=0.5</span> (from 100 samples) for <span class="math inline">\pi_{\lambda_e}</span> where <span class="math inline">\lambda_e</span> is the parameter of the neural network at epoch <span class="math inline">e</span>. The red curve is the mean value and the gray zone is the 95% confidence interval. The learning rate used in the optimization is <span class="math inline">0.0005</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-probit_constr_gap" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, constr_values[:,<span class="dv">0</span>,<span class="dv">1</span>], <span class="st">'-'</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.plot(range_MI, constr_values[:,<span class="dv">0</span>,<span class="dv">1</span>], <span class="st">'*'</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Epochs"</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Constraint gap"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.yscale('log')</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_constr_gap" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_constr_gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_constr_gap-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_constr_gap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Evolution of the constraint value gap during training. It corresponds to the difference between the target and current values for the constraint (in absolute value)
</figcaption>
</figure>
</div>
</div>
</div>
<p>For the posterior, we take as dataset <span class="math inline">50</span> samples from the probit model. For computational reasons, the Metropolis-Hastings algorithm is applied for only <span class="math inline">5\cdot10^4</span> iterations. An important remark is that if the size of the dataset is rather small, the probability that the data is degenerate is not negligible. By degenerate data, we refer to situations when the data points are partitioned into two disjoint subsets when classified according to <span class="math inline">a</span> values, the posterior becomes improper because the likelihood is constant (<span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span>). In such cases, the convergence of the Markov chains is less apparent, the plots for this section are obtained with non-degenerate datasets.</p>
<div id="cell-fig-probit_post_scatterhist" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">50</span>  <span class="co"># non-degenerate</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(int_jeffreys_path, <span class="ss">f'model_J_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    model_J <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>theta_J <span class="op">=</span> model_J[<span class="st">'logs'</span>][<span class="st">'post'</span>][N]</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> theta_post_nocstr[:, <span class="dv">0</span>]</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> theta_post_nocstr[:, <span class="dv">1</span>]</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> theta_J[:, <span class="dv">0</span>]</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> theta_J[:, <span class="dv">1</span>]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>kde_x1 <span class="op">=</span> gaussian_kde(x1)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>kde_y1 <span class="op">=</span> gaussian_kde(y1)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>kde_x2 <span class="op">=</span> gaussian_kde(x2)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>kde_y2 <span class="op">=</span> gaussian_kde(y2)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure and gridspec layout</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>gs <span class="op">=</span> gridspec.GridSpec(<span class="dv">4</span>, <span class="dv">5</span>) </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Main scatter plot </span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>ax_main <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>:<span class="dv">4</span>, <span class="dv">0</span>:<span class="dv">3</span>])</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>ax_main.scatter(x1, y1, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, marker<span class="op">=</span><span class="st">'.'</span>, label<span class="op">=</span><span class="st">'Fitted posterior'</span>,zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>ax_main.scatter(x2, y2, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, marker<span class="op">=</span><span class="st">'.'</span>, label<span class="op">=</span><span class="st">'Jeffreys posterior'</span>,zorder<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>ax_main.set_xlabel(<span class="vs">r'$\theta_1$'</span>)</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>ax_main.set_ylabel(<span class="vs">r'$\theta_2$'</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>ax_main.legend(loc<span class="op">=</span><span class="st">'upper left'</span>, fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>ax_main.grid()</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal histogram / KDE for alpha</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>ax_x_hist <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, <span class="dv">0</span>:<span class="dv">3</span>], sharex<span class="op">=</span>ax_main)</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>ax_x_hist.hist(x1, bins<span class="op">=</span><span class="st">'rice'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Fitted histogram'</span>,density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>ax_x_hist.hist(x2, bins<span class="op">=</span><span class="st">'rice'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Jeffreys histogram'</span>,density<span class="op">=</span><span class="va">True</span>,color<span class="op">=</span>vertCEA)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="bu">min</span>(x1.<span class="bu">min</span>(), x2.<span class="bu">min</span>()), <span class="bu">max</span>(x1.<span class="bu">max</span>(), x2.<span class="bu">max</span>()), <span class="dv">100</span>)</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>ax_x_hist.plot(x_vals, kde_x1(x_vals), color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted KDE'</span>)</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>ax_x_hist.plot(x_vals, kde_x2(x_vals), color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Jeffreys KDE'</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>ax_x_hist.set_ylabel(<span class="vs">r'Marginal $\theta_1$'</span>)</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>ax_x_hist.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, labelbottom<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>ax_x_hist.legend()</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>ax_x_hist.grid()</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal histogram / KDE for beta</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>ax_y_hist <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>:<span class="dv">4</span>, <span class="dv">3</span>:<span class="dv">5</span>], sharey<span class="op">=</span>ax_main)</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>ax_y_hist.hist(y1, bins<span class="op">=</span><span class="st">'rice'</span>, orientation<span class="op">=</span><span class="st">'horizontal'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Fitted histogram'</span>,density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>ax_y_hist.hist(y2, bins<span class="op">=</span><span class="st">'rice'</span>, orientation<span class="op">=</span><span class="st">'horizontal'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Jeffreys histogram'</span>,density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="bu">min</span>(y1.<span class="bu">min</span>(), y2.<span class="bu">min</span>()), <span class="bu">max</span>(y1.<span class="bu">max</span>(), y2.<span class="bu">max</span>()), <span class="dv">100</span>)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>ax_y_hist.plot(kde_y1(y_vals), y_vals, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted KDE'</span>)</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>ax_y_hist.plot(kde_y2(y_vals), y_vals, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Jeffreys KDE'</span>)</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>ax_y_hist.set_xlabel(<span class="vs">r'Marginal $\theta_2$'</span>)</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>ax_y_hist.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>ax_y_hist.legend()</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>ax_y_hist.grid()</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_post_scatterhist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_post_scatterhist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_post_scatterhist-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_post_scatterhist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Scatter histogram of the unconstrained fitted posterior and the Jeffreys posterior distributions obtained from <span class="math inline">5000</span> samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As <a href="#fig-probit_post_scatterhist" class="quarto-xref">Figure&nbsp;8</a> shows, we obtain a relevant approximation of the true Jeffreys posterior especially on the variable <span class="math inline">\theta_1</span>, whereas a small difference is present for the tail of the distribution on <span class="math inline">\theta_2</span>. The latter remark was expected regarding the analytical study of the marginal distribution of <span class="math inline">\pi_\lambda</span> w.r.t. <span class="math inline">\theta_2</span> given the architecture considered for the VA-RP (see <a href="#eq-decay_rates_ptheta2" class="quarto-xref">Equation&nbsp;13</a>). It is interesting to see that the difference between the posteriors is harder to discern in the neighborhood of <span class="math inline">\theta_2=0</span>. Indeed, in such case where the data are not degenerate, the likelihood provides a strong decay rate when <span class="math inline">\theta_2\rightarrow0</span> that makes the influence of the prior negligible (see <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span>): <span class="math display">
L_N(\mathbf{X}\,|\,\theta) \overset{}{\underset{\theta_2\rightarrow 0}{=}} \theta_2^{\|\chi\|_2^2}\exp\left(-\frac{1}{2\theta_2^2}\sum_{i=1}^N\chi_i(\log a_i-\log\theta_1)^2 \right),
</span> where <span class="math inline">\chi\in\{0,1\}^N</span> is a non-null vector that depends on <span class="math inline">\mathbf{X}</span>.</p>
<p>When <span class="math inline">\theta_2\rightarrow\infty</span>, however, the likelihood does not reduce the influence of the prior has it remains asymptotically constant: <span class="math inline">L_N(\mathbf{X}\,|\,\theta) \overset{\ }{\underset{\theta_2\rightarrow\infty}{\to}}2^{-N}</span>.</p>
<div id="cell-fig-probit_post_constr_scatterhist" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">50</span>  <span class="co"># non-degenerate</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(int_jeffreys_path, <span class="ss">f'model_J_constraint_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    model_J <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>theta_J <span class="op">=</span> model_J[<span class="st">'logs'</span>][<span class="st">'post'</span>][N]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> theta_post_cstr[:, <span class="dv">0</span>]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>y1 <span class="op">=</span> theta_post_cstr[:, <span class="dv">1</span>]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> theta_J[:, <span class="dv">0</span>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> theta_J[:, <span class="dv">1</span>]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>kde_x1 <span class="op">=</span> gaussian_kde(x1)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>kde_y1 <span class="op">=</span> gaussian_kde(y1)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>kde_x2 <span class="op">=</span> gaussian_kde(x2)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>kde_y2 <span class="op">=</span> gaussian_kde(y2)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure and gridspec layout</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>gs <span class="op">=</span> gridspec.GridSpec(<span class="dv">4</span>, <span class="dv">5</span>) </span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Main scatter plot </span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>ax_main <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>:<span class="dv">4</span>, <span class="dv">0</span>:<span class="dv">3</span>])</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>ax_main.scatter(x1, y1, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, marker<span class="op">=</span><span class="st">'.'</span>, label<span class="op">=</span><span class="st">'Fitted posterior'</span>,zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>ax_main.scatter(x2, y2, alpha<span class="op">=</span><span class="fl">0.5</span>, s<span class="op">=</span><span class="dv">20</span>, marker<span class="op">=</span><span class="st">'.'</span>, label<span class="op">=</span><span class="st">'Jeffreys posterior'</span>,zorder<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>ax_main.set_xlabel(<span class="vs">r'$\theta_1$'</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>ax_main.set_ylabel(<span class="vs">r'$\theta_2$'</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>ax_main.legend(loc<span class="op">=</span><span class="st">'upper left'</span>,fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>ax_main.grid()</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal histogram / KDE for alpha</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>ax_x_hist <span class="op">=</span> fig.add_subplot(gs[<span class="dv">0</span>, <span class="dv">0</span>:<span class="dv">3</span>], sharex<span class="op">=</span>ax_main)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>ax_x_hist.hist(x1, bins<span class="op">=</span><span class="st">'rice'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Fitted histogram'</span>,density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>ax_x_hist.hist(x2, bins<span class="op">=</span><span class="st">'rice'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Jeffreys histogram'</span>,density<span class="op">=</span><span class="va">True</span>,color<span class="op">=</span>vertCEA)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="bu">min</span>(x1.<span class="bu">min</span>(), x2.<span class="bu">min</span>()), <span class="bu">max</span>(x1.<span class="bu">max</span>(), x2.<span class="bu">max</span>()), <span class="dv">100</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>ax_x_hist.plot(x_vals, kde_x1(x_vals), color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted KDE'</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>ax_x_hist.plot(x_vals, kde_x2(x_vals), color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Jeffreys KDE'</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>ax_x_hist.set_ylabel(<span class="vs">r'Marginal $\theta_1$'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>ax_x_hist.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, labelbottom<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>ax_x_hist.legend()</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>ax_x_hist.grid()</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal histogram / KDE for beta</span></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>ax_y_hist <span class="op">=</span> fig.add_subplot(gs[<span class="dv">1</span>:<span class="dv">4</span>, <span class="dv">3</span>:<span class="dv">5</span>], sharey<span class="op">=</span>ax_main)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>ax_y_hist.hist(y1, bins<span class="op">=</span><span class="st">'rice'</span>, orientation<span class="op">=</span><span class="st">'horizontal'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Fitted histogram'</span>,density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>ax_y_hist.hist(y2, bins<span class="op">=</span><span class="st">'rice'</span>, orientation<span class="op">=</span><span class="st">'horizontal'</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, label<span class="op">=</span><span class="st">'Jeffreys histogram'</span>,density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.linspace(<span class="bu">min</span>(y1.<span class="bu">min</span>(), y2.<span class="bu">min</span>()), <span class="bu">max</span>(y1.<span class="bu">max</span>(), y2.<span class="bu">max</span>()), <span class="dv">100</span>)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>ax_y_hist.plot(kde_y1(y_vals), y_vals, color<span class="op">=</span><span class="st">'blue'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Fitted KDE'</span>)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>ax_y_hist.plot(kde_y2(y_vals), y_vals, color<span class="op">=</span><span class="st">'green'</span>, lw<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="st">'Jeffreys KDE'</span>)</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>ax_y_hist.set_xlabel(<span class="vs">r'Marginal $\theta_2$'</span>)</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>ax_y_hist.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, labelleft<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>ax_y_hist.legend()</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>ax_y_hist.grid()</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_post_constr_scatterhist" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_post_constr_scatterhist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_post_constr_scatterhist-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_post_constr_scatterhist-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Scatter histogram of the constrained fitted posterior and the target posterior distributions obtained from <span class="math inline">5000</span> samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The result on the constrained case (<a href="#fig-probit_post_constr_scatterhist" class="quarto-xref">Figure&nbsp;9</a>) is very similar to the unconstrained one.</p>
<p>Indeed, the priors had already comparable shapes. Altogether, one can observe that the variational inference approach yields close results to the numerical integration approach <span class="citation" data-cites="van2024reference">Van Biesbroeck et al. (<a href="#ref-van2024reference" role="doc-biblioref">2024</a>)</span>, with or without constraints, even tough the matching of the decay rates w.r.t. <span class="math inline">\theta_2</span> remains limited given the simple network that we have used in this case.</p>
<p>To ascertain the relevancy of our posterior approximation, we compute the posterior mean euclidean norm difference <span class="math inline">\mathbb{E}_{\theta}\left[ ||\theta - \theta_{true}||  \right]</span> as a function of the size of the dataset. In each computation, the neural network remains the same but the dataset changes by adding new entries.</p>
<p>Furthermore, in order to assess the stability of the stochastic optimization with respect to the random number generator (RNG) seed, we also compute the empirical cumulative distribution functions (ECDFs) for each posterior distribution. For every seed, the parameters of the neural network are expected to be different, we keep the same dataset for the MCMC sampling however.</p>
<p>Both types of computations are done in the unconstrained case as well as the constrained one. The different plots and details can be found in <a href="#sec-appendix" class="quarto-xref">Section&nbsp;6</a>.</p>
</section>
</section>
<section id="conclusion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Conclusion</h1>
<p>In this work, we developed an algorithm to perform variational approximation of reference priors using a generalized definition of mutual information based on <span class="math inline">f</span>-divergences. To enhance computational efficiency, we derived a lower bound of the generalized mutual information. Additionally, because reference priors often yield improper posteriors, we adapted the variational definition of the problem to incorporate constraints that ensure the posteriors are proper.</p>
<p>Numerical experiments have been carried out on various test cases of different complexities in order to validate our approach. These test cases range from purely toy models to more real-world problems, namely the estimation of seismic fragility curve parameters using a probit statistical model.</p>
<p>The results demonstrate the usefulness of our approach in estimating both prior and posterior distributions across various problems.</p>
<p>Our development is supported by an open source and flexible implementation, which can be adapted to a wide range of statistical models.</p>
<p>Looking forward, the approximation of the tails of the reference priors could be improved. That is a complex problem in the field of variational approximation, as well as the stability of the algorithm when using deeper networks. An extension of this work to the approximation of Maximal Data Information (MDI) priors is also appealing, thanks to the fact MDI are proper under certain assumptions precised in <span class="citation" data-cites="Bousquet2008">Bousquet (<a href="#ref-Bousquet2008" role="doc-biblioref">2008</a>)</span>.</p>
</section>
<section id="acknowledgement" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgement</h1>
<p>This research was supported by the CEA (French Alternative Energies and Atomic Energy Commission) and the SEISM Institute (https://www.institut-seism.fr/en/).</p>
</section>
<section id="sec-appendix" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Appendix</h1>
<section id="sec-grad_comp" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="sec-grad_comp"><span class="header-section-number">6.1</span> Gradient computation of the generalized mutual information</h2>
<p>We recall that <span class="math inline">F(x) = f(x)-xf'(x)</span> and <span class="math inline">p_{\lambda}</span> is a shortcut notation for <span class="math inline">p_{\pi_{\lambda}, N}</span> being the marginal distribution under <span class="math inline">\pi_{\lambda}</span>. The generalized mutual information writes : <span class="math display">
\begin{aligned}
    I_{\mathrm{D}_f}(\pi_{\lambda}; L_N) &amp; = \int_{\Theta}{\mathrm{D}_f}(p_{\lambda}||L_N(\cdot \, | \, \theta)) \pi_{\lambda}(\theta)d\theta \\
    &amp; = \int_{\Theta} \int_{\mathcal{X}^N} \pi_{\lambda}(\theta)L_N(\mathbf{X}\, | \, \theta)f\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\, | \, \theta)}\right) d\mathbf{X}d\theta.
\end{aligned}
</span> For each <span class="math inline">l</span>, taking the derivative with respect to <span class="math inline">\lambda_l</span> yields : <span class="math display">
\begin{aligned}
\frac{\partial I_{\mathrm{D}_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) &amp; = \int_{\Theta} \int_{\mathcal{X}^N} \frac{\partial \pi_{\lambda}}{\partial \lambda_l}(\theta)L_N(\mathbf{X}\, | \, \theta)f\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\, | \, \theta)}\right) d\mathbf{X}d\theta \\
&amp; + \int_{\Theta} \int_{\mathcal{X}^N} \pi_{\lambda}(\theta)L_N(\mathbf{X}\, | \, \theta)\frac{\partial p_{\lambda}}{\partial \lambda_l}\frac{1}{L_N(\mathbf{X}\, | \, \theta)}(\mathbf{X})f'\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\, | \, \theta)}\right) d\mathbf{X}d\theta,
\end{aligned}
</span> or in terms of expectations : <span class="math display">
\frac{\partial I_{\mathrm{D}_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \frac{\partial}{\partial \lambda_l} \mathbb{E}_{\theta \sim \pi_{\lambda}} \left[ \tilde{I}(\theta)  \right]
+ \mathbb{E}_{\theta \sim \pi_{\lambda}}\left[ \mathbb{E}_{\mathbf{X}\sim L_N(\cdot |\theta)}\left[ \frac{1}{L_N(\mathbf{X}\,|\,\theta)} \frac{\partial p_{\lambda}}{\partial \lambda_l}(\mathbf{X})f'\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right)\right]  \right],
</span> where : <span class="math display">
\tilde{I}(\theta) = \int_{\mathcal{X}^N} L_N(\mathbf{X}\, | \, \theta)f\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\, | \, \theta)}\right) d\mathbf{X}.
</span></p>
<p>We note that the derivative with respect to <span class="math inline">\lambda_l</span> does not apply to <span class="math inline">\tilde{I}</span> in the previous equation. Using the chain rule yields : <span class="math display">
\frac{\partial}{\partial \lambda_l} \mathbb{E}_{\theta \sim \pi_{\lambda}} \left[ \tilde{I}(\theta)  \right] = \frac{\partial}{\partial \lambda_l} \mathbb{E}_{\varepsilon} \left[ \tilde{I}(g(\lambda, \varepsilon)) \right] = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{I}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right].
</span></p>
<p>We have the following for every <span class="math inline">j \in \{1,...,q\}</span> : <span class="math display">
\begin{aligned}
\frac{\partial \tilde{I}}{\partial \theta_j}(\theta) &amp; = \int_{\mathcal{X}^N}   \frac{- p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}   \frac{\partial L_N}{\partial \theta_j}(\mathbf{X}\,|\,\theta)f'\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right) + f\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right) \frac{\partial L_N}{\partial \theta_j}(\mathbf{X}\,|\,\theta) d\mathbf{X}\\
&amp; = \int_{\mathcal{X}^N}F\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right) \frac{\partial L_N}{\partial \theta_j}(\mathbf{X}\,|\,\theta) d\mathbf{X}\\
&amp; =  \mathbb{E}_{\mathbf{X}\sim L_N(\cdot |\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\mathbf{X}\,|\,\theta) F\left( \frac{p_{\lambda}(\mathbf{X})}{L_N(\mathbf{X}\,|\,\theta)}\right)\right].
\end{aligned}
</span></p>
<p>Putting everything together, we finally obtain the desired formula. The gradient of the generalized lower bound function is obtained in a very similar manner.</p>
<p>In what follows, we prove that the gradient of <span class="math inline">I_{\mathrm{D}_f}</span>, as formulated in <a href="#eq-gradientIdf" class="quarto-xref">Equation&nbsp;10</a> aligns with the form of <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a>. We write, for <span class="math inline">l\in\{1,\dots,L\}</span>: <span class="math display">
    \frac{\partial I_{\mathrm{D}_f}}{\partial\lambda_l}(\pi_\lambda;L_N) = \mathbb{E}_\varepsilon \left[\sum_{j=1}^q\frac{\partial\tilde I}{\partial\theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l} (\lambda,\varepsilon) \right]
        + \mathcal{G}_l,
</span></p>
<p>where <span class="math display">
    \mathcal{G}_l=\mathbb{E}_{\theta\sim\pi_\lambda}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|\theta)}\left[\frac{1}{L_N(\mathbf{X}|\theta)}\frac{\partial p_\lambda}{\partial \lambda_l}(\mathbf{X})f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|\theta))} \right) \right].
</span></p>
<p>We remark that <span class="math display">
    \frac{\partial p_\lambda}{\partial \lambda_l} (\mathbf{X}) = \mathbb{E}_{\varepsilon_2} \sum_{j=1}^q\frac{\partial L_N}{\partial\theta_j} (\mathbf{X}|g(\lambda,\varepsilon_2))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2).
</span></p>
<p>Thus, we can develop <span class="math inline">\mathcal{G}_l</span> as: <span class="math display">
\begin{aligned}
        \mathcal{G}_l = &amp;\mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\mathbb{E}_{\varepsilon_2}\sum_{j}\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right)\frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)) \frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2)\\
        =&amp; \mathbb{E}_{\varepsilon_2}\mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\sum_{j}\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right)\frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)) \frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2)\\
         =&amp; \mathbb{E}_{\varepsilon_2}\sum_{j=1}^q\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2) \mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))} \frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right) \frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)).
\end{aligned}
</span></p>
<p>Now, calling <span class="math inline">\tilde K</span> the function defined as follows: <span class="math display">
    \tilde K:\theta\mapsto\tilde K(\theta) = \mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\left[\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right) L_N(\mathbf{X}|\theta)\right],
</span></p>
<p>we obtain that <span class="math display">
    \mathcal{G}_l = \mathbb{E}_{\varepsilon_2}\sum_{j=1}^q\frac{\partial g_j}{\partial\lambda_l}(\lambda,\varepsilon_2)\frac{\partial \tilde K}{\partial\theta_j} (g(\lambda,\varepsilon_2)).
</span></p>
<p>Eventually, denoting <span class="math inline">\tilde{\mathbf{I}}=\tilde K+\tilde I</span>, we have: <span class="math display">
    \frac{\partial I_{D_f}}{\partial \lambda_l}(\pi_\lambda;L_N) = \mathbb{E}_\varepsilon\left[\sum_{j=1}^q\frac{\partial\tilde{\mathbf{I}}}{\partial\theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial\lambda_l}(\lambda,\varepsilon)\right],
</span></p>
<p>which is compatible with the form of <a href="#eq-compatible_objective_function" class="quarto-xref">Equation&nbsp;9</a>.</p>
</section>
<section id="gaussian-distribution-with-variance-parameter" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="gaussian-distribution-with-variance-parameter"><span class="header-section-number">6.2</span> Gaussian distribution with variance parameter</h2>
<p>We consider a normal distribution where <span class="math inline">\theta</span> is the variance parameter : <span class="math inline">X_i \sim \mathcal{N}(\mu,\theta)</span> with <span class="math inline">\mu \in \mathbb{R}</span>, <span class="math inline">\mathbf{X}\in \mathcal{X}^N = \mathbb{R}^N</span> and <span class="math inline">\theta \in \mathbb{R}^*_{+}</span>. We take <span class="math inline">\mu=0</span>. The likelihood and score functions are : <span class="math display">  
L_N(\mathbf{X}\,|\,\theta) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi \theta}} \exp \left( -\frac{1}{2\theta}(X_i - \mu)^2 \right)
</span></p>
<p><span class="math display">
\frac{\partial \log L_N}{\partial \theta}(\mathbf{X}\,|\,\theta)  = - \frac{N}{2\theta} + \frac{1}{2\theta^2} \sum_{i=1}^N (X_i - \mu)^2.
</span></p>
<p>The MLE is available : <span class="math inline">\hat{\theta}_{MLE} = \frac{1}{N} \sum_{i=1}^N X_i</span>. However, the Jeffreys prior is an improper distribution in this case : <span class="math inline">J(\theta) \propto 1/\theta</span>. Nevertheless, the Jeffreys posterior is a proper inverse-gamma distribution: <span class="math display">
J_{post}(\theta \, | \ \mathbf{X}) = \Gamma^{-1} \left(\theta; \frac{N}{2}, \frac{1}{2} \sum_{i=1}^N(X_i - \mu)^2  \right).
</span></p>
<p>We use a neural network with one layer and a Softplus activation function. The dimension of the latent variable <span class="math inline">\varepsilon</span> is <span class="math inline">p=10</span>.</p>
<div id="ea69f8b7" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters and classes</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">10</span>    <span class="co"># latent space dimension</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">1</span>     <span class="co"># theta dimension</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span>    <span class="co"># normal parameter (mean)</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10</span>    <span class="co"># number of data samples</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> <span class="dv">1000</span>  <span class="co"># nb samples for MC estimation in MI</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">50</span>    <span class="co"># nb samples MC marginal likelihood</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>Normal <span class="op">=</span> torch_NormalModel_variance(mu<span class="op">=</span>mu)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>low <span class="op">=</span> <span class="fl">0.0001</span>        <span class="co"># lower bound</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>upp <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> low      <span class="co"># upper bound (not relevant here)</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> p</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Normal_results_unconstrained.pkl'</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">0</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>nn.Softplus())</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> AffineTransformation(NN, m_low<span class="op">=</span>low, M_upp<span class="op">=</span>upp)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Normal)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, use_log_lik<span class="op">=</span><span class="va">True</span>, use_baseline<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    theta_sample_init <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">7000</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>loss_fct <span class="op">=</span> <span class="st">"LB_MI"</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>num_samples_MI <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>freq_MI <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>save_best_param <span class="op">=</span> <span class="va">True</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.025</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_Normal_nocstr :</span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training loop</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>    MI, range_MI, lower_MI, upper_MI <span class="op">=</span> Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>                                                            num_samples_MI, freq_MI, save_best_param, </span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>                                                            learning_rate, momentum<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    theta_sample <span class="op">=</span> Div.va.implicit_prior_sampler(n_samples_prior)</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>        theta_sample <span class="op">=</span> theta_sample.numpy()</span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample data from 'true' parameter</span></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>    theta_true <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)  </span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> Normal.sample(theta_true, N, <span class="dv">1</span>)</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> D[:, <span class="dv">0</span>]</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Posterior samples using Jeffreys prior</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>    n_samples_post <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>    jeffreys_post <span class="op">=</span> Normal.sample_post_Jeffreys(X, n_samples_post)</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>    jeffreys_post <span class="op">=</span> np.reshape(jeffreys_post, (n_samples_post,q))</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>    T_mcmc <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>    sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>    eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>    eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a>    theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> theta_MH.numpy()</span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a>        jeffreys_post <span class="op">=</span> jeffreys_post.numpy()</span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>    theta_post <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:]</span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>    theta_post <span class="op">=</span> np.reshape(theta_post, (n_samples_post,q))</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saves all relevant quantities</span></span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>    Mutual_Info <span class="op">=</span> {<span class="st">'values'</span> : MI, <span class="st">'range'</span> : range_MI, <span class="st">'lower'</span> : lower_MI, <span class="st">'upper'</span> : upper_MI}</span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>    Prior <span class="op">=</span> {<span class="st">'samples'</span> : theta_sample, <span class="st">'jeffreys'</span> : jeffreys_sample, <span class="st">'params'</span> : all_params}</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a>    Posterior <span class="op">=</span> {<span class="st">'samples'</span> : theta_post, <span class="st">'jeffreys'</span> : jeffreys_post, <span class="st">'Markov'</span> : theta_MH}</span>
<span id="cb17-77"><a href="#cb17-77" aria-hidden="true" tabindex="-1"></a>    Normal_results_nocstr <span class="op">=</span> {<span class="st">'MI'</span> : Mutual_Info, <span class="st">'prior'</span> : Prior, <span class="st">'post'</span> : Posterior}</span>
<span id="cb17-78"><a href="#cb17-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a>        pickle.dump(Normal_results_nocstr, <span class="bu">file</span>)</span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>        MI_dic <span class="op">=</span> res[<span class="st">'MI'</span>]</span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a>        Prior <span class="op">=</span> res[<span class="st">'prior'</span>] </span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>        Posterior <span class="op">=</span> res[<span class="st">'post'</span>]</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI <span class="op">=</span> MI_dic[<span class="st">'values'</span>], MI_dic[<span class="st">'range'</span>], MI_dic[<span class="st">'lower'</span>], MI_dic[<span class="st">'upper'</span>]</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a>        theta_sample, jeffreys_sample, all_params <span class="op">=</span> Prior[<span class="st">'samples'</span>], Prior[<span class="st">'jeffreys'</span>], Prior[<span class="st">'params'</span>]</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>        theta_post, jeffreys_post, theta_MH <span class="op">=</span> Posterior[<span class="st">'samples'</span>], Posterior[<span class="st">'jeffreys'</span>], Posterior[<span class="st">'Markov'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-normal_prior" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(range_MI, MI, <span class="st">'-'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(range_MI, MI, <span class="st">'*'</span>, color<span class="op">=</span>rougeCEA)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(range_MI, upper_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(range_MI, lower_MI, <span class="st">'-'</span>, color<span class="op">=</span><span class="st">'lightgrey'</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].fill_between(range_MI, lower_MI, upper_MI, color<span class="op">=</span><span class="st">'lightgrey'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_xlabel(<span class="vs">r"Epochs"</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="vs">r"Generalized mutual information"</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].grid()</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].hist(theta_sample_init, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="vs">r"Initial prior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].hist(theta_sample, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Fitted prior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].grid()</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.tight_layout()</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-normal_prior" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal_prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-normal_prior-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal_prior-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Left : Monte Carlo estimation of the generalized mutual information with <span class="math inline">\alpha=0.5</span> (from 200 samples) for <span class="math inline">\pi_{\lambda_e}</span> where <span class="math inline">\lambda_e</span> is the parameter of the neural network at epoch <span class="math inline">e</span>. The red curve is the mean value and the gray zone is the 95% confidence interval. Right : Histograms of the initial prior (at epoch 0) and the fitted prior (after training), each one is obtained from <span class="math inline">10^5</span> samples. The learning rate used in the optimization is <span class="math inline">0.025</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We retrieve close results to those of <span class="citation" data-cites="gauchy_var_rp">Gauchy et al. (<a href="#ref-gauchy_var_rp" role="doc-biblioref">2023</a>)</span>, even though we used the <span class="math inline">\alpha</span>-divergence instead of the classic KL-divergence (<a href="#fig-normal_prior" class="quarto-xref">Figure&nbsp;10</a>). The evolution of the mutual information seems to be more stable during training. We can not however directly compare our result to the target Jeffrey prior since the latter is improper.</p>
<p>For the posterior distribution, we sample 10 times from the normal distribution using <span class="math inline">\theta_{true} = 1</span>.</p>
<div id="cell-fig-normal_post" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">3</span>))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].plot(theta_MH)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].set_ylabel(<span class="vs">r'$\theta$'</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].grid()</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].hist(theta_post, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Fitted posterior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].hist(jeffreys_post, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="st">"rice"</span>, label<span class="op">=</span><span class="vs">r"Jeffreys"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>, color<span class="op">=</span>vertCEA)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].grid()</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-normal_post" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-normal_post-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Left : Markov chain during the Metropolis-Hastings iterations. Right : Histograms of the fitted posterior and the Jeffreys posterior, each one is obtained from <span class="math inline">5\cdot10^4</span> samples.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As <a href="#fig-normal_post" class="quarto-xref">Figure&nbsp;11</a> shows, we obtain a parametric posterior distribution which closely resembles the target distribution, even though the theoretical prior is improper.</p>
<p>In order to evaluate the performance of the algorithm for the prior, we have to add a constraint. The simplest kind of constraints are moment constraints with : <span class="math inline">a(\theta) = \theta^{\beta}</span>, however, we can not use such a constraint here since the integrals for <span class="math inline">\mathcal{K}</span> and <span class="math inline">c</span> from section 2 would diverge either at <span class="math inline">0</span> or at <span class="math inline">+\infty</span>.</p>
<p>If we define : <span class="math inline">a(\theta) = \displaystyle \frac{1}{\theta^{\beta}+\theta^{\tau}}</span> with <span class="math inline">\beta &lt; 0 &lt; \tau</span>, then the integrals for <span class="math inline">\mathcal{K}</span> and <span class="math inline">c</span> are finite, because : <span class="math display">
\forall \, \delta \geq 1, \quad \int_0^{+\infty} \frac{1}{\theta}\cdot \left(\frac{1}{\theta^{\beta}+\theta^{\tau}} \right)^{\delta} d\theta \leq \frac{1}{\delta}\left( \frac{1}{\tau} - \frac{1}{\beta}\right) .
</span></p>
<p>This function of constraint <span class="math inline">a</span> is preferable because it yields different asymptotic rates at <span class="math inline">0</span> and <span class="math inline">+\infty</span> : <span class="math display">
   \begin{cases} \displaystyle
a(\theta) \sim \theta^{-\beta} \quad \text{as} \quad  \theta \longrightarrow 0 \\
a(\theta) \sim \theta^{-\tau} \quad \text{as} \quad  \theta \longrightarrow +\infty.
\end{cases}
</span></p>
<p>In order to apply the algorithm, we are interested in finding : <span class="math display">
\mathcal{K} = \int_0^{+\infty} \frac{1}{\theta}\cdot a(\theta)^{1/\alpha} d\theta \quad \text{and} \quad  c = \int_0^{+\infty} \frac{1}{\theta}\cdot a(\theta)^{1+(1/\alpha)} d\theta.
</span></p>
<p>For instance, let <span class="math inline">\alpha=1/2</span>. If <span class="math inline">\beta=-1</span>, <span class="math inline">\tau=1</span>, then <span class="math inline">\mathcal{K} = 1/2</span> and <span class="math inline">c = \pi/16</span>. The constraint value is <span class="math inline">c/\mathcal{K} = \pi/8</span>. Thus, for this example, we only have to apply the third step of the proposed method. We use in this case a one-layer neural network with <span class="math inline">\exp</span> as the activation function, the parametric set of priors corresponds to log-normal distributions.</p>
<div id="78c2257d" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters and classes</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">10</span>     <span class="co"># latent space dimension</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">1</span>      <span class="co"># theta dimension</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span>     <span class="co"># normal mean parameter</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10</span>     <span class="co"># number of data samples</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>J <span class="op">=</span> <span class="dv">1000</span>   <span class="co"># nb samples for MC estimation in MI</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">50</span>     <span class="co"># nb samples MC marginal likelihood</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>Normal <span class="op">=</span> torch_NormalModel_variance(mu<span class="op">=</span>mu)  </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> p  </span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>low <span class="op">=</span> <span class="fl">0.0001</span>        <span class="co"># lower bound</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>upp <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> low      <span class="co"># upper bound (not relevant)</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>name_file <span class="op">=</span> <span class="st">'Normal_results_constrained.pkl'</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>file_path <span class="op">=</span> os.path.join(path_plot, name_file)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Constraint function parameter</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> torch.tensor([<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>tau <span class="op">=</span> torch.tensor([<span class="dv">1</span>])</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co"># beta = -tau = -1, yields K = 0.5</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># beta = -tau = -1/3, yields K = 1.5</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>constr_val <span class="op">=</span> np.pi <span class="op">/</span> <span class="dv">8</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor([[constr_val]]) </span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>T_cstr <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>eta_augm <span class="op">=</span> torch.tensor([[<span class="fl">1.0</span>]])</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>eta <span class="op">=</span> torch.tensor([[<span class="fl">1.</span>]])</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">0</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> SingleLinear(input_size, output_size, m1<span class="op">=</span><span class="dv">0</span>, s1<span class="op">=</span><span class="fl">0.1</span>, b1<span class="op">=</span><span class="dv">0</span>, act1<span class="op">=</span>torch.exp)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>NN <span class="op">=</span> AffineTransformation(NN, m_low<span class="op">=</span>low, M_upp<span class="op">=</span>upp)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>VA <span class="op">=</span> VA_NeuralNet(neural_net<span class="op">=</span>NN, model<span class="op">=</span>Normal)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="co">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="co">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>Div <span class="op">=</span> DivMetric_NeuralNet(va<span class="op">=</span>VA, T<span class="op">=</span>T, use_alpha<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span>alpha, use_log_lik<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>Constr <span class="op">=</span> Constraints_NeuralNet(div<span class="op">=</span>Div, betas<span class="op">=</span>beta, b<span class="op">=</span>b, T_cstr<span class="op">=</span>T_cstr, </span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>                               objective<span class="op">=</span><span class="st">'LB_MI'</span>, lag_method<span class="op">=</span><span class="st">'augmented'</span>, eta_augm<span class="op">=</span>eta_augm, rule<span class="op">=</span><span class="st">'SGD'</span>,moment<span class="op">=</span><span class="st">'alter'</span>, taus<span class="op">=</span>tau)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch_Adam</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>num_samples_MI <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>freq_MI <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>save_best_param <span class="op">=</span> <span class="va">False</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.0005</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>freq_augm <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_prior(theta, beta, tau, K, alpha):</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (K<span class="op">**-</span><span class="dv">1</span> <span class="op">/</span> theta) <span class="op">*</span> (theta<span class="op">**</span>beta <span class="op">+</span> theta<span class="op">**</span>tau)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>alpha)</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> load_results_Normal_cstr :</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    <span class="co">### Training loop</span></span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    MI, constr_values, range_MI, lower_MI, upper_MI <span class="op">=</span> Constr.Partial_autograd(J, N, eta, num_epochs, optimizer, num_samples_MI, </span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>                                                            freq_MI, save_best_param, learning_rate, momentum<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>                                                            freq_augm<span class="op">=</span>freq_augm, max_violation<span class="op">=</span><span class="fl">0.005</span>, update_eta_augm<span class="op">=</span><span class="fl">2.</span>)</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        theta_sample <span class="op">=</span> Constr.va.implicit_prior_sampler(n_samples_prior).numpy()</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Moment estimation : </span><span class="sc">{</span>np<span class="sc">.</span>mean((theta_sample<span class="op">**</span>beta.item()<span class="op">+</span>theta_sample<span class="op">**</span>tau.item())<span class="op">**-</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">, wanted value : </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    all_params <span class="op">=</span> torch.cat([param.view(<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> param <span class="kw">in</span> NN.parameters()])</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>    seed_all(<span class="dv">0</span>)</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample data from 'true' parameter</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>    theta_true <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)  </span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> Normal.sample(theta_true, N, <span class="dv">1</span>)</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> D[:, <span class="dv">0</span>]</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>    a_X <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> N</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>    b_X <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>(X.numpy()<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> scipy.stats.invgamma.rvs(a_X, <span class="dv">0</span>, b_X, <span class="dv">10</span><span class="op">**</span><span class="dv">6</span>)</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>    cste_post <span class="op">=</span> np.mean((Y<span class="op">**</span>beta.item() <span class="op">+</span> Y<span class="op">**</span>tau.item())<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>alpha))</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Normalizing constant : </span><span class="sc">{</span>cste_post<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> target_posterior(theta, beta, tau, alpha, cste):</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> scipy.stats.invgamma.pdf(theta, a_X, <span class="dv">0</span>, b_X) <span class="op">*</span> (theta<span class="op">**</span>beta <span class="op">+</span> theta<span class="op">**</span>tau)<span class="op">**</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>alpha) <span class="op">/</span> cste</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>    interv <span class="op">=</span> np.linspace(<span class="fl">0.01</span>,<span class="dv">10</span>,<span class="dv">10000</span>)</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>    target_post_values <span class="op">=</span> target_posterior(interv,beta.item(),tau.item(),alpha,cste_post)</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>    n_samples_post <span class="op">=</span> <span class="dv">5</span><span class="op">*</span><span class="dv">10</span><span class="op">**</span><span class="dv">4</span></span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>    T_mcmc <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">5</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>    sigma2_0 <span class="op">=</span> torch.tensor(<span class="fl">1.</span>)</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>    eps_0 <span class="op">=</span> torch.randn(p)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>    eps_MH, batch_acc <span class="op">=</span> VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap<span class="op">=</span><span class="va">True</span>, Cov<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>    theta_MH <span class="op">=</span> NN(eps_MH)</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>        theta_MH <span class="op">=</span> theta_MH.numpy()</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>    theta_post <span class="op">=</span> theta_MH[<span class="op">-</span>n_samples_post:]</span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>    theta_post <span class="op">=</span> np.reshape(theta_post, n_samples_post)</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Saves all relevant quantities</span></span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>    Mutual_Info <span class="op">=</span> {<span class="st">'values'</span> : MI, <span class="st">'range'</span> : range_MI, <span class="st">'lower'</span> : lower_MI, <span class="st">'upper'</span> : upper_MI}</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a>    Prior <span class="op">=</span> {<span class="st">'samples'</span> : theta_sample, <span class="st">'jeffreys'</span> : jeffreys_sample, <span class="st">'params'</span> : all_params}</span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>    Posterior <span class="op">=</span> {<span class="st">'samples'</span> : theta_post, <span class="st">'jeffreys'</span> : jeffreys_post, <span class="st">'target'</span> : target_post_values}</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>    Normal_results_cstr <span class="op">=</span> {<span class="st">'MI'</span> : Mutual_Info, <span class="st">'prior'</span> : Prior, <span class="st">'post'</span> : Posterior}</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'wb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>        pickle.dump(Normal_results_cstr, <span class="bu">file</span>)</span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span> :</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>        MI_dic <span class="op">=</span> res[<span class="st">'MI'</span>]</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a>        Prior <span class="op">=</span> res[<span class="st">'prior'</span>] </span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a>        Posterior <span class="op">=</span> res[<span class="st">'post'</span>]</span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>        MI, range_MI, lower_MI, upper_MI <span class="op">=</span> MI_dic[<span class="st">'values'</span>], MI_dic[<span class="st">'range'</span>], MI_dic[<span class="st">'lower'</span>], MI_dic[<span class="st">'upper'</span>]</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>        theta_sample, jeffreys_sample, all_params <span class="op">=</span> Prior[<span class="st">'samples'</span>], Prior[<span class="st">'jeffreys'</span>], Prior[<span class="st">'params'</span>]</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a>        theta_post, jeffreys_post, target_post_values <span class="op">=</span> Posterior[<span class="st">'samples'</span>], Posterior[<span class="st">'jeffreys'</span>], Posterior[<span class="st">'target'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-normal_prior_constr" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>interv <span class="op">=</span> np.linspace(<span class="fl">0.01</span>,<span class="dv">10</span>,<span class="dv">10000</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.hist(theta_sample, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="dv">500</span>, label<span class="op">=</span><span class="vs">r"Fitted prior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.plot(interv, target_prior(interv,beta.item(),tau.item(),K,alpha), label<span class="op">=</span><span class="st">"Target prior"</span>,color<span class="op">=</span>vertCEA)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.1</span>,<span class="dv">10</span>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-normal_prior_constr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal_prior_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-normal_prior_constr-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal_prior_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Histogram of the constrained fitted prior obtained from <span class="math inline">10^5</span> samples, and density function of the target prior. The learning rate used in the optimization is <span class="math inline">0.0005</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>In this case we are able to compare prior distributions since both are proper, as <a href="#fig-normal_prior_constr" class="quarto-xref">Figure&nbsp;12</a> shows, we recover a relevant result using our algorithm even with added constraints.</p>
<p>The density function of the posterior is known up to a multiplicative constant, more precisely, it corresponds to the product of the constraint function and the density function of an inverse-gamma distribution. Hence, the constant can be estimated using Monte Carlo samples from the inverse-gamma distribution in question. We apply the same approach as before in order to obtain the posterior from the parametric prior.</p>
<div id="cell-fig-normal_post_constr" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>interv <span class="op">=</span> np.linspace(<span class="fl">0.01</span>,<span class="dv">10</span>,<span class="dv">10000</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.hist(theta_post, density<span class="op">=</span><span class="va">True</span>, bins<span class="op">=</span><span class="dv">500</span>, label<span class="op">=</span><span class="vs">r"Fitted posterior"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.plot(interv, target_post_values, label<span class="op">=</span><span class="st">"Target posterior"</span>,color<span class="op">=</span>vertCEA)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\theta$"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="fl">0.1</span>,<span class="dv">6</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-normal_post_constr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-normal_post_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-normal_post_constr-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-normal_post_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Histogram of the fitted posterior obtained from <span class="math inline">5\cdot10^4</span> samples, and density function of the target posterior.
</figcaption>
</figure>
</div>
</div>
</div>
<p>As shown in <a href="#fig-normal_post_constr" class="quarto-xref">Figure&nbsp;13</a>, the parametric posterior has a shape similar to the theoretical distribution.</p>
</section>
<section id="probit-model-and-robustness" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="probit-model-and-robustness"><span class="header-section-number">6.3</span> Probit model and robustness</h2>
<p>As mentioned in <a href="#sec-probit_model" class="quarto-xref">Section&nbsp;4.2</a> regarding the probit model, we present several additional results.</p>
<div id="cell-fig-quad_error_post" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>num_M <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>N_max <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>N_init <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>tab_N <span class="op">=</span> np.arange(N_init, N_max<span class="op">+</span>N_init, <span class="dv">5</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>QE <span class="op">=</span> np.zeros((<span class="bu">len</span>(tab_N), num_M))</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>file_name <span class="op">=</span> os.path.join(varp_probit_path, <span class="st">'Quadratic_errors/Quad_error_post'</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>first_file <span class="op">=</span> file_name <span class="op">+</span> <span class="st">'1.pkl'</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> M <span class="kw">in</span> <span class="bu">range</span>(num_M):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span> <span class="op">=</span> file_name <span class="op">+</span> <span class="ss">f'</span><span class="sc">{</span>M<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.pkl'</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="bu">file</span>, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>        QE[:,M] <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>mean_vals <span class="op">=</span> np.mean(QE, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>lower_quantile <span class="op">=</span> np.quantile(QE, <span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>upper_quantile <span class="op">=</span> np.quantile(QE, <span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">1</span>)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>theta_vrai <span class="op">=</span> np.array([<span class="fl">3.37610525</span>, <span class="fl">0.43304097</span>])</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>N_max <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>N_init <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>tab_N2 <span class="op">=</span> np.arange(N_init, N_max<span class="op">+</span>N_init, <span class="dv">5</span>)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>num_MC <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>num_MC <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>err_jeff <span class="op">=</span> np.zeros((<span class="bu">len</span>(tab_N2), M))</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tab_N2 : </span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    resultats_theta_post_N <span class="op">=</span> np.zeros((M, num_MC, <span class="dv">2</span>))</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(M), desc<span class="op">=</span><span class="ss">f'Iterations for N=</span><span class="sc">{</span>N<span class="sc">}</span><span class="ss">'</span>, disable<span class="op">=</span><span class="va">True</span>): </span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>        file_path <span class="op">=</span> os.path.join(int_jeffreys_path, <span class="ss">f'model_J_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>            Jeffreys <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        resultats_theta_post_N[i] <span class="op">=</span> Jeffreys[<span class="st">'logs'</span>][<span class="st">'post'</span>][N]</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>        err_jeff[j, i] <span class="op">=</span> np.mean(np.linalg.norm(resultats_theta_post_N[i] <span class="op">-</span> theta_vrai[np.newaxis], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb23-35"><a href="#cb23-35" aria-hidden="true" tabindex="-1"></a>    j <span class="op">=</span> j <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb23-36"><a href="#cb23-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-37"><a href="#cb23-37" aria-hidden="true" tabindex="-1"></a>mean_vals2 <span class="op">=</span> np.mean(err_jeff, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-38"><a href="#cb23-38" aria-hidden="true" tabindex="-1"></a>lower_quantile2 <span class="op">=</span> np.quantile(err_jeff, <span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-39"><a href="#cb23-39" aria-hidden="true" tabindex="-1"></a>upper_quantile2 <span class="op">=</span> np.quantile(err_jeff, <span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-40"><a href="#cb23-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-41"><a href="#cb23-41" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb23-42"><a href="#cb23-42" aria-hidden="true" tabindex="-1"></a>plt.plot(tab_N, mean_vals, label<span class="op">=</span><span class="st">'Mean Approx'</span>)</span>
<span id="cb23-43"><a href="#cb23-43" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tab_N, lower_quantile, upper_quantile, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI Approx'</span>)</span>
<span id="cb23-44"><a href="#cb23-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-45"><a href="#cb23-45" aria-hidden="true" tabindex="-1"></a>plt.plot(tab_N2, mean_vals2, label<span class="op">=</span><span class="st">'Mean Jeffreys'</span>)</span>
<span id="cb23-46"><a href="#cb23-46" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color<span class="op">=</span><span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI Jeffreys'</span>)</span>
<span id="cb23-47"><a href="#cb23-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'N'</span>)</span>
<span id="cb23-48"><a href="#cb23-48" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean Norm Difference'</span>)</span>
<span id="cb23-49"><a href="#cb23-49" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb23-50"><a href="#cb23-50" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb23-51"><a href="#cb23-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-quad_error_post" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quad_error_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-quad_error_post-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quad_error_post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Mean norm difference as a function of the size <span class="math inline">N</span> of the dataset for the unconstrained fitted posterior and the Jeffreys posterior. For each value of <span class="math inline">N</span>, 10 different datasets are considered from which we obtain 95% confidence intervals.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-quad_error_post_constr" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>num_M <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>N_max <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>N_init <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>tab_N <span class="op">=</span> np.arange(N_init, N_max<span class="op">+</span>N_init, <span class="dv">5</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>QE <span class="op">=</span> np.zeros((<span class="bu">len</span>(tab_N), num_M))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>file_name <span class="op">=</span> os.path.join(varp_probit_path, <span class="st">'Quadratic_errors/Quad_error_post_constr'</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>first_file <span class="op">=</span> file_name <span class="op">+</span> <span class="st">'1.pkl'</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> M <span class="kw">in</span> <span class="bu">range</span>(num_M):</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span> <span class="op">=</span> file_name <span class="op">+</span> <span class="ss">f'</span><span class="sc">{</span>M<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.pkl'</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="bu">file</span>, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        QE[:,M] <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>mean_vals <span class="op">=</span> np.mean(QE, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>lower_quantile <span class="op">=</span> np.quantile(QE, <span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>upper_quantile <span class="op">=</span> np.quantile(QE, <span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>seed_all(<span class="dv">1</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>theta_vrai <span class="op">=</span> np.array([<span class="fl">3.37610525</span>, <span class="fl">0.43304097</span>])</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>N_max <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>N_init <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>tab_N2 <span class="op">=</span> np.arange(N_init, N_max<span class="op">+</span>N_init, <span class="dv">5</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>num_MC <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>num_MC <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>err_jeff <span class="op">=</span> np.zeros((<span class="bu">len</span>(tab_N2), M))</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>j <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> N <span class="kw">in</span> tab_N2 : </span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    resultats_theta_post_N <span class="op">=</span> np.zeros((M, num_MC, <span class="dv">2</span>))</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> tqdm(<span class="bu">range</span>(M), desc<span class="op">=</span><span class="ss">f'Iterations for N=</span><span class="sc">{</span>N<span class="sc">}</span><span class="ss">'</span>, disable<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        file_path <span class="op">=</span> os.path.join(int_jeffreys_path, <span class="ss">f'model_J_constraint_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>) </span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(file_path, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>            Jeffreys <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        resultats_theta_post_N[i] <span class="op">=</span> Jeffreys[<span class="st">'logs'</span>][<span class="st">'post'</span>][N]</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>        err_jeff[j, i] <span class="op">=</span> np.mean(np.linalg.norm(resultats_theta_post_N[i] <span class="op">-</span> theta_vrai[np.newaxis], axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    j <span class="op">=</span> j <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a>mean_vals2 <span class="op">=</span> np.mean(err_jeff, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>lower_quantile2 <span class="op">=</span> np.quantile(err_jeff, <span class="fl">0.025</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-39"><a href="#cb24-39" aria-hidden="true" tabindex="-1"></a>upper_quantile2 <span class="op">=</span> np.quantile(err_jeff, <span class="fl">0.975</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb24-40"><a href="#cb24-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-41"><a href="#cb24-41" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>))</span>
<span id="cb24-42"><a href="#cb24-42" aria-hidden="true" tabindex="-1"></a>plt.plot(tab_N, mean_vals, label<span class="op">=</span><span class="st">'Mean Approx'</span>)</span>
<span id="cb24-43"><a href="#cb24-43" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tab_N, lower_quantile, upper_quantile, color<span class="op">=</span><span class="st">'b'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI Approx'</span>)</span>
<span id="cb24-44"><a href="#cb24-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-45"><a href="#cb24-45" aria-hidden="true" tabindex="-1"></a>plt.plot(tab_N2, mean_vals2, label<span class="op">=</span><span class="st">'Mean Jeffreys'</span>)</span>
<span id="cb24-46"><a href="#cb24-46" aria-hidden="true" tabindex="-1"></a>plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color<span class="op">=</span><span class="st">'orange'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'95% CI Jeffreys'</span>)</span>
<span id="cb24-47"><a href="#cb24-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'N'</span>)</span>
<span id="cb24-48"><a href="#cb24-48" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Mean Norm Difference'</span>)</span>
<span id="cb24-49"><a href="#cb24-49" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb24-50"><a href="#cb24-50" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb24-51"><a href="#cb24-51" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-quad_error_post_constr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-quad_error_post_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-quad_error_post_constr-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-quad_error_post_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Mean norm difference as a function of the size <span class="math inline">N</span> of the dataset for the constrained fitted posterior and the Jeffreys posterior. For each value of <span class="math inline">N</span>, 10 different datasets are considered from which we obtain 95% confidence intervals.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-quad_error_post" class="quarto-xref">Figure&nbsp;14</a> and <a href="#fig-quad_error_post_constr" class="quarto-xref">Figure&nbsp;15</a> show the evolution of the posterior mean norm difference as the size <span class="math inline">N</span> of the dataset considered for the posterior distribution increases. For each value of <span class="math inline">N</span>, 10 different datasets are used in order to quantify the variability of said error. The proportion of degenerate datasets is rather high when <span class="math inline">N=5</span> or <span class="math inline">N=10</span>, the consequence is that the approximation tends to be more unstable. The main observation is that the error is decreasing in all cases when <span class="math inline">N</span> increases, also, the behaviour of the error for the fitted distributions on one hand, and the behaviour for the Jeffreys distribution on the other hand are quite similar in terms of mean value and confidence intervals.</p>
<div id="cell-fig-probit_ecdf" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>n_exp <span class="op">=</span> <span class="dv">100</span>   <span class="co"># Number of experiments</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> os.path.join(varp_probit_path, <span class="st">'probit_samples/probit'</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>first_file <span class="op">=</span> model <span class="op">+</span> <span class="st">'_seed_1.pkl'</span> </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>n_samples_post <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate results from all experiments</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>theta_prior <span class="op">=</span> np.zeros((n_exp,n_samples_prior,q))</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>jeffreys_prior <span class="op">=</span> np.zeros((n_exp,n_samples_prior,q))</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>theta_post <span class="op">=</span> np.zeros((n_exp,n_samples_post,q))</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>jeffreys_post <span class="op">=</span> np.zeros((n_exp,n_samples_post,q))</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_exp):</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    file_name <span class="op">=</span> model <span class="op">+</span> <span class="ss">f'_seed</span><span class="sc">{</span>k<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.pkl'</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_name, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>        theta_prior[k,:,:] <span class="op">=</span> data[<span class="st">'prior'</span>][<span class="st">'VA'</span>] </span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#jeffreys_prior[k,:,:] = data['prior']['Jeffreys']</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        theta_post[k,:,:] <span class="op">=</span> data[<span class="st">'post'</span>][<span class="st">'VA'</span>]</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        jeffreys_post[k,:,:] <span class="op">=</span> data[<span class="st">'post'</span>][<span class="st">'Jeffreys'</span>]</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Post CDF computations </span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>num_x <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>x_values_alpha <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">8.</span>, num_x)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>x_values_beta <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">2.</span>, num_x)</span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>cdf_post_alpha <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>cdf_post_beta <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>jeff_post_alpha <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>jeff_post_beta <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_exp):</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    cdf_post_alpha[k,:] <span class="op">=</span> compute_ecdf(theta_post[k,:,<span class="dv">0</span>], x_values_alpha)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    cdf_post_beta[k,:] <span class="op">=</span> compute_ecdf(theta_post[k,:,<span class="dv">1</span>], x_values_beta)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    jeff_post_alpha[k,:] <span class="op">=</span> compute_ecdf(jeffreys_post[k,:,<span class="dv">0</span>], x_values_alpha)</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    jeff_post_beta[k,:] <span class="op">=</span> compute_ecdf(jeffreys_post[k,:,<span class="dv">1</span>], x_values_beta)</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>lower_bound_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">0.</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>upper_bound_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">1.0</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a>median_cdf_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a>median_jeff_alpha <span class="op">=</span> np.quantile(jeff_post_alpha, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a>lower_bound_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">0.</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>upper_bound_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">1.</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-47"><a href="#cb25-47" aria-hidden="true" tabindex="-1"></a>median_cdf_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-48"><a href="#cb25-48" aria-hidden="true" tabindex="-1"></a>median_jeff_beta <span class="op">=</span> np.quantile(jeff_post_beta, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb25-49"><a href="#cb25-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-50"><a href="#cb25-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Create subplots for alpha and beta</span></span>
<span id="cb25-51"><a href="#cb25-51" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">3</span>))</span>
<span id="cb25-52"><a href="#cb25-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-53"><a href="#cb25-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for alpha</span></span>
<span id="cb25-54"><a href="#cb25-54" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values_alpha, median_cdf_alpha, label<span class="op">=</span><span class="st">'Median fitted ECDF'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb25-55"><a href="#cb25-55" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values_alpha, median_jeff_alpha, label<span class="op">=</span><span class="st">'Jeffreys ECDF'</span>, color<span class="op">=</span><span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">2</span>) </span>
<span id="cb25-56"><a href="#cb25-56" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'ECDF Band'</span>)</span>
<span id="cb25-57"><a href="#cb25-57" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="vs">r'$\theta_1$'</span>)</span>
<span id="cb25-58"><a href="#cb25-58" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'CDF'</span>)</span>
<span id="cb25-59"><a href="#cb25-59" aria-hidden="true" tabindex="-1"></a>ax1.grid()</span>
<span id="cb25-60"><a href="#cb25-60" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-61"><a href="#cb25-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-62"><a href="#cb25-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for beta</span></span>
<span id="cb25-63"><a href="#cb25-63" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values_beta, median_cdf_beta, label<span class="op">=</span><span class="st">'Median fitted ECDF'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb25-64"><a href="#cb25-64" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values_beta, median_jeff_beta, label<span class="op">=</span><span class="st">'Jeffreys ECDF'</span>, color<span class="op">=</span><span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)  </span>
<span id="cb25-65"><a href="#cb25-65" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'ECDF Band'</span>)</span>
<span id="cb25-66"><a href="#cb25-66" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="vs">r'$\theta_2$'</span>)</span>
<span id="cb25-67"><a href="#cb25-67" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'CDF'</span>)</span>
<span id="cb25-68"><a href="#cb25-68" aria-hidden="true" tabindex="-1"></a>ax2.grid()</span>
<span id="cb25-69"><a href="#cb25-69" aria-hidden="true" tabindex="-1"></a>ax2.legend(loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb25-70"><a href="#cb25-70" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()  </span>
<span id="cb25-71"><a href="#cb25-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_ecdf" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_ecdf-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_ecdf-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Empirical cumulative distribution functions for the unconstrained fitted posterior and the Jeffreys posterior using <span class="math inline">5000</span> samples. The band is obtained by computing the ECDFs over <span class="math inline">100</span> different seeds and monitoring the maximum and minimum ECDF values for each <span class="math inline">\theta</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="cell-fig-probit_ecdf_constr" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>n_exp <span class="op">=</span> <span class="dv">100</span>   <span class="co"># Number of experiments</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> os.path.join(varp_probit_path, <span class="st">'probit_samples/probit_constr'</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>first_file <span class="op">=</span> model <span class="op">+</span> <span class="st">'_seed1.pkl'</span> </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>n_samples_prior <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>n_samples_post <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate results from all experiments</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>theta_prior <span class="op">=</span> np.zeros((n_exp,n_samples_prior,q))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>jeffreys_prior <span class="op">=</span> np.zeros((n_exp,n_samples_prior,q))</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>theta_post <span class="op">=</span> np.zeros((n_exp,n_samples_post,q))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>jeffreys_post <span class="op">=</span> np.zeros((n_exp,n_samples_post,q))</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_exp):</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    file_name <span class="op">=</span> model <span class="op">+</span> <span class="ss">f'_seed</span><span class="sc">{</span>k<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">.pkl'</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">#file_name = model + f'_seed_lognormal{k+1}.pkl'</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(file_name, <span class="st">'rb'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> pickle.load(<span class="bu">file</span>)</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>        theta_prior[k,:,:] <span class="op">=</span> data[<span class="st">'prior'</span>][<span class="st">'VA'</span>] </span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">#jeffreys_prior[k,:,:] = data['prior']['Jeffreys']</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>        theta_post[k,:,:] <span class="op">=</span> data[<span class="st">'post'</span>][<span class="st">'VA'</span>]</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>        jeffreys_post[k,:,:] <span class="op">=</span> data[<span class="st">'post'</span>][<span class="st">'Jeffreys'</span>]</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Post CDF computations </span></span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>num_x <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>x_values_alpha <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">8.</span>, num_x)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>x_values_beta <span class="op">=</span> np.linspace(<span class="fl">0.</span>, <span class="fl">2.</span>, num_x)</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>cdf_post_alpha <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>cdf_post_beta <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>jeff_post_alpha <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>jeff_post_beta <span class="op">=</span> np.zeros((n_exp, num_x))</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_exp):</span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a>    cdf_post_alpha[k,:] <span class="op">=</span> compute_ecdf(theta_post[k,:,<span class="dv">0</span>], x_values_alpha)</span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>    cdf_post_beta[k,:] <span class="op">=</span> compute_ecdf(theta_post[k,:,<span class="dv">1</span>], x_values_beta)</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>    jeff_post_alpha[k,:] <span class="op">=</span> compute_ecdf(jeffreys_post[k,:,<span class="dv">0</span>], x_values_alpha)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a>    jeff_post_beta[k,:] <span class="op">=</span> compute_ecdf(jeffreys_post[k,:,<span class="dv">1</span>], x_values_beta)</span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>lower_bound_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">0.</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>upper_bound_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">1.0</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>median_cdf_alpha <span class="op">=</span> np.quantile(cdf_post_alpha, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>median_jeff_alpha <span class="op">=</span> np.quantile(jeff_post_alpha, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>lower_bound_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">0.</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>upper_bound_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">1.0</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>median_cdf_beta <span class="op">=</span> np.quantile(cdf_post_beta, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>median_jeff_beta <span class="op">=</span> np.quantile(jeff_post_beta, <span class="fl">0.5</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Create subplots for alpha and beta</span></span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a>fig, (ax1, ax2) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">3</span>))</span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for alpha</span></span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values_alpha, median_cdf_alpha, label<span class="op">=</span><span class="st">'Median fitted ECDF'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>ax1.plot(x_values_alpha, median_jeff_alpha, label<span class="op">=</span><span class="st">'Jeffreys ECDF'</span>, color<span class="op">=</span><span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">2</span>) </span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'ECDF Band'</span>)</span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="vs">r'$\theta_1$'</span>)</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">'CDF'</span>)</span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a>ax1.grid()</span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>ax1.legend(loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot for beta</span></span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values_beta, median_cdf_beta, label<span class="op">=</span><span class="st">'Median fitted ECDF'</span>, color<span class="op">=</span><span class="st">'blue'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>ax2.plot(x_values_beta, median_jeff_beta, label<span class="op">=</span><span class="st">'Jeffreys ECDF'</span>, color<span class="op">=</span><span class="st">'green'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)  </span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color<span class="op">=</span><span class="st">'lightblue'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="st">'ECDF Band'</span>)</span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="vs">r'$\theta_2$'</span>)</span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">'CDF'</span>)</span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>ax2.grid()</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>ax2.legend(loc<span class="op">=</span><span class="st">'lower right'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()  </span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-probit_ecdf_constr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-probit_ecdf_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="main_nb_files/figure-html/fig-probit_ecdf_constr-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-probit_ecdf_constr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Empirical cumulative distribution functions for the constrained fitted posterior and the Jeffreys posterior using <span class="math inline">5000</span> samples. The band is obtained by computing the ECDFs over <span class="math inline">100</span> different seeds and monitoring the maximum and minimum ECDF values for each <span class="math inline">\theta</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p><a href="#fig-probit_ecdf" class="quarto-xref">Figure&nbsp;16</a> and <a href="#fig-probit_ecdf_constr" class="quarto-xref">Figure&nbsp;17</a> compare the empirical distribution functions of the fitted posterior and the Jeffreys posterior. In the unconstrained case, one can observe that the ECDFs are very close for <span class="math inline">\theta_1</span>, whereas the variability is slightly higher for <span class="math inline">\theta_2</span> although still reasonable. When imposing a constraint on <span class="math inline">\theta_2</span>, one remarks that the variability of the result is higher. The Jeffreys ECDF is contained in the band when <span class="math inline">\theta_2</span> is close to zero, but not when <span class="math inline">\theta_2</span> increases (<span class="math inline">\theta_2 &gt; 0.5</span>). This is coherent with the previous scatter histograms where the Jeffreys posterior on <span class="math inline">\theta_2</span> tends to have a heavier tail than the variational approximation.</p>
<p>Altogether, despite the stochastic nature of the developed algorithm, we consider that the result tends to be reasonably robust to the RNG seed for the optimization part, and robust to the dataset used for the posterior distribution for the MCMC part.</p>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-basir2023adaptive" class="csl-entry" role="listitem">
Basir, Shamsulhaq, and Inanc Senocak. 2023. <span>“An Adaptive Augmented Lagrangian Method for Training Physics and Equality Constrained Artificial Neural Networks.”</span> <a href="https://arxiv.org/abs/2306.04904">https://arxiv.org/abs/2306.04904</a>.
</div>
<div id="ref-berger2009formal" class="csl-entry" role="listitem">
Berger, James O., José M. Bernardo, and Dongchu Sun. 2009. <span>“<span class="nocase">The formal definition of reference priors</span>.”</span> <em>The Annals of Statistics</em> 37 (2): 905–38. <a href="https://doi.org/10.1214/07-AOS587">https://doi.org/10.1214/07-AOS587</a>.
</div>
<div id="ref-Bernardo1979a" class="csl-entry" role="listitem">
Bernardo, José M. 1979a. <span>“<span class="nocase">Expected Information as Expected Utility</span>.”</span> <em>The Annals of Statistics</em> 7 (3): 686–90. <a href="https://doi.org/10.1214/aos/1176344689">https://doi.org/10.1214/aos/1176344689</a>.
</div>
<div id="ref-bernardo1979" class="csl-entry" role="listitem">
———. 1979b. <span>“Reference Posterior Distributions for <span>B</span>ayesian Inference.”</span> <em>Journal of the Royal Statistical Society. Series B</em> 41 (2): 113–47. <a href="https://doi.org/10.1111/j.2517-6161.1979.tb01066.x">https://doi.org/10.1111/j.2517-6161.1979.tb01066.x</a>.
</div>
<div id="ref-bernardo2005reference" class="csl-entry" role="listitem">
———. 2005. <span>“Reference Analysis.”</span> In <em>Bayesian Thinking</em>, edited by D. K. Dey and C. R. Rao, 25:17–90. Handbook of Statistics. Elsevier. <a href="https://doi.org/10.1016/S0169-7161(05)25002-2">https://doi.org/10.1016/S0169-7161(05)25002-2</a>.
</div>
<div id="ref-Bioche2016" class="csl-entry" role="listitem">
Bioche, Christele, and Pierre Druilhet. 2016. <span>“Approximation of Improper Priors.”</span> <em>Bernoulli</em> 22 (3): 1709–28. <a href="https://doi.org/10.3150/15-bej708">https://doi.org/10.3150/15-bej708</a>.
</div>
<div id="ref-Bousquet2008" class="csl-entry" role="listitem">
Bousquet, Nicolas. 2008. <span>“Eliciting Vague but Proper Maximal Entropy Priors in Bayesian Experiments.”</span> <em>Statistical Papers</em> 51 (3): 613–28. <a href="https://doi.org/10.1007/s00362-008-0149-9">https://doi.org/10.1007/s00362-008-0149-9</a>.
</div>
<div id="ref-clarke1994jeffreys" class="csl-entry" role="listitem">
Clarke, Bertrand S., and Andrew R. Barron. 1994. <span>“Jeffreys’ Prior Is Asymptotically Least Favorable Under Entropy Risk.”</span> <em>Journal of Statistical Planning and Inference</em> 41 (1): 37–60. <a href="https://doi.org/10.1016/0378-3758(94)90153-8">https://doi.org/10.1016/0378-3758(94)90153-8</a>.
</div>
<div id="ref-Dandrea2021" class="csl-entry" role="listitem">
D’Andrea, Vera L. D. AND Aljohani, Amanda M. E. AND Tomazella. 2021. <span>“Objective Bayesian Analysis for Multiple Repairable Systems.”</span> <em>PLOS ONE</em> 16 (November): 1–19. <a href="https://doi.org/10.1371/journal.pone.0258581">https://doi.org/10.1371/journal.pone.0258581</a>.
</div>
<div id="ref-gao2022deep" class="csl-entry" role="listitem">
Gao, Yansong, Rahul Ramesh, and Pratik Chaudhari. 2022. <span>“Deep Reference Priors: What Is the Best Way to Pretrain a Model?”</span> In <em>Proceedings of the 39th International Conference on Machine Learning</em>, 162:7036–51. Proceedings of Machine Learning Research. PMLR. <a href="https://Proceedings.mlr.press/v162/gao22d.html">https://Proceedings.mlr.press/v162/gao22d.html</a>.
</div>
<div id="ref-gauchy_thesis" class="csl-entry" role="listitem">
Gauchy, Clément. 2022. <span>“<span class="nocase">Uncertainty quantification methodology for seismic fragility curves of mechanical structures : Application to a piping system of a nuclear power plant</span>.”</span> Theses, Institut Polytechnique de Paris. <a href="https://theses.hal.science/tel-04102809">https://theses.hal.science/tel-04102809</a>.
</div>
<div id="ref-gauchy_var_rp" class="csl-entry" role="listitem">
Gauchy, Clément, Antoine Van Biesbroeck, Cyril Feau, and Josselin Garnier. 2023. <span>“Inférence Variationnelle de Lois a Priori de Référence.”</span> In <em>Proceedings Des 54èmes Journées de Statistiques (JdS)</em>. SFDS. <a href="https://jds2023.sciencesconf.org/resource/page/id/19">https://jds2023.sciencesconf.org/resource/page/id/19</a>.
</div>
<div id="ref-gelman2013" class="csl-entry" role="listitem">
Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. <span>“Bayesian Data Analysis, Third Edition.”</span> In, 293–300. Chapman; Hall/CRC. <a href="https://doi.org/10.1201/b16018">https://doi.org/10.1201/b16018</a>.
</div>
<div id="ref-gretton_mmd" class="csl-entry" role="listitem">
Gretton, Arthur, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Schölkopf, and Alexander Smola. 2012. <span>“A Kernel Two-Sample Test.”</span> <em>Journal of Machine Learning Research</em> 13 (25): 723–73. <a href="http://jmlr.org/papers/v13/gretton12a.html">http://jmlr.org/papers/v13/gretton12a.html</a>.
</div>
<div id="ref-Gu2016" class="csl-entry" role="listitem">
Gu, Mengyang, and James O. Berger. 2016. <span>“<span class="nocase">Parallel partial Gaussian process emulation for computer models with massive output</span>.”</span> <em>The Annals of Applied Statistics</em> 10 (3): 1317–47. <a href="https://doi.org/10.1214/16-AOAS934">https://doi.org/10.1214/16-AOAS934</a>.
</div>
<div id="ref-jang2017categorical" class="csl-entry" role="listitem">
Jang, Eric, Shixiang Gu, and Ben Poole. 2017. <span>“Categorical Reparameterization with Gumbel-Softmax.”</span> In <em>Proceedings of the 5th International Conference on Learning Representations (ICLR)</em>. Toulon, France. <a href="https://doi.org/10.48550/arXiv.1611.01144">https://doi.org/10.48550/arXiv.1611.01144</a>.
</div>
<div id="ref-Jaynes1957" class="csl-entry" role="listitem">
Jaynes, E. T. 1957. <span>“Information Theory and Statistical Mechanics.”</span> <em>Phys. Rev.</em> 106 (May): 620–30. <a href="https://doi.org/10.1103/PhysRev.106.620">https://doi.org/10.1103/PhysRev.106.620</a>.
</div>
<div id="ref-Kass1996" class="csl-entry" role="listitem">
Kass, Robert E., and Larry Wasserman. 1996. <span>“The Selection of Prior Distributions by Formal Rules.”</span> <em>Journal of the American Statistical Association</em> 91 (435): 1343–70. <a href="https://doi.org/10.1080/01621459.1996.10477003">https://doi.org/10.1080/01621459.1996.10477003</a>.
</div>
<div id="ref-kennedy1980" class="csl-entry" role="listitem">
Kennedy, Robert P., C. Allin Cornell, Robert D. Campbell, Stan J. Kaplan, and F. Harold. 1980. <span>“Probabilistic Seismic Safety Study of an Existing Nuclear Power Plant.”</span> <em>Nuclear Engineering and Design</em> 59 (2): 315–38. <a href="https://doi.org/10.1016/0029-5493(80)90203-4">https://doi.org/10.1016/0029-5493(80)90203-4</a>.
</div>
<div id="ref-kingma2017adam" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2015. <span>“Adam: A Method for Stochastic Optimization.”</span> In <em>Proceedings of the 3rd International Conference on Learning Representations (ICLR)</em>. San Diego, USA. <a href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</a>.
</div>
<div id="ref-Kingma2014" class="csl-entry" role="listitem">
Kingma, Diederik P., and Max Welling. 2014. <span>“<span>Auto-Encoding Variational Bayes</span>.”</span> In <em>Proceedings of the 2nd International Conference on Learning Representations (ICLR)</em>. Banff, Canada. <a href="https://doi.org/10.48550/arXiv.1312.6114">https://doi.org/10.48550/arXiv.1312.6114</a>.
</div>
<div id="ref-kingma2019introduction" class="csl-entry" role="listitem">
———. 2019. <span>“An Introduction to Variational Autoencoders.”</span> <em>Foundations and Trends® in Machine Learning</em> 12 (4): 307--392. <a href="https://doi.org/10.1561/2200000056">https://doi.org/10.1561/2200000056</a>.
</div>
<div id="ref-kobyzev_flows" class="csl-entry" role="listitem">
Kobyzev, Ivan, Simon J. D. Prince, and Marcus A. Brubaker. 2021. <span>“Normalizing Flows: An Introduction and Review of Current Methods.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 43 (11): 3964–79. <a href="https://doi.org/10.1109/TPAMI.2020.2992934">https://doi.org/10.1109/TPAMI.2020.2992934</a>.
</div>
<div id="ref-lafferty2013iterative" class="csl-entry" role="listitem">
Lafferty, John D., and Larry A. Wasserman. 2001. <span>“Iterative Markov Chain Monte Carlo Computation of Reference Priors and Minimax Risk.”</span> In <em>Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI)</em>, edited by Jack S. Breese and Daphne Koller, 293–300. Seattle, USA: Morgan Kaufmann. <a href="https://doi.org/10.48550/arXiv.1301.2286">https://doi.org/10.48550/arXiv.1301.2286</a>.
</div>
<div id="ref-Li2021" class="csl-entry" role="listitem">
Li, Hanmo, and Mengyang Gu. 2021. <span>“Robust Estimation of <span>SARS-CoV-2</span> Epidemic in <span>US</span> Counties.”</span> <em>Scientific Reports</em> 11 (11841): 2045–2322. <a href="https://doi.org/10.1038/s41598-021-90195-6">https://doi.org/10.1038/s41598-021-90195-6</a>.
</div>
<div id="ref-MacKay2003" class="csl-entry" role="listitem">
MacKay, D. J. C. 2003. <em>Information Theory, Inference, and Learning Algorithms</em>. Copyright Cambridge University Press.
</div>
<div id="ref-Marzouk2016" class="csl-entry" role="listitem">
Marzouk, Y., T. Moselhy, M. Parno, and A. Spantini. 2016. <span>“Sampling via Measure Transport: An Introduction.”</span> In <em>Handbook of Uncertainty Quantification</em>, 1–41. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-11259-6\_23-1">https://doi.org/10.1007/978-3-319-11259-6\_23-1</a>.
</div>
<div id="ref-mure2018objective" class="csl-entry" role="listitem">
Muré, Joseph. 2018. <span>“Objective Bayesian Analysis of Kriging Models with Anisotropic Correlation Kernel.”</span> PhD thesis, Universit<span>é</span> Sorbonne Paris Cit<span>é</span>. <a href="https://theses.hal.science/tel-02184403/file/MURE_Joseph_2_complete_20181005.pdf">https://theses.hal.science/tel-02184403/file/MURE_Joseph_2_complete_20181005.pdf</a>.
</div>
<div id="ref-nalisnick2017learning" class="csl-entry" role="listitem">
Nalisnick, Eric, and Padhraic Smyth. 2017. <span>“Learning Approximately Objective Priors.”</span> In <em>Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI)</em>. Sydney, Australia: Association for Uncertainty in Artificial Intelligence (AUAI). <a href="https://doi.org/10.48550/arXiv.1704.01168">https://doi.org/10.48550/arXiv.1704.01168</a>.
</div>
<div id="ref-Natarajan2000" class="csl-entry" role="listitem">
Natarajan, Ranjini, and Robert E. Kass. 2000. <span>“Reference Bayesian Methods for Generalized Linear Mixed Models.”</span> <em>Journal of the American Statistical Association</em> 95 (449): 227–37. <a href="https://doi.org/10.1080/01621459.2000.10473916">https://doi.org/10.1080/01621459.2000.10473916</a>.
</div>
<div id="ref-nocedal2006penalty" class="csl-entry" role="listitem">
Nocedal, Jorge, and Stephen J. Wright. 2006. <span>“Numerical Optimization.”</span> In <em>Springer Series in Operations Research and Financial Engineering</em>, 497–528. Springer New York. <a href="https://doi.org/10.1007/978-0-387-40065-5\_17">https://doi.org/10.1007/978-0-387-40065-5\_17</a>.
</div>
<div id="ref-papamakarios2021nf" class="csl-entry" role="listitem">
Papamakarios, George, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. 2021. <span>“Normalizing Flows for Probabilistic Modeling and Inference.”</span> <em>Journal of Machine Learning Research</em> 22 (57): 1–64. <a href="http://jmlr.org/papers/v22/19-1028.html">http://jmlr.org/papers/v22/19-1028.html</a>.
</div>
<div id="ref-torch2019" class="csl-entry" role="listitem">
Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. <span>“PyTorch: An Imperative Style, High-Performance Deep Learning Library.”</span> In <em>Advances in Neural Information Processing Systems</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc. <a href="https://Proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf">https://Proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</a>.
</div>
<div id="ref-Paulo2005" class="csl-entry" role="listitem">
Paulo, Rui. 2005. <span>“<span class="nocase">Default priors for Gaussian processes</span>.”</span> <em>The Annals of Statistics</em> 33 (2): 556–82. <a href="https://doi.org/10.1214/009053604000001264">https://doi.org/10.1214/009053604000001264</a>.
</div>
<div id="ref-Press2009" class="csl-entry" role="listitem">
Press, S James. 2009. <em>Subjective and Objective Bayesian Statistics: Principles, Models, and Applications</em>. John Wiley &amp; Sons.
</div>
<div id="ref-Reid2003" class="csl-entry" role="listitem">
Reid, N, R Mukerjee, and DAS Fraser. 2003. <span>“Some Aspects of Matching Priors.”</span> <em>Lecture Notes-Monograph Series</em>, 31–43.
</div>
<div id="ref-Simpson2017" class="csl-entry" role="listitem">
Simpson, Daniel, Håvard Rue, Andrea Riebler, Thiago G. Martins, and Sigrunn H. Sørbye. 2017. <span>“<span class="nocase">Penalising Model Component Complexity: A Principled, Practical Approach to Constructing Priors</span>.”</span> <em>Statistical Science</em> 32 (1): 1–28. <a href="https://doi.org/10.1214/16-STS576">https://doi.org/10.1214/16-STS576</a>.
</div>
<div id="ref-Soofi2000" class="csl-entry" role="listitem">
Soofi, Ehsan S. 2000. <span>“Principal Information Theoretic Approaches.”</span> <em>Journal of the American Statistical Association</em> 95 (452): 1349–53. <a href="https://doi.org/10.1080/01621459.2000.10474346">https://doi.org/10.1080/01621459.2000.10474346</a>.
</div>
<div id="ref-van2023generalized" class="csl-entry" role="listitem">
Van Biesbroeck, Antoine. 2024a. <span>“Generalized Mutual Information and Their Reference Priors Under Csizar f-Divergence.”</span> <a href="https://arxiv.org/abs/2310.10530">https://arxiv.org/abs/2310.10530</a>.
</div>
<div id="ref-van2024constr" class="csl-entry" role="listitem">
———. 2024b. <span>“Properly Constrained Reference Priors Decay Rates for Efficient and Robust Posterior Inference.”</span> <a href="https://arxiv.org/abs/2409.13041">https://arxiv.org/abs/2409.13041</a>.
</div>
<div id="ref-van2024reference" class="csl-entry" role="listitem">
Van Biesbroeck, Antoine, Clément Gauchy, Cyril Feau, and Josselin Garnier. 2024. <span>“Reference Prior for Bayesian Estimation of Seismic Fragility Curves.”</span> <em>Probabilistic Engineering Mechanics</em> 76 (April): 103622. <a href="https://doi.org/10.1016/j.probengmech.2024.103622">https://doi.org/10.1016/j.probengmech.2024.103622</a>.
</div>
<div id="ref-Zellner1996" class="csl-entry" role="listitem">
Zellner, Arnold. 1996. <span>“Models, Prior Information, and Bayesian Analysis.”</span> <em>Journal of Econometrics</em> 75 (1): 51–68. <a href="https://doi.org/10.1016/0304-4076(95)01768-2">https://doi.org/10.1016/0304-4076(95)01768-2</a>.
</div>
</div>
<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{baillie2025,
  author = {Baillie, Nils and Van Biesbroeck, Antoine and Gauchy,
    Clément},
  publisher = {Société Française de Statistique},
  title = {Variational Inference for Approximate Reference Priors Using
    Neural Networks},
  journal = {Computo},
  date = {2025-02-11},
  url = {https://computo.sfds.asso.fr/template-computo-quarto},
  doi = {xxxx},
  issn = {2824-7795},
  langid = {en},
  abstract = {In Bayesian statistics, the choice of the prior can have
    an important influence on the posterior and the parameter
    estimation, especially when few data samples are available. To limit
    the added subjectivity from a priori information, one can use the
    framework of reference priors. However, computing such priors is a
    difficult task in general. We develop in this paper a flexible
    algorithm based on variational inference which computes
    approximations of reference priors from a set of parametric
    distributions using neural networks. We also show that our algorithm
    can retrieve reference priors when constraints are specified in the
    optimization problem to ensure the solution is proper. We propose a
    simple method to recover a relevant approximation of the parametric
    posterior distribution using Markov Chain Monte Carlo (MCMC) methods
    even if the density function of the parametric prior is not known in
    general. Numerical experiments on several statistical models of
    increasing complexity are presented. We show the usefulness of this
    approach by recovering the target distribution. The performance of
    the algorithm is evaluated on the prior distributions as well as the
    posterior distributions, jointly using variational inference and
    MCMC sampling.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-baillie2025" class="csl-entry quarto-appendix-citeas" role="listitem">
Baillie, Nils, Antoine Van Biesbroeck, and Clément Gauchy. 2025.
<span>“Variational Inference for Approximate Reference Priors Using
Neural Networks.”</span> <em>Computo</em>, February. <a href="https://doi.org/xxxx">https://doi.org/xxxx</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb27" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Variational inference for approximate reference priors using neural networks"</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="an">subtitle:</span><span class="co"> ""</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Nils Baillie</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co">    corresponding: true</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co">    email: nils.baillie@cea.fr</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CEA, Service d'Études Mécaniques et Thermiques, 91191 Gif-sur-Yvette, France</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Antoine Van Biesbroeck</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: CMAP, CNRS, École polytechnique, Institut Polytechnique de Paris, 91120 Palaiseau, France</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co">        url: https://cmap.ip-paris.fr/ </span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Clément Gauchy</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Université Paris-Saclay, CEA, Service de Génie Logiciel pour la Simulation, 91191 Gif-sur-Yvette, France</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> last-modified</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> &gt;+</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="co">  In Bayesian statistics, the choice of the prior can have an important influence on the posterior and the parameter estimation, especially when few data samples are available. To limit the added subjectivity from a priori information, one can use the framework of reference priors. However, computing such priors is a difficult task in general. We develop in this paper a flexible algorithm based on variational inference which computes approximations of reference priors from a set of parametric distributions using neural networks. We also show that our algorithm can retrieve reference priors when constraints are specified in the optimization problem to ensure the solution is proper. We propose a simple method to recover a relevant approximation of the parametric posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if the density function of the parametric prior is not known in general. Numerical experiments on several statistical models of increasing complexity are presented. We show the usefulness of this approach by recovering the target distribution. The performance of the algorithm is evaluated on the prior distributions as well as the posterior distributions, jointly using variational inference and MCMC sampling.</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span><span class="co"> [Reference priors, Variational inference, Neural networks]</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "xxxx"</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a><span class="co">  url: https://computo.sfds.asso.fr/template-computo-quarto</span></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "Société Française de Statistique"</span></span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computorg</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> "Computo_VA-RP"</span></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> true # set to false once the build is running</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> false # will be set to true once accepted</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>::: {.hidden}</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>\def\bX{\mathbf{X}}</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>\newcommand{\rD}{\mathrm{D}}</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>\newcommand{\aseq}<span class="co">[</span><span class="ot">2</span><span class="co">][]</span>{\overset{#1}{\underset{#2}{=}}}</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>\newcommand{\conv}<span class="co">[</span><span class="ot">2</span><span class="co">][\ ]</span>{\overset{#1}{\underset{#2}{\to}}}</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a><span class="in">import os</span></span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a><span class="in">import sys</span></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a><span class="in">base_dir = os.getcwd()  # the path must be set on VA-RP</span></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a><span class="in">py_dir = os.path.join(base_dir, "python_files")</span></span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a><span class="in">if py_dir not in sys.path:</span></span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a><span class="in">    sys.path.append(py_dir)</span></span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a><span class="in">#print(sys.path)</span></span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a><span class="in">from aux_optimizers import *</span></span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a><span class="in">from stat_models_torch import *</span></span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a><span class="in">from neural_nets import *</span></span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a><span class="in">from variational_approx import *</span></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a><span class="in">from div_metrics_torch import *</span></span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a><span class="in">from constraints import *</span></span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a><span class="in"># Packages used</span></span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a><span class="in">from scipy.stats import gaussian_kde</span></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a><span class="in">from matplotlib import rc</span></span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a><span class="in">from matplotlib import gridspec</span></span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a><span class="in">import pickle</span></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a><span class="in">rc('text', usetex=False) </span></span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a><span class="in">rc('lines', linewidth=2)</span></span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a><span class="in">rougeCEA = "#b81420"</span></span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a><span class="in">vertCEA = "#73be4b"</span></span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a><span class="in">path_plot = os.path.join(base_dir, "plots_data")</span></span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a><span class="in">probit_path = os.path.join(base_dir, "data_probit")</span></span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a><span class="in">tirages_path = os.path.join(probit_path, "tirages_probit")</span></span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a><span class="in">int_jeffreys_path = os.path.join(probit_path, "Multiseed_AJ")</span></span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a><span class="in">varp_probit_path = os.path.join(probit_path, "Multiseed_VARP")</span></span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a><span class="in">### Load parameter for each main computation</span></span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a><span class="in"># If True, loads the different results without re-doing every computation</span></span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a><span class="in"># If False, all computations will be done and saved (and possibly overwrite the data files !)</span></span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_Multinom = True</span></span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_MultiMMD = True</span></span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_Probit_nocstr = True</span></span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_Probit_cstr = True</span></span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_Normal_nocstr = True</span></span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a><span class="in">load_results_Normal_cstr = True</span></span>
<span id="cb27-92"><a href="#cb27-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-93"><a href="#cb27-93" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-94"><a href="#cb27-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-95"><a href="#cb27-95" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction</span></span>
<span id="cb27-96"><a href="#cb27-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-97"><a href="#cb27-97" aria-hidden="true" tabindex="-1"></a>The Bayesian approach to statistical inference aims to produce estimations using the posterior distribution. The latter is derived by updating the prior distribution with the observed statistics thanks to Bayes' theorem. However, the shape of the posterior can be heavily influenced by the prior choice when the amount of available data is limited or the prior distribution is highly informative. </span>
<span id="cb27-98"><a href="#cb27-98" aria-hidden="true" tabindex="-1"></a>For this reason, selecting a prior remains a daunting task that must be handled carefully. Hence, systematic methods has been introduced by statisticians to help the choice of the prior distribution, both in cases where subjective knowledge is available or not @Press2009. @Kass1996 propose different ways of defining the level of non-informativeness of a prior distribution. The most famous method is the Maximum Entropy (ME) prior distribution that has been popularized by @Jaynes1957. In a Bayesian context, ME and Maximal Data Information (MDI) priors have been studied by @Zellner1996, @Soofi2000. Other candidates for objective priors are the so-called matching priors @Reid2003, which are priors such that the Bayesian posterior credible intervals correspond to confidence intervals of the sampling model. Moreover, when a simpler model is known, the Penalizing Complexity (PC) priors are yet another rationale of choosing an objective (or reference) prior distribution @Simpson2017.</span>
<span id="cb27-99"><a href="#cb27-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-100"><a href="#cb27-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-101"><a href="#cb27-101" aria-hidden="true" tabindex="-1"></a>In this paper, we will focus on the reference prior theory. First introduced in @Bernardo1979a and further formalized in @berger2009formal, the main rationale behind the reference prior theory is the maximization of the information brought by the data during Bayesian inference. Specifically, reference priors (RPs) are constructed to maximize the mutual information metric, which is defined as a divergence between itself and the posterior. In this way, it ensures that the data plays a dominant role in the Bayesian framework. This approach has been extensively studied (see e.g. @bernardo1979, @clarke1994jeffreys, @van2023generalized) and applied to various statistical models, such as Gaussian process-based models @Paulo2005, @Gu2016, generalized linear models @Natarajan2000, and even  Bayesian Neural Networks @gao2022deep. The RPs are recognized for their objective nature in practical studies @Dandrea2021, @Li2021, @van2024reference, yet they suffer from their low computational feasibility. Indeed, the expression of the RPs often leads to an intricate theoretical expression, which necessitates a heavy numerical cost to be derived that becomes even more cumbersome as the dimensionality of the problem increases. Moreover, in many applications, a posteriori estimates are obtained using Markov Chain Monte Carlo (MCMC) methods, which require a large number of prior evaluations, further compounding the computational burden.</span>
<span id="cb27-102"><a href="#cb27-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-103"><a href="#cb27-103" aria-hidden="true" tabindex="-1"></a>In general, when we look for sampling or approximating a probability distribution,</span>
<span id="cb27-104"><a href="#cb27-104" aria-hidden="true" tabindex="-1"></a>several approaches arise and may be used within a Bayesian framework. </span>
<span id="cb27-105"><a href="#cb27-105" aria-hidden="true" tabindex="-1"></a>In this work, we focus on variational inference methods.</span>
<span id="cb27-106"><a href="#cb27-106" aria-hidden="true" tabindex="-1"></a>Variational inference seeks to approximate a complex target distribution </span>
<span id="cb27-107"><a href="#cb27-107" aria-hidden="true" tabindex="-1"></a> $p$, ---e.g. a posterior--- by optimizing over a family of simpler parameterized distributions $q_{\lambda}$. The goal then is to find the distribution $q_{\lambda^\ast}$ that is the best approximation of $p$ by minimizing a divergence, such as the Kullback-Leibler (KL) divergence. </span>
<span id="cb27-108"><a href="#cb27-108" aria-hidden="true" tabindex="-1"></a> Variational inference methods have been widely adopted in various contexts, including popular models such as Variational Autoencoders (VAEs) @kingma2019introduction, which are a class of generative models where one wants to learn the underlying distribution of data samples. We can also mention normalizing flows @papamakarios2021nf, @kobyzev_flows,</span>
<span id="cb27-109"><a href="#cb27-109" aria-hidden="true" tabindex="-1"></a> which consider diffeomorphism transformations to recover the density of the approximated distribution from the simpler one taken as input.</span>
<span id="cb27-110"><a href="#cb27-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-111"><a href="#cb27-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-112"><a href="#cb27-112" aria-hidden="true" tabindex="-1"></a>When it resorts to approximate RPs, it is possible to leverage the optimal characteristic of the reference prior (that is, it maximizes the mutual information metric) instead of directly maximizing a divergence between a target and an output. Indeed, the mutual information metric does not depend on the target distribution that we want to reach so  iterative derivations of the theoretical RP are not necessary. In @nalisnick2017learning, the authors propose a variational inference procedure to approximate the RP using a lower bound of the mutual information as an optimization criterion. In @gauchy_var_rp, a variational inference procedure is  proposed using stochastic gradient descent of the mutual information criterion and illustrated on simple statistical models. </span>
<span id="cb27-113"><a href="#cb27-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-114"><a href="#cb27-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-115"><a href="#cb27-115" aria-hidden="true" tabindex="-1"></a>By building on these foundations, this paper proposes a novel variational inference algorithm to compute RPs. As in @nalisnick2017learning and @gauchy_var_rp, the RP is approximated in a parametric family of probability distributions implicitly defined by the push-forward probability distribution through a nonlinear function (see e.g. @papamakarios2021nf and @Marzouk2016). We will focus in this paper to push-forward probability measures through neural networks. In comparison with the previous works, we benchmark extensively our algorithm on statistical models of different complexity and nature to ensure its robustness. We also extend our algorithm to handle a more general definition of RPs @van2023generalized, where a generalized mutual information criterion is defined using $f$-divergences. In this paper, we restrict the different benchmarks to $\alpha$-divergences. Additionally, we extend the framework to allow the integration of linear constraints on the prior in the pipeline. That last feature permits handling situations where the RP may be improper (i.e. it integrates to infinity). Improper priors pose a challenge because (i) one can not sample from the  a priori distribution, and (ii) they do not ensure that the posterior is proper, jeopardizing  a posteriori inference. Recent work detailed in @van2024constr introduces linear constraints that ensure the proper aspects of RPs. Our algorithm incorporates these constraints, providing a principled way to address improper priors and ensuring that the resulting posterior distributions are well-defined and suitable for practical use.</span>
<span id="cb27-116"><a href="#cb27-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-117"><a href="#cb27-117" aria-hidden="true" tabindex="-1"></a>First, we will introduce the reference prior theory of @bernardo1979 and the recent developments around generalized reference priors made by @van2023generalized in @sec-rp_theory. Next, the variational approximation of the reference prior (VA-RP) methodology is detailed in @sec-VA-RP. A stochastic gradient algorithm is proposed, as well as an augmented Lagrangian algorithm for the constrained optimization problem, for learning the parameters of an implicitly defined probability density function that will approximate the reference prior. Moreover, a mindful trick to sample from the posterior distribution by MCMC using the implicitly defined prior distribution is proposed. In @sec-numexp, different numerical experiments from various test cases are carried out in order to benchmark the VA-RP. Analytical statistical models where the true asymptotic RP is known are tested to allow comparison between the VA-RP and the true asymptotic RP.</span>
<span id="cb27-118"><a href="#cb27-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-119"><a href="#cb27-119" aria-hidden="true" tabindex="-1"></a><span class="fu"># Reference priors theory  {#sec-rp_theory}</span></span>
<span id="cb27-120"><a href="#cb27-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-121"><a href="#cb27-121" aria-hidden="true" tabindex="-1"></a>The reference prior theory fits into the usual framework of statistical inference.</span>
<span id="cb27-122"><a href="#cb27-122" aria-hidden="true" tabindex="-1"></a>The situation is the following: we observe i.i.d data samples $\bX = (X_1,..., X_N) \in \mathcal{X}^N$ with $\mathcal{X} \subset \mathbb{R}^d$. We suppose that the likelihood function $L_N(\mathbf{X} \, | \, \theta) = \prod_{i=1}^N L(X_i \, | \, \theta)$ is known and $\theta \in \Theta \subset \mathbb{R}^q$ is the parameter we want to infer. Since we use the Bayesian framework, $\theta$ is considered to be a random variable with a prior distribution $\pi$. We also define the marginal likelihood $p_{\pi, N}(\bX) = \int_{\Theta}\pi(\theta)L_N(\bX \, | \, \theta)d\theta$ associated to the marginal probability measure $\mathbb{P}_{\pi, N}$. The non-asymptotic RP, first introduced in @Bernardo1979a and formalized in @berger2009formal, is defined to be one of the priors verifying:</span>
<span id="cb27-123"><a href="#cb27-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-124"><a href="#cb27-124" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-125"><a href="#cb27-125" aria-hidden="true" tabindex="-1"></a>    \pi^\ast \in \underset{\pi \in \mathcal{P}}{\operatorname{argmax}} \, I(\pi; L_N) \ ,</span>
<span id="cb27-126"><a href="#cb27-126" aria-hidden="true" tabindex="-1"></a>$$ {#eq-RPdef}</span>
<span id="cb27-127"><a href="#cb27-127" aria-hidden="true" tabindex="-1"></a>where $\mathcal{P}$ is a class of admissible probability distributions and  $I(\pi; L_N)$ is the mutual information for the prior $\pi$ and the likelihood $L_N$ between the random variable of the parameters $\theta \sim \pi$ and the random variable of the data $\bX \sim \mathbb{P}_{\pi, N}$: </span>
<span id="cb27-128"><a href="#cb27-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-129"><a href="#cb27-129" aria-hidden="true" tabindex="-1"></a>    I(\pi; L_N) = \int_{\mathcal{X}^N}{\rm KL}(\pi(\cdot \,|\, \bX) \, || \,  \pi) p_{\pi,N}(\bX)d\bX</span>
<span id="cb27-130"><a href="#cb27-130" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mutualinfoberger}</span>
<span id="cb27-131"><a href="#cb27-131" aria-hidden="true" tabindex="-1"></a>Hence, $\pi^\ast$ is a prior that maximizes the Kullback-Leibler divergence between itself and its posterior averaged by the marginal distribution of datasets. The Kullback-Leibler divergence between two probability measures of density $p$ and $q$ defined on a generic set $\Omega$ writes:</span>
<span id="cb27-132"><a href="#cb27-132" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-133"><a href="#cb27-133" aria-hidden="true" tabindex="-1"></a>    {\rm KL}(p\,||\,q) = \int_\Omega \log\left(\frac{p(\omega)}{q(\omega)}\right) p(\omega)d\omega.</span>
<span id="cb27-134"><a href="#cb27-134" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-135"><a href="#cb27-135" aria-hidden="true" tabindex="-1"></a>Thus, $\pi^\ast$ is the prior that maximises the influence of the data on the posterior distribution, justifying its reference (or objective) properties. The RP $\pi^\ast$ can also be interpreted using channel coding and information theory @MacKay2003 (chapter 9). Indeed, remark that $I(\pi; L_N)$ corresponds to the mutual information $I(\theta, \bX)$ between the random variable $\theta \sim \pi$ and the data $\bX \sim \mathbb{P}_{\pi, N}$, it measures the information that conveys the data $\bX$ about the parameters $\theta$. The maximal value of this mutual information is defined as the channel's capacity. The RP thus corresponds to the prior distribution that maximizes the information about $\theta$ conveyed by the data $\bX$. </span>
<span id="cb27-136"><a href="#cb27-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-137"><a href="#cb27-137" aria-hidden="true" tabindex="-1"></a>Using Fubini's theorem and Bayes' theorem, we can derive an alternative and more practical expression for the mutual information : </span>
<span id="cb27-138"><a href="#cb27-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-139"><a href="#cb27-139" aria-hidden="true" tabindex="-1"></a>    I(\pi; L_N) = \int_{\Theta}{\rm KL}(L_N(\cdot \, | \, \theta)||p_{\pi,N}) \pi(\theta)d\theta.</span>
<span id="cb27-140"><a href="#cb27-140" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mutualinfo}</span>
<span id="cb27-141"><a href="#cb27-141" aria-hidden="true" tabindex="-1"></a>A more generalized definition of RPs has been proposed in @van2023generalized using $f$-divergences. The $f$-divergence mutual information is defined by</span>
<span id="cb27-142"><a href="#cb27-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-143"><a href="#cb27-143" aria-hidden="true" tabindex="-1"></a>    I_{\rD_f}(\pi; L_N) = \int_{\Theta}{\rD_f}(p_{\pi,N}||L_N(\cdot \, | \, \theta)) \pi(\theta)d\theta \ ,</span>
<span id="cb27-144"><a href="#cb27-144" aria-hidden="true" tabindex="-1"></a>$$ {#eq-mutual-info-Df}</span>
<span id="cb27-145"><a href="#cb27-145" aria-hidden="true" tabindex="-1"></a>with </span>
<span id="cb27-146"><a href="#cb27-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-147"><a href="#cb27-147" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-148"><a href="#cb27-148" aria-hidden="true" tabindex="-1"></a>    \rD_f(p\,||\,q) = \int_{\Omega} f\left(\frac{p(\omega)}{q(\omega)}\right) q(\omega)d\omega. \ ,</span>
<span id="cb27-149"><a href="#cb27-149" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-150"><a href="#cb27-150" aria-hidden="true" tabindex="-1"></a>where $f$ is usually chosen to be a convex function mapping 1 to 0. Remark that the classical mutual information is obtained by choosing $f = -\log$, indeed, $\rD_{-\log}(p\,||\,q) = {\rm KL}(q\,||\,p)$. The formal RP is defined as $N$ goes to infinity, but since we want to develop an algorithm to approximate the distribution of the RP, we are restricted to the case where $N$ takes a finite value. However, the limit case $N \rightarrow +\infty$ is relevant because it has been shown in @clarke1994jeffreys, @van2023generalized</span>
<span id="cb27-151"><a href="#cb27-151" aria-hidden="true" tabindex="-1"></a>that the solution of this asymptotic problem is the Jeffreys prior when the mutual information is expressed as in @eq-mutualinfoberger, or when it is defined using an $\alpha$-divergence, as in @eq-mutual-info-Df with $f=f_\alpha$, where:</span>
<span id="cb27-152"><a href="#cb27-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-153"><a href="#cb27-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-154"><a href="#cb27-154" aria-hidden="true" tabindex="-1"></a>    f_\alpha(x) = \frac{x^{\alpha}-\alpha x -(1-\alpha)}{\alpha(\alpha-1)}, \quad \alpha\in(0,1).</span>
<span id="cb27-155"><a href="#cb27-155" aria-hidden="true" tabindex="-1"></a>$$ {#eq-falpha}</span>
<span id="cb27-156"><a href="#cb27-156" aria-hidden="true" tabindex="-1"></a>The Jeffreys prior, denoted by $J$, is defined as follows:</span>
<span id="cb27-157"><a href="#cb27-157" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-158"><a href="#cb27-158" aria-hidden="true" tabindex="-1"></a> J(\theta) \propto \det (\mathcal{I}(\theta))^{1/2} \quad \text{with} \quad \mathcal{I}(\theta) = - \int_{\mathcal{X}^N} \frac{\partial^2 \log L_N}{\partial \theta^2}(\bX\, | \, \theta)\cdot L_N(\bX\, | \, \theta) \, d\bX . </span>
<span id="cb27-159"><a href="#cb27-159" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-160"><a href="#cb27-160" aria-hidden="true" tabindex="-1"></a>We suppose that the likelihood function is smooth such that the Fisher information matrix $\mathcal I$ is well-defined. The Jeffreys prior and the RP have the relevant property to be ``invariant by reparametrization'': </span>
<span id="cb27-161"><a href="#cb27-161" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-162"><a href="#cb27-162" aria-hidden="true" tabindex="-1"></a>\forall \varphi \,\, \text{diffeomorphism} ,  \quad J(\theta) = \left| \frac{\partial \varphi}{\partial \theta} \right| \cdot J(\varphi(\theta)) .</span>
<span id="cb27-163"><a href="#cb27-163" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-164"><a href="#cb27-164" aria-hidden="true" tabindex="-1"></a>This property expresses non-information in the sense that if there is no information on $\theta$, there should not be more information on $\varphi(\theta)$ when $\varphi$ is a diffeomorphism: an invertible and differentiable transformation. </span>
<span id="cb27-165"><a href="#cb27-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-166"><a href="#cb27-166" aria-hidden="true" tabindex="-1"></a>Actually, the historical definition of RPs involves the KL-divergence in the definition of the mutual information. Yet the use of $\alpha$-divergences instead is relevant because they can be seen as a continuous path between the KL-divergence and the Reverse-KL-divergence when $\alpha$ varies from $0$ to $1$.  We can also mention that for $\alpha = 1/2$, the $\alpha$-divergence is the squared Hellinger distance whose square root is a metric since it is symmetric and verifies the triangle inequality.</span>
<span id="cb27-167"><a href="#cb27-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-168"><a href="#cb27-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-169"><a href="#cb27-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-170"><a href="#cb27-170" aria-hidden="true" tabindex="-1"></a>Technically, the formal RP is constructed such that its projection on every compact subset (or open subset in @mure2018objective) of $\Theta$ maximizes asymptotically the mutual information, which allows for improper distributions to be RPs in some cases. The Jeffreys prior is itself often improper. </span>
<span id="cb27-171"><a href="#cb27-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-172"><a href="#cb27-172" aria-hidden="true" tabindex="-1"></a>In our algorithm we consider probability distributions defined on the space $\Theta$ and not on sequences of subsets. A consequence of this statement is that our algorithm may tend to approximate improper priors in some cases. Indeed, any given sample by our algorithm results, by construction, from a proper distribution, which is expected to be a good approximation of the solution of the optimization problem expressed in @eq-RPdef. If $N$ is large enough, the latter should be close to the  ---potentially improper--- theoretical RP. This approach is justified to some extent since in the context of Q-vague convergence defined in @Bioche2016 for instance, improper priors can be the limit of sequences of proper priors. Although this theoretical notion of convergence is defined, no concrete metric is given, making quantification of the difference between proper and improper priors infeasible in practice. Furthermore, as mentioned in the introduction, improper priors can also compromise the validity of {a posteriori} estimates in some cases. To address this issue, we adapted our algorithm to handle the developments made in @van2024constr, which suggest a method to define proper RPs by simply resolving a constrained version of the initial optimization problem:</span>
<span id="cb27-173"><a href="#cb27-173" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-174"><a href="#cb27-174" aria-hidden="true" tabindex="-1"></a>    \tilde\pi^* \in \underset{\substack{\pi \, \text{prior}<span class="sc">\\</span> \text{s.t.}\,\mathcal{C}(\pi)&lt;\infty}}{\operatorname{argmax}} I_{\rD_{f_\alpha}}(\pi; L_N),</span>
<span id="cb27-175"><a href="#cb27-175" aria-hidden="true" tabindex="-1"></a>$$ {#eq-def_const_RP}</span>
<span id="cb27-176"><a href="#cb27-176" aria-hidden="true" tabindex="-1"></a>where $\mathcal{C}(\pi)$ defines a constraint of the form $\int_\Theta a(\theta)\pi(\theta)d\theta$, $a$ being a positive function. When the mutual information in the above optimization problem is defined from an $\alpha$-divergence, and when $a$ verifies</span>
<span id="cb27-177"><a href="#cb27-177" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-178"><a href="#cb27-178" aria-hidden="true" tabindex="-1"></a>    \int_\Theta J(\theta)a(\theta)^{1/\alpha}d\theta&lt;\infty\quad \text{and}\quad \int_\Theta J(\theta)a(\theta)^{1+1/\alpha}d\theta&lt;\infty,</span>
<span id="cb27-179"><a href="#cb27-179" aria-hidden="true" tabindex="-1"></a>$$ {#eq-condtitions_a}</span>
<span id="cb27-180"><a href="#cb27-180" aria-hidden="true" tabindex="-1"></a>the author has proven that the constrained RP $\tilde\pi^\ast$ asymptotically takes the following form:</span>
<span id="cb27-181"><a href="#cb27-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-182"><a href="#cb27-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-183"><a href="#cb27-183" aria-hidden="true" tabindex="-1"></a>    \tilde\pi^\ast(\theta) \propto J(\theta)a(\theta)^{1/\alpha},</span>
<span id="cb27-184"><a href="#cb27-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-185"><a href="#cb27-185" aria-hidden="true" tabindex="-1"></a>which is proper.</span>
<span id="cb27-186"><a href="#cb27-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-187"><a href="#cb27-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-188"><a href="#cb27-188" aria-hidden="true" tabindex="-1"></a><span class="fu"># Variational approximation of the reference prior (VA-RP)  {#sec-VA-RP}</span></span>
<span id="cb27-189"><a href="#cb27-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-190"><a href="#cb27-190" aria-hidden="true" tabindex="-1"></a><span class="fu">## Implicitly defined parametric probability distributions using neural networks</span></span>
<span id="cb27-191"><a href="#cb27-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-192"><a href="#cb27-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-193"><a href="#cb27-193" aria-hidden="true" tabindex="-1"></a>Variational inference refers to techniques that aim to approximate a probability distribution by solving an optimization problem ---that often takes a variational form, such as maximizing evidence lower bound (ELBO) @Kingma2014.</span>
<span id="cb27-194"><a href="#cb27-194" aria-hidden="true" tabindex="-1"></a>It is thus relevant to consider them for approximating RPs, as the goal is to maximize, w.r.t. the prior,  the mutual information defined in @eq-mutualinfo. </span>
<span id="cb27-195"><a href="#cb27-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-196"><a href="#cb27-196" aria-hidden="true" tabindex="-1"></a>We restrict the set of priors to a parametric space $<span class="sc">\{</span>\pi_\lambda,\,\lambda\in\Lambda<span class="sc">\}</span>$, $\Lambda\subset\mathbb{R}^L$, reducing the original optimization problem into a finite-dimensional one. The optimization problem in @eq-RPdef or @eq-def_const_RP becomes finding $\underset{\lambda\in\Lambda}{\operatorname{argmax}} \, I_{\rD_f}(\pi_\lambda; L_N)$.</span>
<span id="cb27-197"><a href="#cb27-197" aria-hidden="true" tabindex="-1"></a>Our approach is to define the set of priors $\pi_\lambda$ implicitly, as in @gauchy_var_rp:</span>
<span id="cb27-198"><a href="#cb27-198" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-199"><a href="#cb27-199" aria-hidden="true" tabindex="-1"></a>\theta \sim \pi_{\lambda} \iff \theta = g(\lambda,\varepsilon) \quad \text{and} \quad \varepsilon \sim \mathbb{P}_{\varepsilon}.</span>
<span id="cb27-200"><a href="#cb27-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-201"><a href="#cb27-201" aria-hidden="true" tabindex="-1"></a>Here, $g$ is a measurable function parameterized by $\lambda$, typically a neural network with $\lambda$ corresponding to its weights and biases, and we impose that $g$ is differentiable with respect to $\lambda$. </span>
<span id="cb27-202"><a href="#cb27-202" aria-hidden="true" tabindex="-1"></a>The variable $\varepsilon$ can be seen as a latent variable. It has an easy-to-sample distribution $\mathbb{P}_{\varepsilon}$ with a simple density function. In practice we use the centered multivariate Gaussian $\mathcal{N}(0,\mathbb{I}_{p\times p})$. </span>
<span id="cb27-203"><a href="#cb27-203" aria-hidden="true" tabindex="-1"></a>The construction described above allows the consideration of a vast family of priors. However, except in very simple cases, the density of $\pi_\lambda$ is not known and cannot be evaluated. Only samples of $\theta\sim\pi_\lambda$ can be obtained.</span>
<span id="cb27-204"><a href="#cb27-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-205"><a href="#cb27-205" aria-hidden="true" tabindex="-1"></a>In the work of @nalisnick2017learning, this implicit sampling method is compared to several other algorithms used to learn RPs in the case of one-dimensional models. Among these methods, we can mention an algorithm proposed by @berger2009formal which does not sample from the RP but only evaluates it for specific points, or an MCMC-based approach by @lafferty2013iterative, which is inspired from the previous one but can sample from the RP.</span>
<span id="cb27-206"><a href="#cb27-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-207"><a href="#cb27-207" aria-hidden="true" tabindex="-1"></a>According to this comparison, implicit sampling is, in the worst case, competitive with the other methods, but achieves state-of-the-art results in the best case. Hence, computing the variational approximation of the RP, which we will refer to as the VA-RP, seems to be a promising technique. </span>
<span id="cb27-208"><a href="#cb27-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-209"><a href="#cb27-209" aria-hidden="true" tabindex="-1"></a>The situations presented by @gauchy_var_rp and  @nalisnick2017learning are in dimension one and use the Kullback-Leibler divergence within the definition of the mutual information. </span>
<span id="cb27-210"><a href="#cb27-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-211"><a href="#cb27-211" aria-hidden="true" tabindex="-1"></a>The construction of the algorithm that we propose in the following accommodates multi-dimensional modeling. It is also compatible with the extended form of the mutual information, as defined in @eq-mutualinfo from an $f$-divergence.</span>
<span id="cb27-212"><a href="#cb27-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-213"><a href="#cb27-213" aria-hidden="true" tabindex="-1"></a>The choice of the neural network is up to the user, we will showcase in our numerical applications simple networks, composed of one fully connected linear layer and one activation function. However, the method can be used with deeper networks, such as normalizing flows @papamakarios2021nf, or larger networks obtained through a mixture model of smaller networks utilizing the ``Gumbel-Softmax trick'' @jang2017categorical for example. Such choices lead to more flexible parametric distributions, but increase the difficulty of fine-tuning hyperparameters.</span>
<span id="cb27-214"><a href="#cb27-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-215"><a href="#cb27-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-216"><a href="#cb27-216" aria-hidden="true" tabindex="-1"></a><span class="fu">## Learning the VA-RP using stochastic gradient algorithm  {#sec-sga}</span></span>
<span id="cb27-217"><a href="#cb27-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-218"><a href="#cb27-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-219"><a href="#cb27-219" aria-hidden="true" tabindex="-1"></a>The VA-RP is formulated as the solution to the following optimization problem:</span>
<span id="cb27-220"><a href="#cb27-220" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-221"><a href="#cb27-221" aria-hidden="true" tabindex="-1"></a>    \pi_{\lambda^\ast}=\underset{\lambda\in\Lambda}{\operatorname{argmax}} \, \mathcal{O}_{\rD_f}(\pi; L_N),</span>
<span id="cb27-222"><a href="#cb27-222" aria-hidden="true" tabindex="-1"></a>$$ {#eq-opti_pb_pilambda}</span>
<span id="cb27-223"><a href="#cb27-223" aria-hidden="true" tabindex="-1"></a>where $\pi_\lambda$ is parameterized through the relation between a latent variable $\varepsilon$ and  the parameter $\theta$, as outlined in the preceding Section. The function $\mathcal{O}_{\rD_f}$ is called the objective function, it is maximized using  stochastic gradient optimization, following the approach described in @gauchy_var_rp.</span>
<span id="cb27-224"><a href="#cb27-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-225"><a href="#cb27-225" aria-hidden="true" tabindex="-1"></a>It is intuitive to fix $\mathcal{O}_{\rD_f}$ to equal $I_{\rD_f}$, in order to maximize the mutual information of interest.</span>
<span id="cb27-226"><a href="#cb27-226" aria-hidden="true" tabindex="-1"></a>In this Section, we suggest alternative objective functions that can be considered to compute the VA-RP.</span>
<span id="cb27-227"><a href="#cb27-227" aria-hidden="true" tabindex="-1"></a>Our method is adaptable to any objective function $\mathcal{O}_{\rD_f}$  admitting a gradient w.r.t. $\lambda=(\lambda_1,\dots,\lambda_L)$ that takes the form</span>
<span id="cb27-228"><a href="#cb27-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-229"><a href="#cb27-229" aria-hidden="true" tabindex="-1"></a>    \frac{\partial \mathcal{O}_{\rD_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{\mathcal{O}}_{\rD_f}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right]</span>
<span id="cb27-230"><a href="#cb27-230" aria-hidden="true" tabindex="-1"></a>$$ {#eq-compatible_objective_function}</span>
<span id="cb27-231"><a href="#cb27-231" aria-hidden="true" tabindex="-1"></a>for any $l\in<span class="sc">\{</span>1,\dots,L<span class="sc">\}</span>$, where  $\tilde{\mathcal O}_{\rD_f}$ is independent of $\lambda$.</span>
<span id="cb27-232"><a href="#cb27-232" aria-hidden="true" tabindex="-1"></a>This framework allows for flexible implementation, as it permits the separation of sampling and differentiation operations:</span>
<span id="cb27-233"><a href="#cb27-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-234"><a href="#cb27-234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The gradient of $\tilde{\mathcal{O}}_{\rD_f}$ mostly relies on random sampling and depends only on the likelihood $L_N$ and the function $f$.</span>
<span id="cb27-235"><a href="#cb27-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The gradient of $g$ is</span>
<span id="cb27-236"><a href="#cb27-236" aria-hidden="true" tabindex="-1"></a>    computed independently. In practice, </span>
<span id="cb27-237"><a href="#cb27-237" aria-hidden="true" tabindex="-1"></a>    it is possible to leverage usual  differentiation techniques for the neural network.  In our work, we rely on PyTorch's automatic differentiation feature ``autograd'' @torch2019.</span>
<span id="cb27-238"><a href="#cb27-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-239"><a href="#cb27-239" aria-hidden="true" tabindex="-1"></a>This separation is advantageous as  automatic differentiation tools ---such as autograd--- are well-suited to differentiating complex networks but struggle with functions incorporating randomness.</span>
<span id="cb27-240"><a href="#cb27-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-241"><a href="#cb27-241" aria-hidden="true" tabindex="-1"></a>This way, the optimization problem can be addressed using stochastic gradient optimization, approximating at each step the gradient in @eq-compatible_objective_function via Monte Carlo estimates.</span>
<span id="cb27-242"><a href="#cb27-242" aria-hidden="true" tabindex="-1"></a>In our experiments, the implementation of the algorithm is done with the popular Adam optimizer (@kingma2017adam), with its default hyperparameters, $\beta_1=0.9$ and $\beta_2=0.999$. The learning rate is tuned more specifically for each numerical benchmark.</span>
<span id="cb27-243"><a href="#cb27-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-244"><a href="#cb27-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-245"><a href="#cb27-245" aria-hidden="true" tabindex="-1"></a>Concerning the choice of objective function, we verify that $I_{\rD_f}$ is compatible with our method by computing its gradient:</span>
<span id="cb27-246"><a href="#cb27-246" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-247"><a href="#cb27-247" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-248"><a href="#cb27-248" aria-hidden="true" tabindex="-1"></a>\frac{\partial I_{\rD_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) &amp; = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{I}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right] <span class="sc">\\</span></span>
<span id="cb27-249"><a href="#cb27-249" aria-hidden="true" tabindex="-1"></a>&amp; + \mathbb{E}_{\theta \sim \pi_{\lambda}}\left[ \mathbb{E}_{\bX \sim L_N(\cdot |\theta)}\left[ \frac{1}{L_N(\bX \,|\,\theta)} \frac{\partial p_{\lambda}}{\partial \lambda_l}(\bX)f'\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right)\right]  \right],</span>
<span id="cb27-250"><a href="#cb27-250" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-251"><a href="#cb27-251" aria-hidden="true" tabindex="-1"></a>$$ {#eq-gradientIdf}</span>
<span id="cb27-252"><a href="#cb27-252" aria-hidden="true" tabindex="-1"></a>where:</span>
<span id="cb27-253"><a href="#cb27-253" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-254"><a href="#cb27-254" aria-hidden="true" tabindex="-1"></a>    \frac{\partial \tilde{I}}{\partial \theta_j}(\theta) = \mathbb{E}_{\bX \sim L_N(\cdot \,|\,\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\bX \,| \,\theta)F \left(\frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)} \right)  \right], </span>
<span id="cb27-255"><a href="#cb27-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-256"><a href="#cb27-256" aria-hidden="true" tabindex="-1"></a>with $F(x) = f(x)-xf'(x)$ and $p_{\lambda}$ is a shortcut notation for $p_{\pi_{\lambda}, N}$ being the marginal distribution under $\pi_{\lambda}$.</span>
<span id="cb27-257"><a href="#cb27-257" aria-hidden="true" tabindex="-1"></a>In @sec-grad_comp, we provide a detailed derivation of the above equation and show that the second term can be developed to align with the form of @eq-compatible_objective_function.</span>
<span id="cb27-258"><a href="#cb27-258" aria-hidden="true" tabindex="-1"></a>Remark that only the case $f=-\log$ is considered by @gauchy_var_rp, but it leads to a simplification of the gradient since the second term vanishes.</span>
<span id="cb27-259"><a href="#cb27-259" aria-hidden="true" tabindex="-1"></a>Each term in the above equations is approximated as follows:</span>
<span id="cb27-260"><a href="#cb27-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-261"><a href="#cb27-261" aria-hidden="true" tabindex="-1"></a> \begin{cases} \displaystyle</span>
<span id="cb27-262"><a href="#cb27-262" aria-hidden="true" tabindex="-1"></a>     p_{\lambda}(\bX) = \mathbb{E}_{\theta \sim \pi_{\lambda}}[L_N(\bX \, | \, \theta)] \approx \frac{1}{T} \sum_{t=1}^T L_N(\bX \, | \, g(\lambda, \varepsilon_t)) \quad \text{where} \quad \varepsilon_1,...\,, \varepsilon_T \sim \mathbb{P}_{\varepsilon} <span class="sc">\\</span></span>
<span id="cb27-263"><a href="#cb27-263" aria-hidden="true" tabindex="-1"></a>\displaystyle</span>
<span id="cb27-264"><a href="#cb27-264" aria-hidden="true" tabindex="-1"></a>     \frac{\partial \tilde{I}}{\partial \theta_j}(\theta) \approx \frac{1}{U} \sum_{u=1}^{U} \frac{\partial \log L_N}{\partial \theta_j}(\bX^u \,| \,\theta)F \left(\frac{p_{\lambda}(\bX^u)}{L_N(\bX^u \,|\, \theta)} \right) \quad \text{where} \quad \bX^1,...\,,\bX^{U} \sim \mathbb{P}_{\bX|\theta}</span>
<span id="cb27-265"><a href="#cb27-265" aria-hidden="true" tabindex="-1"></a>.\end{cases}  </span>
<span id="cb27-266"><a href="#cb27-266" aria-hidden="true" tabindex="-1"></a>$$ {#eq-MCgradI}</span>
<span id="cb27-267"><a href="#cb27-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-268"><a href="#cb27-268" aria-hidden="true" tabindex="-1"></a>In their work, @nalisnick2017learning propose an alternative objective function to optimize, that we call $B_{\rD_f}$.</span>
<span id="cb27-269"><a href="#cb27-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-270"><a href="#cb27-270" aria-hidden="true" tabindex="-1"></a>This function corresponds to a lower bound of the mutual information. It is derived from an upper bound on the marginal distribution and relies on maximizing the likelihood. Their approach is only presented for $f=-\log$, we generalize the lower bound for any decreasing function $f$: </span>
<span id="cb27-271"><a href="#cb27-271" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-272"><a href="#cb27-272" aria-hidden="true" tabindex="-1"></a>    B_{\rD_f}(\pi; L_N) = \displaystyle \int_{\Theta}\int_{\mathcal{X}^N}f\left(\frac{L_N(\bX\, |\, \hat{\theta}_{MLE})}{L_N(\bX\, |\, \theta)}\right)\pi(\theta)L_N(\bX\, | \, \theta)d\bX d\theta,</span>
<span id="cb27-273"><a href="#cb27-273" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Bdf}</span>
<span id="cb27-274"><a href="#cb27-274" aria-hidden="true" tabindex="-1"></a>where $\hat{\theta}_{MLE}$ being the maximum likelihood estimator (MLE). </span>
<span id="cb27-275"><a href="#cb27-275" aria-hidden="true" tabindex="-1"></a>It only depends on the likelihood and not on $\lambda$ which simplifies the gradient computation: </span>
<span id="cb27-276"><a href="#cb27-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-277"><a href="#cb27-277" aria-hidden="true" tabindex="-1"></a>\frac{\partial B_{\rD_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{B}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right], </span>
<span id="cb27-278"><a href="#cb27-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-279"><a href="#cb27-279" aria-hidden="true" tabindex="-1"></a>where: </span>
<span id="cb27-280"><a href="#cb27-280" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-281"><a href="#cb27-281" aria-hidden="true" tabindex="-1"></a>\frac{\partial \tilde{B}}{\partial \theta_j}(\theta) = \mathbb{E}_{\bX \sim L_N(\cdot \,|\,\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\bX \,|\, \theta)F \left(\frac{L_N(\bX\,| \,\hat{\theta}_{MLE})}{L_N(\bX \,|\,\theta)} \right)  \right]. </span>
<span id="cb27-282"><a href="#cb27-282" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-283"><a href="#cb27-283" aria-hidden="true" tabindex="-1"></a>Its form corresponds to the one expressed in @eq-compatible_objective_function.</span>
<span id="cb27-284"><a href="#cb27-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-285"><a href="#cb27-285" aria-hidden="true" tabindex="-1"></a>Given that  $p_{\lambda}(\bX) \leq \max_{\theta' \in \Theta} L_N(\bX \,|\, \theta') = L_N(\bX\,|\,\hat{\theta}_{MLE})$ for all $\lambda$, we have  $B_{\rD_f}(\pi_{\lambda}; L_N) \leq I_{\rD_f}(\pi_{\lambda};L_N)$.</span>
<span id="cb27-286"><a href="#cb27-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-287"><a href="#cb27-287" aria-hidden="true" tabindex="-1"></a>Since $f_\alpha$, used in $\alpha$-divergence (@eq-falpha), is not decreasing, we replace it with $\hat{f}_\alpha$ defined hereafter, because  $\rD_{f_\alpha}=\rD_{\hat{f}_\alpha}$: </span>
<span id="cb27-288"><a href="#cb27-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-289"><a href="#cb27-289" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-290"><a href="#cb27-290" aria-hidden="true" tabindex="-1"></a>  \hat{f}_{\alpha}(x) = \frac{x^{\alpha}-1}{\alpha(\alpha-1)} = f_{\alpha}(x) + \frac{1}{\alpha-1}(x-1) .</span>
<span id="cb27-291"><a href="#cb27-291" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-292"><a href="#cb27-292" aria-hidden="true" tabindex="-1"></a>The use of this function results in a more stable computation overall.</span>
<span id="cb27-293"><a href="#cb27-293" aria-hidden="true" tabindex="-1"></a>Moreover, one argument for the use of $\alpha$-divergences rather that the KL-divergence, is that we have an universal and explicit upper bound on the mutual information: </span>
<span id="cb27-294"><a href="#cb27-294" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-295"><a href="#cb27-295" aria-hidden="true" tabindex="-1"></a> I_{\rD_{f_\alpha}}(\pi; L_N) = I_{\rD_{\hat{f}_{\alpha}}}(\pi ; L_N) \leq \hat{f}_{\alpha}(0) = \frac{1}{\alpha(1-\alpha)} .  </span>
<span id="cb27-296"><a href="#cb27-296" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-297"><a href="#cb27-297" aria-hidden="true" tabindex="-1"></a>This bound can be an indicator on how well the mutual information is optimized, although there is no guarantee that it can be attained in general.</span>
<span id="cb27-298"><a href="#cb27-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-299"><a href="#cb27-299" aria-hidden="true" tabindex="-1"></a>The gradient of the objective function $B_{\rD_f}$ can be approximated via Monte Carlo, in the same manner as in @eq-MCgradI.</span>
<span id="cb27-300"><a href="#cb27-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-301"><a href="#cb27-301" aria-hidden="true" tabindex="-1"></a>It requires to compute the MLE, which can also be done using samples of $\varepsilon$: </span>
<span id="cb27-302"><a href="#cb27-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-303"><a href="#cb27-303" aria-hidden="true" tabindex="-1"></a>L_N(\bX \, | \, \hat{\theta}_{MLE}) \approx \max_{t\in\{1,\dots,T\}} L_N(\bX \, | \, g(\lambda, \varepsilon_t)) \quad \text{where} \quad \varepsilon_1,..., \varepsilon_T \sim \mathbb{P}_{\varepsilon}. </span>
<span id="cb27-304"><a href="#cb27-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-305"><a href="#cb27-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-306"><a href="#cb27-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-307"><a href="#cb27-307" aria-hidden="true" tabindex="-1"></a><span class="fu">## Adaptation for the constrained VA-RP</span></span>
<span id="cb27-308"><a href="#cb27-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-309"><a href="#cb27-309" aria-hidden="true" tabindex="-1"></a>Reference priors are often criticized, because it can lead to improper posteriors. However, the variational optimization problem defined in @eq-opti_pb_pilambda can be adapted to incorporate simple constraints on the prior.</span>
<span id="cb27-310"><a href="#cb27-310" aria-hidden="true" tabindex="-1"></a>As mentioned in @sec-rp_theory, there exist specific constraints that would make the theoretical solution proper.</span>
<span id="cb27-311"><a href="#cb27-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-312"><a href="#cb27-312" aria-hidden="true" tabindex="-1"></a>This is also a way to incorporate expert knowledge to some extent. We consider $K$ constraints of the form:</span>
<span id="cb27-313"><a href="#cb27-313" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-314"><a href="#cb27-314" aria-hidden="true" tabindex="-1"></a> \forall \, k \in <span class="sc">\{</span>1,\ldots,K<span class="sc">\}</span> \text{,}\, \,   \, \mathcal{C}_k(\pi_{\lambda}) = \mathbb{E}_{\theta \sim \pi_{\lambda}} \left<span class="co">[</span><span class="ot"> a_k(\theta) \right</span><span class="co">]</span> - b_k,</span>
<span id="cb27-315"><a href="#cb27-315" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-316"><a href="#cb27-316" aria-hidden="true" tabindex="-1"></a>with $a_k$ : $\Theta \mapsto \mathbb{R}^+$ integrable and linearly independent functions, and $b_k \in \mathbb{R}$. We then adapt the optimization problem in @eq-opti_pb_pilambda to propose the following constrained optimization problem:</span>
<span id="cb27-317"><a href="#cb27-317" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-318"><a href="#cb27-318" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-319"><a href="#cb27-319" aria-hidden="true" tabindex="-1"></a>&amp; \pi^C_{\lambda^\ast} \in \underset{\lambda \in \Lambda}{\operatorname{argmax}} \, \mathcal{O}_{\rD_f}(\pi_{\lambda}; L_N) <span class="sc">\\</span></span>
<span id="cb27-320"><a href="#cb27-320" aria-hidden="true" tabindex="-1"></a>&amp; \text{subject to} \quad \forall \,k \in <span class="sc">\{</span>1, \ldots, K<span class="sc">\}</span>, \, \,   \, \mathcal{C}_k(\pi_{\lambda}) = 0,</span>
<span id="cb27-321"><a href="#cb27-321" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-322"><a href="#cb27-322" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-323"><a href="#cb27-323" aria-hidden="true" tabindex="-1"></a>where $\pi^C_{\lambda^\ast}$ is the constrained VA-RP. The optimization problem with the mutual information has an explicit asymptotic solution for proper priors verifying the previous conditions: </span>
<span id="cb27-324"><a href="#cb27-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-325"><a href="#cb27-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the case of the KL-divergence (@bernardo2005reference): </span>
<span id="cb27-326"><a href="#cb27-326" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-327"><a href="#cb27-327" aria-hidden="true" tabindex="-1"></a> \pi^C(\theta) \propto J(\theta) \exp \left( 1 + \sum_{k=1}^K \nu_k a_k(\theta)  \right) .  </span>
<span id="cb27-328"><a href="#cb27-328" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-329"><a href="#cb27-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-330"><a href="#cb27-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the case of $\alpha$-divergences (@van2024constr):</span>
<span id="cb27-331"><a href="#cb27-331" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-332"><a href="#cb27-332" aria-hidden="true" tabindex="-1"></a> \pi^C(\theta) \propto J(\theta)  \left( 1 + \sum_{k=1}^K \nu_k a_k(\theta)  \right)^{1/\alpha} . </span>
<span id="cb27-333"><a href="#cb27-333" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-334"><a href="#cb27-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-335"><a href="#cb27-335" aria-hidden="true" tabindex="-1"></a>where $\nu_1,\dots, \nu_K \in \mathbb{R}$ are constants determined by the constraints.</span>
<span id="cb27-336"><a href="#cb27-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-337"><a href="#cb27-337" aria-hidden="true" tabindex="-1"></a>Recent work by @van2024constr makes it possible to build a proper reference prior under a relevant constraint function with $\alpha$-divergence.</span>
<span id="cb27-338"><a href="#cb27-338" aria-hidden="true" tabindex="-1"></a>The theorem considers $a:\Theta\mapsto\mathbb{R}^+$ which verifies the conditions expressed in @eq-condtitions_a. Letting $\mathcal{P}_a$ be the set of priors $\pi$ on $\Theta$ such that $\pi\cdot a\in L^1$, the reference prior $\tilde\pi^\ast$ under the constraint $\tilde\pi^\ast\in\mathcal{P}_a$ is:</span>
<span id="cb27-339"><a href="#cb27-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-340"><a href="#cb27-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-341"><a href="#cb27-341" aria-hidden="true" tabindex="-1"></a>  \tilde\pi^\ast(\theta) \propto J(\theta)a(\theta)^{1/\alpha} .</span>
<span id="cb27-342"><a href="#cb27-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-343"><a href="#cb27-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-344"><a href="#cb27-344" aria-hidden="true" tabindex="-1"></a>We propose the following general method to approximate the VA-RP under such constraints:</span>
<span id="cb27-345"><a href="#cb27-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-346"><a href="#cb27-346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Compute the VA-RP $\pi_{\lambda} \approx J$, in the same manner as for the unconstrained case.</span>
<span id="cb27-347"><a href="#cb27-347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Estimate the constants $\mathcal{K}$ and $c$ using Monte Carlo samples from the VA-RP, as: </span>
<span id="cb27-348"><a href="#cb27-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-349"><a href="#cb27-349" aria-hidden="true" tabindex="-1"></a> \mathcal{K}_{\lambda} = \int_{\Theta} \pi_{\lambda}(\theta)a(\theta)^{1/\alpha}d\theta   \approx \int_{\Theta} J(\theta)a(\theta)^{1/\alpha}d\theta = \mathcal{K},</span>
<span id="cb27-350"><a href="#cb27-350" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb27-351"><a href="#cb27-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-352"><a href="#cb27-352" aria-hidden="true" tabindex="-1"></a>  c_{\lambda} =  \int_{\Theta} \pi_{\lambda}(\theta)a(\theta)^{1+(1/\alpha)}d\theta \approx \int_{\Theta} J(\theta)a(\theta)^{1+(1/\alpha)}d\theta = c .</span>
<span id="cb27-353"><a href="#cb27-353" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-354"><a href="#cb27-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-355"><a href="#cb27-355" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Since we have the equality:</span>
<span id="cb27-356"><a href="#cb27-356" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-357"><a href="#cb27-357" aria-hidden="true" tabindex="-1"></a> \mathbb{E}_{\theta \sim \tilde\pi^\ast}[a(\theta)] = \int_{\Theta} \tilde\pi^\ast(\theta)a(\theta)d\theta = \frac{1}{\mathcal{K}}\int_{\Theta}J(\theta)a(\theta)^{1+(1/\alpha)}d\theta  = \frac{c}{\mathcal{K}},  </span>
<span id="cb27-358"><a href="#cb27-358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-359"><a href="#cb27-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-360"><a href="#cb27-360" aria-hidden="true" tabindex="-1"></a>we compute the constrained VA-RP using the constraint : $\mathbb{E}_{\theta \sim \pi_{\lambda'}}[a(\theta)] = c_{\lambda} / \mathcal{K}_{\lambda}$ to approximate $\pi_{\lambda'} \approx \tilde\pi^\ast$.</span>
<span id="cb27-361"><a href="#cb27-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-362"><a href="#cb27-362" aria-hidden="true" tabindex="-1"></a>One might use different variational approximations for $\pi_{\lambda}$ and $\pi_{\lambda'}$ because $J$ and $\tilde\pi^\ast$ could have very different forms depending on the function $a$.</span>
<span id="cb27-363"><a href="#cb27-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-364"><a href="#cb27-364" aria-hidden="true" tabindex="-1"></a>The idea is to solve the constrained optimization problem as an unconstrained problem but with a Lagrangian as the objective function. We take the work of @nocedal2006penalty as support. </span>
<span id="cb27-365"><a href="#cb27-365" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb27-366"><a href="#cb27-366" aria-hidden="true" tabindex="-1"></a>We denote $\eta$ the Lagrange multiplier. Instead of using the usual Lagrangian function, @nocedal2006penalty suggest adding a term defined with $\tilde{\eta}$, a vector with positive components which serve as penalization coefficients, and $\eta'$ which can be thought of a prior estimate of $\eta$, although not in a Bayesian sense.</span>
<span id="cb27-367"><a href="#cb27-367" aria-hidden="true" tabindex="-1"></a>The objective is to find a saddle point $(\lambda^*, \eta^*)$ which is a solution of the updated optimization problem:</span>
<span id="cb27-368"><a href="#cb27-368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-369"><a href="#cb27-369" aria-hidden="true" tabindex="-1"></a> \max_{\lambda} \, \left(\min_{\eta} \,  \mathcal{O}_{\rD_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k \mathcal{C}_k(\pi_{\lambda})  + \sum_{k=1}^K \frac{1}{2\tilde{\eta}_k} ( \eta_k - \eta_k')^2 \right)   .  </span>
<span id="cb27-370"><a href="#cb27-370" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb27-371"><a href="#cb27-371" aria-hidden="true" tabindex="-1"></a> One can see that the third term serves as a penalization for large deviations from $\eta'$. The minimization on $\eta$ is feasible because it is a convex quadratic, and we get $\eta = \eta' - \tilde{\eta} \cdot \mathcal{C}(\pi_\lambda)$. Replacing $\eta$ by its expression leads to the resolution of the problem: </span>
<span id="cb27-372"><a href="#cb27-372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-373"><a href="#cb27-373" aria-hidden="true" tabindex="-1"></a>  \max_{\lambda} \, \mathcal{O}_{\rD_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k' \mathcal{C}_k(\pi_{\lambda}) - \sum_{k=1}^K \frac{\tilde{\eta}_k}{2} \mathcal{C}_k(\pi_{\lambda})^2 .</span>
<span id="cb27-374"><a href="#cb27-374" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb27-375"><a href="#cb27-375" aria-hidden="true" tabindex="-1"></a>This motivates the definition of the augmented Lagrangian:</span>
<span id="cb27-376"><a href="#cb27-376" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb27-377"><a href="#cb27-377" aria-hidden="true" tabindex="-1"></a>\mathcal{L}_A(\lambda, \eta, \tilde{\eta}) = \mathcal{O}_{\rD_f}(\pi_{\lambda}; L_N) + \sum_{k=1}^K \eta_k \mathcal{C}_k(\pi_{\lambda}) - \sum_{k=1}^K \frac{\tilde{\eta}_k}{2} \mathcal{C}_k(\pi_{\lambda})^2 .</span>
<span id="cb27-378"><a href="#cb27-378" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-379"><a href="#cb27-379" aria-hidden="true" tabindex="-1"></a>Its gradient has a form that  which is compatible with our algorithm, as depicted in @sec-sga (see @eq-compatible_objective_function):</span>
<span id="cb27-380"><a href="#cb27-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-381"><a href="#cb27-381" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-382"><a href="#cb27-382" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-383"><a href="#cb27-383" aria-hidden="true" tabindex="-1"></a> \frac{\partial \mathcal{L}_A}{\partial \lambda}(\lambda, \eta, \tilde{\eta}) &amp; = \frac{\partial \mathcal{O}_{\rD_f}}{\partial \lambda}(\pi_{\lambda}; L_N) +  \mathbb{E}_{\varepsilon}\left[ \left(\sum_{k=1}^K  \frac{\partial a_k}{\partial \theta}(g(\lambda, \varepsilon))(\eta_k - \tilde{\eta}_k\mathcal{C}_k(\pi_{\lambda}))\right)\frac{\partial g}{\partial \lambda}(\lambda,\varepsilon)\right] <span class="sc">\\</span></span>
<span id="cb27-384"><a href="#cb27-384" aria-hidden="true" tabindex="-1"></a>&amp; = \mathbb{E}_{\varepsilon}\left[ \left( \frac{\partial \tilde{\mathcal{O}}}{\partial \theta}(g(\lambda,\varepsilon))  + \sum_{k=1}^K  \frac{\partial a_k}{\partial \theta}(g(\lambda, \varepsilon))(\eta_k - \tilde{\eta}_k\mathcal{C}_k(\pi_{\lambda}))\right)\frac{\partial g}{\partial \lambda}(\lambda,\varepsilon)  \right] .</span>
<span id="cb27-385"><a href="#cb27-385" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-386"><a href="#cb27-386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-387"><a href="#cb27-387" aria-hidden="true" tabindex="-1"></a>In practice, the augmented Lagrangian algorithm is of the form: </span>
<span id="cb27-388"><a href="#cb27-388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-389"><a href="#cb27-389" aria-hidden="true" tabindex="-1"></a>   \begin{cases}</span>
<span id="cb27-390"><a href="#cb27-390" aria-hidden="true" tabindex="-1"></a>    \lambda^{t+1} = \underset{\lambda}{\operatorname{argmax}} \, \mathcal{L}_A(\lambda, \eta^t, \tilde{\eta}) <span class="sc">\\</span></span>
<span id="cb27-391"><a href="#cb27-391" aria-hidden="true" tabindex="-1"></a>    \forall k \in <span class="sc">\{</span>1, \ldots, K<span class="sc">\}</span>, \, \eta_k^{t+1} = \eta_k^t - \tilde{\eta}_k\cdot \mathcal{C}_k(\pi_{\lambda^{t+1}}).</span>
<span id="cb27-392"><a href="#cb27-392" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-393"><a href="#cb27-393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-394"><a href="#cb27-394" aria-hidden="true" tabindex="-1"></a>In our implementation, $\eta$ is updated every $100$ epochs. The penalty parameter $\tilde{\eta}$ can be interpreted as the learning rate of $\eta$, we use an adaptive scheme  inspired by @basir2023adaptive where we check if the largest constraint value $|| \mathcal{C}(\pi_{\lambda}) ||_{\infty}$ is higher than a specified threshold $M$ or not. If $|| \mathcal{C}(\pi_{\lambda}) ||_{\infty} &gt; M$, we multiply $\tilde{\eta}$ by $v$, otherwise we divide by $v$. We also impose a maximum value $\tilde{\eta}_{max}$.</span>
<span id="cb27-395"><a href="#cb27-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-396"><a href="#cb27-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-397"><a href="#cb27-397" aria-hidden="true" tabindex="-1"></a><span class="fu">## Posterior sampling using implicitly defined prior distributions {#sec-posterio-sampling}</span></span>
<span id="cb27-398"><a href="#cb27-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-399"><a href="#cb27-399" aria-hidden="true" tabindex="-1"></a>Although our main object of study is the prior distribution, one needs to find the posterior distribution given an observed dataset $\bX$ in order to do the inference on $\theta$. The posterior is of the form : </span>
<span id="cb27-400"><a href="#cb27-400" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-401"><a href="#cb27-401" aria-hidden="true" tabindex="-1"></a>\pi_{\lambda}(\theta \, | \, \bX) = \frac{\pi_{\lambda}(\theta)L_N(\bX \, | \, \theta)}{p_{\lambda}(\bX)} . </span>
<span id="cb27-402"><a href="#cb27-402" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-403"><a href="#cb27-403" aria-hidden="true" tabindex="-1"></a>As discussed in the introduction, one can approximate the posterior distribution when knowing the prior either by using MCMC or variational inference. In both cases, knowing the marginal distribution is not required. Indeed,  MCMC samplers inspired by the Metropolis-Hastings algorithm can be applied, even if the posterior distribution is only known up to a multiplicative constant. The same can be said for variational approximation since the ELBO can be expressed without the marginal. </span>
<span id="cb27-404"><a href="#cb27-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-405"><a href="#cb27-405" aria-hidden="true" tabindex="-1"></a>The issue here is that the density function $\pi_{\lambda}(\theta)$ is not explicit and can not be evaluated, except for very simple cases. However, we imposed that the distribution of the variable $\varepsilon$ is simple enough so one is able to evaluate its density. We propose to use $\varepsilon$ as the variable of interest instead of $\theta$ because it lets us circumvent this issue. In practice, the idea is to reverse the order of operations : instead of sampling $\varepsilon$, then transforming $\varepsilon$ into $\theta$, which defines the prior on $\theta$, and finally sampling posterior samples of $\theta$ given $X$, one can proceed as follows : </span>
<span id="cb27-406"><a href="#cb27-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-407"><a href="#cb27-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-408"><a href="#cb27-408" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define the posterior distribution on $\varepsilon$ : </span>
<span id="cb27-409"><a href="#cb27-409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-410"><a href="#cb27-410" aria-hidden="true" tabindex="-1"></a>\pi_{\varepsilon,\lambda}(\varepsilon \, | \, \bX) = \frac{p_{\varepsilon}(\varepsilon)L_N(\bX\, | \, g(\lambda, \varepsilon))}{p_{\lambda}(\bX)} \ , </span>
<span id="cb27-411"><a href="#cb27-411" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-412"><a href="#cb27-412" aria-hidden="true" tabindex="-1"></a>where $p_{\varepsilon}$ is the probability density function of $\varepsilon$. $\pi_{\varepsilon,\lambda}(\varepsilon \, | \, \bX)$ is known up to a multiplicative constant since the marginal $p_{\lambda}$ is intractable in general. It is indeed a probability distribution on $\mathbb{R}^p$ because : </span>
<span id="cb27-413"><a href="#cb27-413" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-414"><a href="#cb27-414" aria-hidden="true" tabindex="-1"></a>p_{\lambda}(\bX) = \int_{\Theta} \pi_{\lambda}(\theta)L_N(\bX \, | \, \theta)d\theta = \int_{\mathbb{R}^p} L_N(\bX\, | \, g(\lambda, \varepsilon)) d\mathbb{P}_{\varepsilon} </span>
<span id="cb27-415"><a href="#cb27-415" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-416"><a href="#cb27-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-417"><a href="#cb27-417" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sample posterior $\varepsilon$ samples from the previous distribution, approximated by MCMC or variational inference. </span>
<span id="cb27-418"><a href="#cb27-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-419"><a href="#cb27-419" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Apply the transformation $\theta = g(\lambda, \varepsilon)$, and one gets posterior $\theta$ samples :  $\theta \sim \pi_{\lambda}(\cdot \, | \, \bX)$.</span>
<span id="cb27-420"><a href="#cb27-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-421"><a href="#cb27-421" aria-hidden="true" tabindex="-1"></a>More precisely, we denote for a fixed dataset $\bX$ : </span>
<span id="cb27-422"><a href="#cb27-422" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-423"><a href="#cb27-423" aria-hidden="true" tabindex="-1"></a> \theta \sim  \tilde{\pi}_{\lambda}(\cdot \, | \, \bX) \iff \theta = g(\lambda, \varepsilon) \quad \text{with} \quad \varepsilon \sim \pi_{\varepsilon,\lambda}(\cdot \, | \, \bX). </span>
<span id="cb27-424"><a href="#cb27-424" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-425"><a href="#cb27-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-426"><a href="#cb27-426" aria-hidden="true" tabindex="-1"></a>The previous approach is valid because $\pi_{\lambda}(\cdot \, | \, \bX)$ and $\tilde{\pi}_{\lambda}(\cdot \, | \, \bX)$ lead to the same distribution, as proven by the following derivation : let $\varphi$ be a bounded and measurable function on $\Theta$. </span>
<span id="cb27-427"><a href="#cb27-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-428"><a href="#cb27-428" aria-hidden="true" tabindex="-1"></a>Using the definitions of the different distributions, we have that: </span>
<span id="cb27-429"><a href="#cb27-429" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-430"><a href="#cb27-430" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-431"><a href="#cb27-431" aria-hidden="true" tabindex="-1"></a>    \int_{\Theta} \varphi(\theta) \tilde{\pi}_{\lambda}(\theta \, | \, \bX) d\theta &amp; = \int_{\mathbb{R}^p} \varphi(g(\lambda, \varepsilon)) \pi_{\varepsilon,\lambda}(\varepsilon \, | \, \bX) d\varepsilon <span class="sc">\\</span></span>
<span id="cb27-432"><a href="#cb27-432" aria-hidden="true" tabindex="-1"></a>    &amp; = \int_{\mathbb{R}^p} \varphi(g(\lambda, \varepsilon)) \frac{p_{\varepsilon}(\varepsilon)L_N(X\, | \, g(\lambda, \varepsilon))}{p_{\lambda}(\bX)} d\varepsilon <span class="sc">\\</span></span>
<span id="cb27-433"><a href="#cb27-433" aria-hidden="true" tabindex="-1"></a>    &amp; = \int_{\Theta} \varphi(\theta)\pi_{\lambda}(\theta) \frac{L_N(\bX \, | \, \theta)}{p_{\lambda}(\bX)}</span>
<span id="cb27-434"><a href="#cb27-434" aria-hidden="true" tabindex="-1"></a>     d\theta <span class="sc">\\</span></span>
<span id="cb27-435"><a href="#cb27-435" aria-hidden="true" tabindex="-1"></a>    &amp; = \int_{\Theta} \varphi(\theta) \pi_{\lambda}(\theta \, | \, \bX) d\theta.</span>
<span id="cb27-436"><a href="#cb27-436" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-437"><a href="#cb27-437" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-438"><a href="#cb27-438" aria-hidden="true" tabindex="-1"></a>As mentioned in the last Section, when the RP is improper, we compare the posterior distributions, namely, the exact reference posterior when available and the posterior obtained from the VA-RP using the previous method. Altogether, we are able to sample $\theta$ from the posterior even if the density of the parametric prior $\pi_{\lambda}$ on $\theta$ is unavailable due to an implicit definition of the prior distribution. </span>
<span id="cb27-439"><a href="#cb27-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-440"><a href="#cb27-440" aria-hidden="true" tabindex="-1"></a>For our computations, we choose MCMC sampling, namely an adaptive Metropolis-Hastings sampler with a multivariate Gaussian as the proposition distribution. The adaptation scheme is the following:  for each batch of iterations, we monitor the acceptance rate and we adapt the variance parameter of the Gaussian proposition in order to have an acceptance rate close to 40\%, which is the advised value @gelman2013 for models in small dimensions. We refer to this algorithm as MH($\varepsilon$). Because we apply MCMC sampling on variable $\varepsilon \in \mathbb{R}^p$ with a reasonable value for $p$, we expect this step of the algorithm to be fast compared to the computation of the VA-RP. </span>
<span id="cb27-441"><a href="#cb27-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-442"><a href="#cb27-442" aria-hidden="true" tabindex="-1"></a>One could also use classic variational inference on $\varepsilon$ instead, but the parametric set of distributions must be chosen wisely. In VAEs for instance, multivariate Gaussian are often considered since it simplifies the KL-divergence term in the ELBO. However, this might be too simplistic in our case since we must apply the neural network $g$ to recover $\theta$ samples. This means that the approximated posterior on $\theta$ belongs to a very similar set of distributions to those used for the VA-RP, since we already used a multivariate Gaussian for the prior on $\varepsilon$. On the other hand, applying once again the implicit sampling approach does not exploit the additional information we have on $\pi_{\varepsilon, \lambda}(\varepsilon \, | \, \bX)$ compared to $\pi_{\lambda}(\theta)$, specifically, that its density function is known up to a multiplicative constant. Hence, we argue that using a Metropolis-Hastings sampler is more straightforward in this situation.</span>
<span id="cb27-443"><a href="#cb27-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-444"><a href="#cb27-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-445"><a href="#cb27-445" aria-hidden="true" tabindex="-1"></a><span class="fu"># Numerical experiments  {#sec-numexp}</span></span>
<span id="cb27-446"><a href="#cb27-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-447"><a href="#cb27-447" aria-hidden="true" tabindex="-1"></a>We want to apply our algorithm to different statistical models, the first one is the multinomial model, which is the simplest in the sense that the target distributions ---the Jeffreys prior and posterior--- have explicit expressions and are part of a usual parametric family of proper distributions. The second model ---the probit model--- will be highlighted with supplementary computations, in regards to the assessment of the stability of our stochastic algorithm, and also with the addition of a moment constraint. </span>
<span id="cb27-448"><a href="#cb27-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-449"><a href="#cb27-449" aria-hidden="true" tabindex="-1"></a>The one-dimensional statistical model of the Gaussian distribution with variance parameter is also presented in @sec-appendix.</span>
<span id="cb27-450"><a href="#cb27-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-451"><a href="#cb27-451" aria-hidden="true" tabindex="-1"></a>Since we only have to compute quotients of the likelihood or the gradient of the log-likelihood, we can omit the multiplicative constant which does not depend on $\theta$.</span>
<span id="cb27-452"><a href="#cb27-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-453"><a href="#cb27-453" aria-hidden="true" tabindex="-1"></a>As for the output of the neural networks, the activation function just before the output is different for each statistical model, the same can be said for the learning rate. In some cases, we apply an affine transformation on the variable $\theta$ to avoid divisions by zero during training. In every test case, we consider simple networks for an easier fine-tuning of the hyperparameters and also because the precise computation of the loss function is an important bottleneck.</span>
<span id="cb27-454"><a href="#cb27-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-455"><a href="#cb27-455" aria-hidden="true" tabindex="-1"></a>For the initialization of the neural networks, biases are set to zero and weights are randomly sampled from a Gaussian distribution. As for the several hyperparameters, we take $N=10$, $T=50$ and $U=1000$ unless stated otherwise. We take a latent space of dimension $p=50$. For the posterior calculations, we keep the last $5\cdot 10^4$ samples from the Markov chain over a total of $10^5$ Metropolis-Hastings iterations. Increasing $N$ is advised in order to get closer to the asymptotic case for the optimization problem, and increasing $U$ and $T$ is relevant for the precision of the Monte Carlo estimates. Nevertheless, this increases computation times and we have to do a trade-off between the former and the latter. As for the constrained optimization, we use $v=2$, $M=0.005$ and $\tilde{\eta}_{max} = 10^4$.</span>
<span id="cb27-456"><a href="#cb27-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-457"><a href="#cb27-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-458"><a href="#cb27-458" aria-hidden="true" tabindex="-1"></a><span class="fu">## Multinomial model</span></span>
<span id="cb27-459"><a href="#cb27-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-460"><a href="#cb27-460" aria-hidden="true" tabindex="-1"></a>The multinomial distribution can be interpreted as the generalization of the binomial distribution for higher dimensions. We denote : </span>
<span id="cb27-461"><a href="#cb27-461" aria-hidden="true" tabindex="-1"></a>$X_i \sim \text{Multinomial}(n,(\theta_1,...,\theta_q))$ with $n \in \mathbb{N}^*$, $\bX \in \mathcal{X}^N$ and $\theta \in \Theta$, with : $\mathcal{X} = <span class="sc">\{</span> X \in <span class="sc">\{</span>0,...,n<span class="sc">\}</span>^q \, | \, \sum_{j = 1}^q X^j = n <span class="sc">\}</span>$ and </span>
<span id="cb27-462"><a href="#cb27-462" aria-hidden="true" tabindex="-1"></a>$\Theta = <span class="sc">\{</span> \theta \in (0,1)^q \, | \, \sum_{j = 1}^q \theta_j = 1  <span class="sc">\}</span>$. We use $n=10$ and $q=\text{dim}(\theta)=4$.</span>
<span id="cb27-463"><a href="#cb27-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-464"><a href="#cb27-464" aria-hidden="true" tabindex="-1"></a>The likelihood function and the gradient of its logarithm are:</span>
<span id="cb27-465"><a href="#cb27-465" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-466"><a href="#cb27-466" aria-hidden="true" tabindex="-1"></a> L_N(\bX\,|\,\theta) = \prod_{i=1}^N \frac{n!}{X_i^1 ! \cdot ... \cdot X_i^q !} \prod_{j=1}^q \theta_{j}^{X_i^j} \propto  \prod_{i=1}^N \prod_{j=1}^q \theta_{j}^{X_i^j} </span>
<span id="cb27-467"><a href="#cb27-467" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-468"><a href="#cb27-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-469"><a href="#cb27-469" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-470"><a href="#cb27-470" aria-hidden="true" tabindex="-1"></a> \forall (i,j), \,  \frac{\partial \log L}{\partial \theta_j}(X_i\,|\,\theta) = \frac{X_i^j}{\theta_j}. </span>
<span id="cb27-471"><a href="#cb27-471" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-472"><a href="#cb27-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-473"><a href="#cb27-473" aria-hidden="true" tabindex="-1"></a>The MLE is available : $\forall j, \, \hat{\theta}_{MLE}(j) = \frac{1}{nN}\sum_{i=1}^N X_i^j$ and the Jeffreys prior is the $\text{Dir}_q \left(\frac{1}{2}, ... , \frac{1}{2} \right)$ distribution, which is proper. The Jeffreys posterior is a conjugate Dirichlet distribution: </span>
<span id="cb27-474"><a href="#cb27-474" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-475"><a href="#cb27-475" aria-hidden="true" tabindex="-1"></a>J_{post}(\theta \, | \, \bX) = \text{Dir}_q(\theta; \gamma) \quad \text{with} \quad \gamma_j = \frac{1}{2} + \sum_{i=1}^N X_i^j .</span>
<span id="cb27-476"><a href="#cb27-476" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-477"><a href="#cb27-477" aria-hidden="true" tabindex="-1"></a>We recall that the probability density function of a Dirichlet distribution is the following: </span>
<span id="cb27-478"><a href="#cb27-478" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-479"><a href="#cb27-479" aria-hidden="true" tabindex="-1"></a> \text{Dir}_q(x; \gamma) = \frac{\Gamma(\sum_{j=1}^q \gamma_j)}{\prod_{j=1}^q \Gamma(\gamma_j)} \prod_{j=1}^q x_j^{\gamma_j - 1}.  </span>
<span id="cb27-480"><a href="#cb27-480" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-481"><a href="#cb27-481" aria-hidden="true" tabindex="-1"></a>For approximating the RP, we opt for a simple neural network with one linear layer and a Softmax activation function assuring that all components are positive and sum to 1. Explicitly, we have that: </span>
<span id="cb27-482"><a href="#cb27-482" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-483"><a href="#cb27-483" aria-hidden="true" tabindex="-1"></a>\theta = \text{Softmax}(W^{\top}\varepsilon + b),  </span>
<span id="cb27-484"><a href="#cb27-484" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-485"><a href="#cb27-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-486"><a href="#cb27-486" aria-hidden="true" tabindex="-1"></a>with $W \in \mathbb{R}^{p,4}$ the weight matrix and $b \in \mathbb{R}^4$ the bias vector. The density function of $\theta$ does not have a closed expression. The following results are obtained with $\alpha=0.5$ for the divergence and the lower bound is used as the objective function.</span>
<span id="cb27-487"><a href="#cb27-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-488"><a href="#cb27-488" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-489"><a href="#cb27-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-490"><a href="#cb27-490" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameters and classes</span></span>
<span id="cb27-491"><a href="#cb27-491" aria-hidden="true" tabindex="-1"></a><span class="in">p = 50     # latent space dimension</span></span>
<span id="cb27-492"><a href="#cb27-492" aria-hidden="true" tabindex="-1"></a><span class="in">q = 4      # parameter space dimension</span></span>
<span id="cb27-493"><a href="#cb27-493" aria-hidden="true" tabindex="-1"></a><span class="in">n = 10     # multinomial parameter</span></span>
<span id="cb27-494"><a href="#cb27-494" aria-hidden="true" tabindex="-1"></a><span class="in">N = 10     # number of data samples</span></span>
<span id="cb27-495"><a href="#cb27-495" aria-hidden="true" tabindex="-1"></a><span class="in">J = 1000   # nb samples for MC estimation in MI</span></span>
<span id="cb27-496"><a href="#cb27-496" aria-hidden="true" tabindex="-1"></a><span class="in">T = 50     # nb samples MC marginal likelihood</span></span>
<span id="cb27-497"><a href="#cb27-497" aria-hidden="true" tabindex="-1"></a><span class="in">Multinom = torch_MultinomialModel(n=n, q=q)  </span></span>
<span id="cb27-498"><a href="#cb27-498" aria-hidden="true" tabindex="-1"></a><span class="in">input_size = p  </span></span>
<span id="cb27-499"><a href="#cb27-499" aria-hidden="true" tabindex="-1"></a><span class="in">output_size = q</span></span>
<span id="cb27-500"><a href="#cb27-500" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**5</span></span>
<span id="cb27-501"><a href="#cb27-501" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Multinomial_results.pkl'</span></span>
<span id="cb27-502"><a href="#cb27-502" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-503"><a href="#cb27-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-504"><a href="#cb27-504" aria-hidden="true" tabindex="-1"></a><span class="in">low = 0.001          # lower bound (must verifiy q * low &lt; 1)</span></span>
<span id="cb27-505"><a href="#cb27-505" aria-hidden="true" tabindex="-1"></a><span class="in">upp = 1 - low*(q-1)  # upper bound (forced value with low and q)</span></span>
<span id="cb27-506"><a href="#cb27-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-507"><a href="#cb27-507" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(0)</span></span>
<span id="cb27-508"><a href="#cb27-508" aria-hidden="true" tabindex="-1"></a><span class="in">NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Softmax(dim=-1))</span></span>
<span id="cb27-509"><a href="#cb27-509" aria-hidden="true" tabindex="-1"></a><span class="in">NN = AffineTransformation(NN, m_low=low, M_upp=upp)</span></span>
<span id="cb27-510"><a href="#cb27-510" aria-hidden="true" tabindex="-1"></a><span class="in">VA = VA_NeuralNet(neural_net=NN, model=Multinom)</span></span>
<span id="cb27-511"><a href="#cb27-511" aria-hidden="true" tabindex="-1"></a><span class="in">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb27-512"><a href="#cb27-512" aria-hidden="true" tabindex="-1"></a><span class="in">Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=0.5, use_log_lik=True, use_baseline=False)</span></span>
<span id="cb27-513"><a href="#cb27-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-514"><a href="#cb27-514" aria-hidden="true" tabindex="-1"></a><span class="in">with torch.no_grad():</span></span>
<span id="cb27-515"><a href="#cb27-515" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-516"><a href="#cb27-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-517"><a href="#cb27-517" aria-hidden="true" tabindex="-1"></a><span class="in">num_epochs = 10000</span></span>
<span id="cb27-518"><a href="#cb27-518" aria-hidden="true" tabindex="-1"></a><span class="in">loss_fct = "LB_MI"</span></span>
<span id="cb27-519"><a href="#cb27-519" aria-hidden="true" tabindex="-1"></a><span class="in">optimizer = torch_Adam</span></span>
<span id="cb27-520"><a href="#cb27-520" aria-hidden="true" tabindex="-1"></a><span class="in">num_samples_MI = 200</span></span>
<span id="cb27-521"><a href="#cb27-521" aria-hidden="true" tabindex="-1"></a><span class="in">freq_MI = 200</span></span>
<span id="cb27-522"><a href="#cb27-522" aria-hidden="true" tabindex="-1"></a><span class="in">save_best_param = True</span></span>
<span id="cb27-523"><a href="#cb27-523" aria-hidden="true" tabindex="-1"></a><span class="in">learning_rate = 0.0025</span></span>
<span id="cb27-524"><a href="#cb27-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-525"><a href="#cb27-525" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_Multinom :</span></span>
<span id="cb27-526"><a href="#cb27-526" aria-hidden="true" tabindex="-1"></a><span class="in">    ### Training loop</span></span>
<span id="cb27-527"><a href="#cb27-527" aria-hidden="true" tabindex="-1"></a><span class="in">    MI, range_MI, lower_MI, upper_MI = Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span></span>
<span id="cb27-528"><a href="#cb27-528" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            num_samples_MI, freq_MI, save_best_param, </span></span>
<span id="cb27-529"><a href="#cb27-529" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            learning_rate, momentum=True)</span></span>
<span id="cb27-530"><a href="#cb27-530" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-531"><a href="#cb27-531" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-532"><a href="#cb27-532" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-533"><a href="#cb27-533" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_sample = Div.model.sample_Jeffreys(n_samples=n_samples_prior).numpy()</span></span>
<span id="cb27-534"><a href="#cb27-534" aria-hidden="true" tabindex="-1"></a><span class="in">    all_params = torch.cat([param.view(-1) for param in NN.parameters()])</span></span>
<span id="cb27-535"><a href="#cb27-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-536"><a href="#cb27-536" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-537"><a href="#cb27-537" aria-hidden="true" tabindex="-1"></a><span class="in">    # Sample data from 'true' parameter</span></span>
<span id="cb27-538"><a href="#cb27-538" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_true = torch.tensor(q*[1/q])  </span></span>
<span id="cb27-539"><a href="#cb27-539" aria-hidden="true" tabindex="-1"></a><span class="in">    N = 10</span></span>
<span id="cb27-540"><a href="#cb27-540" aria-hidden="true" tabindex="-1"></a><span class="in">    D = Multinom.sample(theta_true, N, 1)</span></span>
<span id="cb27-541"><a href="#cb27-541" aria-hidden="true" tabindex="-1"></a><span class="in">    X = D[:, 0]</span></span>
<span id="cb27-542"><a href="#cb27-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-543"><a href="#cb27-543" aria-hidden="true" tabindex="-1"></a><span class="in">    # Posterior samples using Jeffreys prior</span></span>
<span id="cb27-544"><a href="#cb27-544" aria-hidden="true" tabindex="-1"></a><span class="in">    n_samples_post = 5*10**4</span></span>
<span id="cb27-545"><a href="#cb27-545" aria-hidden="true" tabindex="-1"></a><span class="in">    jeffreys_post = Multinom.sample_post_Jeffreys(X, n_samples_post)</span></span>
<span id="cb27-546"><a href="#cb27-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-547"><a href="#cb27-547" aria-hidden="true" tabindex="-1"></a><span class="in">    T_mcmc = 10**5 + 1</span></span>
<span id="cb27-548"><a href="#cb27-548" aria-hidden="true" tabindex="-1"></a><span class="in">    sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-549"><a href="#cb27-549" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_0 = torch.randn(p)</span></span>
<span id="cb27-550"><a href="#cb27-550" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap=True, Cov=True)</span></span>
<span id="cb27-551"><a href="#cb27-551" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_MH = NN(eps_MH)</span></span>
<span id="cb27-552"><a href="#cb27-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-553"><a href="#cb27-553" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-554"><a href="#cb27-554" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = theta_MH.numpy()</span></span>
<span id="cb27-555"><a href="#cb27-555" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_post = jeffreys_post.numpy()</span></span>
<span id="cb27-556"><a href="#cb27-556" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post = theta_MH[-n_samples_post:,:]</span></span>
<span id="cb27-557"><a href="#cb27-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-558"><a href="#cb27-558" aria-hidden="true" tabindex="-1"></a><span class="in">    # Saves all relevant quantities</span></span>
<span id="cb27-559"><a href="#cb27-559" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-560"><a href="#cb27-560" aria-hidden="true" tabindex="-1"></a><span class="in">    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI}</span></span>
<span id="cb27-561"><a href="#cb27-561" aria-hidden="true" tabindex="-1"></a><span class="in">    Prior = {'samples' : theta_sample, 'jeffreys' : jeffreys_sample, 'params' : all_params}</span></span>
<span id="cb27-562"><a href="#cb27-562" aria-hidden="true" tabindex="-1"></a><span class="in">    Posterior = {'samples' : theta_post, 'jeffreys' : jeffreys_post}</span></span>
<span id="cb27-563"><a href="#cb27-563" aria-hidden="true" tabindex="-1"></a><span class="in">    Multinom_results = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}</span></span>
<span id="cb27-564"><a href="#cb27-564" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-565"><a href="#cb27-565" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(Multinom_results, file)</span></span>
<span id="cb27-566"><a href="#cb27-566" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-567"><a href="#cb27-567" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-568"><a href="#cb27-568" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-569"><a href="#cb27-569" aria-hidden="true" tabindex="-1"></a><span class="in">        res = pickle.load(file)</span></span>
<span id="cb27-570"><a href="#cb27-570" aria-hidden="true" tabindex="-1"></a><span class="in">        MI_dic = res['MI']</span></span>
<span id="cb27-571"><a href="#cb27-571" aria-hidden="true" tabindex="-1"></a><span class="in">        Prior = res['prior'] </span></span>
<span id="cb27-572"><a href="#cb27-572" aria-hidden="true" tabindex="-1"></a><span class="in">        Posterior = res['post']</span></span>
<span id="cb27-573"><a href="#cb27-573" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper']</span></span>
<span id="cb27-574"><a href="#cb27-574" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']</span></span>
<span id="cb27-575"><a href="#cb27-575" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post, jeffreys_post = Posterior['samples'], Posterior['jeffreys']</span></span>
<span id="cb27-576"><a href="#cb27-576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-577"><a href="#cb27-577" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-578"><a href="#cb27-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-579"><a href="#cb27-579" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-580"><a href="#cb27-580" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-multi_mi</span></span>
<span id="cb27-581"><a href="#cb27-581" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 200 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. The learning rate used in the optimization is $0.0025$."</span></span>
<span id="cb27-582"><a href="#cb27-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-583"><a href="#cb27-583" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-584"><a href="#cb27-584" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '-', color=rougeCEA)</span></span>
<span id="cb27-585"><a href="#cb27-585" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '*', color=rougeCEA)</span></span>
<span id="cb27-586"><a href="#cb27-586" aria-hidden="true" tabindex="-1"></a><span class="in"># for i in range(len(range_MI)):</span></span>
<span id="cb27-587"><a href="#cb27-587" aria-hidden="true" tabindex="-1"></a><span class="in">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='lightgrey')</span></span>
<span id="cb27-588"><a href="#cb27-588" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, upper_MI, '-', color='lightgrey')</span></span>
<span id="cb27-589"><a href="#cb27-589" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, lower_MI, '-', color='lightgrey')</span></span>
<span id="cb27-590"><a href="#cb27-590" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)</span></span>
<span id="cb27-591"><a href="#cb27-591" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"Epochs")</span></span>
<span id="cb27-592"><a href="#cb27-592" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel("Generalized mutual information")</span></span>
<span id="cb27-593"><a href="#cb27-593" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-594"><a href="#cb27-594" aria-hidden="true" tabindex="-1"></a><span class="in">#plt.yscale('log')</span></span>
<span id="cb27-595"><a href="#cb27-595" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-596"><a href="#cb27-596" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-597"><a href="#cb27-597" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-598"><a href="#cb27-598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-599"><a href="#cb27-599" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-600"><a href="#cb27-600" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-multi_prior</span></span>
<span id="cb27-601"><a href="#cb27-601" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Histograms of the fitted prior and the Jeffreys prior Dir$(\\frac{1}{2},\\frac{1}{2},\\frac{1}{2},\\frac{1}{2})$ for each dimension of $\\theta$, each one is obtained from $10^5$ samples."</span></span>
<span id="cb27-602"><a href="#cb27-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-603"><a href="#cb27-603" aria-hidden="true" tabindex="-1"></a><span class="in">fig, axs = plt.subplots(1, q, figsize=(15, 4))  </span></span>
<span id="cb27-604"><a href="#cb27-604" aria-hidden="true" tabindex="-1"></a><span class="in">for i in range(q):</span></span>
<span id="cb27-605"><a href="#cb27-605" aria-hidden="true" tabindex="-1"></a><span class="in">    #axs[i].hist(theta_sample_init[:, i], density=True, bins="rice", color="red", label=r"Initial prior", alpha=0.4)</span></span>
<span id="cb27-606"><a href="#cb27-606" aria-hidden="true" tabindex="-1"></a><span class="in">    #axs[i].hist(theta_sample[:,i], density=True, bins="rice", label=r"Fitted prior", alpha=0.8, color='red', linewidth=1.2)</span></span>
<span id="cb27-607"><a href="#cb27-607" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].hist(theta_sample[:,i], density=True, bins="rice", label=r"Fitted prior", alpha=0.4)</span></span>
<span id="cb27-608"><a href="#cb27-608" aria-hidden="true" tabindex="-1"></a><span class="in">    #axs[i].hist(jeffreys_sample[:,i], density=True, bins="rice", label="Jeffreys", alpha=0.6, color='blue', linewidth=1.2)</span></span>
<span id="cb27-609"><a href="#cb27-609" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].hist(jeffreys_sample[:,i], density=True, bins="rice", label="Jeffreys", alpha=0.4, color=vertCEA)</span></span>
<span id="cb27-610"><a href="#cb27-610" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].set_xlabel(r"$\theta_{}$".format(i+1))  </span></span>
<span id="cb27-611"><a href="#cb27-611" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].grid()</span></span>
<span id="cb27-612"><a href="#cb27-612" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].legend(fontsize=16)</span></span>
<span id="cb27-613"><a href="#cb27-613" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-614"><a href="#cb27-614" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-615"><a href="#cb27-615" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-616"><a href="#cb27-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-617"><a href="#cb27-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-618"><a href="#cb27-618" aria-hidden="true" tabindex="-1"></a>For the posterior distribution, we sample 10 times from the Multinomial distribution using $\theta_{true} = (\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$. The covariance matrix in the proposition distribution of the Metropolis-Hastings algorithm is not diagonal, since we have a relation between the different components of $\theta$, we introduce non-zero covariances. We also verified that the auto-correlation between the successive remaining samples of the Markov chain decreases rapidly on each component.</span>
<span id="cb27-619"><a href="#cb27-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-620"><a href="#cb27-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-621"><a href="#cb27-621" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-622"><a href="#cb27-622" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-multi_post</span></span>
<span id="cb27-623"><a href="#cb27-623" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Histograms of the fitted posterior and the Jeffreys posterior for each dimension of $\\theta$, each one is obtained from $5\\cdot10^4$ samples."</span></span>
<span id="cb27-624"><a href="#cb27-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-625"><a href="#cb27-625" aria-hidden="true" tabindex="-1"></a><span class="in">fig, axs = plt.subplots(1, q, figsize=(15, 4))</span></span>
<span id="cb27-626"><a href="#cb27-626" aria-hidden="true" tabindex="-1"></a><span class="in">for i in range(q): </span></span>
<span id="cb27-627"><a href="#cb27-627" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].hist(jeffreys_post[:,i], density=True, bins="rice", label=r"Fitted post", alpha=0.4)</span></span>
<span id="cb27-628"><a href="#cb27-628" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].hist(theta_post[:,i], density=True, bins="rice", label=r"Jeffreys", alpha=0.4,color=vertCEA)</span></span>
<span id="cb27-629"><a href="#cb27-629" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].grid()</span></span>
<span id="cb27-630"><a href="#cb27-630" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].legend(fontsize=12)</span></span>
<span id="cb27-631"><a href="#cb27-631" aria-hidden="true" tabindex="-1"></a><span class="in">    axs[i].set_xlabel(r"$\theta_{}$".format(i+1))</span></span>
<span id="cb27-632"><a href="#cb27-632" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-633"><a href="#cb27-633" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-634"><a href="#cb27-634" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-635"><a href="#cb27-635" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-636"><a href="#cb27-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-637"><a href="#cb27-637" aria-hidden="true" tabindex="-1"></a>We notice (@fig-multi_mi) that the mutual information lies between $0$ and $1/\alpha(1-\alpha) = 4$, which is coherent with the theory, the confidence interval is rather large, but the mean value has an increasing trend.</span>
<span id="cb27-638"><a href="#cb27-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-639"><a href="#cb27-639" aria-hidden="true" tabindex="-1"></a>Although the shape of the fitted prior resembles the one of the Jeffreys prior, one can notice that it tends to put more weight towards the extremities of the interval (@fig-multi_prior). The posterior distribution however is quite similar to the target Jeffreys posterior on every component (@fig-multi_post).</span>
<span id="cb27-640"><a href="#cb27-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-641"><a href="#cb27-641" aria-hidden="true" tabindex="-1"></a>Since the multinomial model is simple and computationally practical, we would like to quantify the effect of the divergence with different $\alpha$ values on the output of the algorithm. In order to do so, we utilize the maximum mean discrepancy (MMD) defined as : </span>
<span id="cb27-642"><a href="#cb27-642" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-643"><a href="#cb27-643" aria-hidden="true" tabindex="-1"></a>   \text{MMD}(p,q) = || \mu_{p} - \mu_{q} ||_{\mathcal{H}},  </span>
<span id="cb27-644"><a href="#cb27-644" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-645"><a href="#cb27-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-646"><a href="#cb27-646" aria-hidden="true" tabindex="-1"></a>where $\mu_{p}$ and $\mu_{q}$ are respectively the kernel mean embeddings of distributions $p$ and $q$ in a reproducible kernel Hilbert space (RKHS) $(\mathcal{H}, || \cdot ||_{\mathcal{H}})$, meaning : $\mu_p(\theta') = \mathbb{E}_{\theta \sim p }<span class="co">[</span><span class="ot">K(\theta,\theta')</span><span class="co">]</span>$ for all $\theta' \in \Theta$ and $K$ being the kernel. The MMD is used for instance in the context of two-sample tests @gretton_mmd, whose purpose is to compare distributions. We use in our computations the Gaussian or RBF kernel : </span>
<span id="cb27-647"><a href="#cb27-647" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-648"><a href="#cb27-648" aria-hidden="true" tabindex="-1"></a>  K(\theta, \theta') = \exp(-0.5 \cdot ||\theta - \theta'||_2^2), </span>
<span id="cb27-649"><a href="#cb27-649" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-650"><a href="#cb27-650" aria-hidden="true" tabindex="-1"></a>for which the MMD is a metric, this means that the following implication: </span>
<span id="cb27-651"><a href="#cb27-651" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-652"><a href="#cb27-652" aria-hidden="true" tabindex="-1"></a> \text{MMD}(p,q) = 0 \implies p = q </span>
<span id="cb27-653"><a href="#cb27-653" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-654"><a href="#cb27-654" aria-hidden="true" tabindex="-1"></a>is verified with the other axioms. In practice, we consider an unbiased estimator of the MMD$^2$ given by: </span>
<span id="cb27-655"><a href="#cb27-655" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-656"><a href="#cb27-656" aria-hidden="true" tabindex="-1"></a> \widehat{\text{MMD}^2}(p,q) =  \frac{1}{m(m-1)} \sum_{i \neq j}K(x_i,x_j) +  \frac{1}{n(n-1)} \sum_{i \neq j}K(y_i,y_j) - \frac{2}{mn} \sum_{i,j} K(x_i,y_j), </span>
<span id="cb27-657"><a href="#cb27-657" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-658"><a href="#cb27-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-659"><a href="#cb27-659" aria-hidden="true" tabindex="-1"></a>where $(x_1,...,x_m)$ and $(y_1,...,y_n)$ are samples from $p$ and $q$ respectively. In our case, $p$ is the distribution obtained through variational inference and $q$ is the target Jeffreys distribution. Since the MMD can be time-consuming or memory inefficient to compute in practice for very large samples, we consider only the last $2 \cdot 10^4$ entries of our priors and posterior samples.</span>
<span id="cb27-660"><a href="#cb27-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-661"><a href="#cb27-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-662"><a href="#cb27-662" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-663"><a href="#cb27-663" aria-hidden="true" tabindex="-1"></a><span class="in">compute_autocorr = False</span></span>
<span id="cb27-664"><a href="#cb27-664" aria-hidden="true" tabindex="-1"></a><span class="in">if compute_autocorr :</span></span>
<span id="cb27-665"><a href="#cb27-665" aria-hidden="true" tabindex="-1"></a><span class="in">    def autocorrelation(x, lag):</span></span>
<span id="cb27-666"><a href="#cb27-666" aria-hidden="true" tabindex="-1"></a><span class="in">        """Compute the autocorrelation of a 1D array at a given lag."""</span></span>
<span id="cb27-667"><a href="#cb27-667" aria-hidden="true" tabindex="-1"></a><span class="in">        x = np.asarray(x)</span></span>
<span id="cb27-668"><a href="#cb27-668" aria-hidden="true" tabindex="-1"></a><span class="in">        n = len(x)</span></span>
<span id="cb27-669"><a href="#cb27-669" aria-hidden="true" tabindex="-1"></a><span class="in">        x_mean = np.mean(x)</span></span>
<span id="cb27-670"><a href="#cb27-670" aria-hidden="true" tabindex="-1"></a><span class="in">        x_var = np.var(x)</span></span>
<span id="cb27-671"><a href="#cb27-671" aria-hidden="true" tabindex="-1"></a><span class="in">        if x_var == 0:</span></span>
<span id="cb27-672"><a href="#cb27-672" aria-hidden="true" tabindex="-1"></a><span class="in">            return 0 </span></span>
<span id="cb27-673"><a href="#cb27-673" aria-hidden="true" tabindex="-1"></a><span class="in">        return np.correlate(x - x_mean, x - x_mean, mode="full")[n - 1 + lag] / (n * x_var)</span></span>
<span id="cb27-674"><a href="#cb27-674" aria-hidden="true" tabindex="-1"></a><span class="in">    lags = 100 </span></span>
<span id="cb27-675"><a href="#cb27-675" aria-hidden="true" tabindex="-1"></a><span class="in">    autocorrs = {i: [autocorrelation(theta_post[:, i], lag) for lag in range(lags + 1)] for i in range(theta_post.shape[1])}</span></span>
<span id="cb27-676"><a href="#cb27-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-677"><a href="#cb27-677" aria-hidden="true" tabindex="-1"></a><span class="in">    for i, ac in autocorrs.items():</span></span>
<span id="cb27-678"><a href="#cb27-678" aria-hidden="true" tabindex="-1"></a><span class="in">        plt.plot(ac, label=f'Component {i+1}')</span></span>
<span id="cb27-679"><a href="#cb27-679" aria-hidden="true" tabindex="-1"></a><span class="in">    plt.legend()</span></span>
<span id="cb27-680"><a href="#cb27-680" aria-hidden="true" tabindex="-1"></a><span class="in">    plt.xlabel('Lag')</span></span>
<span id="cb27-681"><a href="#cb27-681" aria-hidden="true" tabindex="-1"></a><span class="in">    plt.ylabel('Autocorrelation')</span></span>
<span id="cb27-682"><a href="#cb27-682" aria-hidden="true" tabindex="-1"></a><span class="in">    plt.grid()</span></span>
<span id="cb27-683"><a href="#cb27-683" aria-hidden="true" tabindex="-1"></a><span class="in">    plt.show()</span></span>
<span id="cb27-684"><a href="#cb27-684" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-685"><a href="#cb27-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-686"><a href="#cb27-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-687"><a href="#cb27-687" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-688"><a href="#cb27-688" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Multinomial_MMD.pkl'</span></span>
<span id="cb27-689"><a href="#cb27-689" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-690"><a href="#cb27-690" aria-hidden="true" tabindex="-1"></a><span class="in">alphas = np.array([0.1, 0.25, 0.5, 0.75, 0.9])</span></span>
<span id="cb27-691"><a href="#cb27-691" aria-hidden="true" tabindex="-1"></a><span class="in">len_alpha = len(alphas)</span></span>
<span id="cb27-692"><a href="#cb27-692" aria-hidden="true" tabindex="-1"></a><span class="in">nb_samples_mmd = 2*10**4</span></span>
<span id="cb27-693"><a href="#cb27-693" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_MultiMMD :</span></span>
<span id="cb27-694"><a href="#cb27-694" aria-hidden="true" tabindex="-1"></a><span class="in">    MMDvalues = np.zeros((len_alpha,2))</span></span>
<span id="cb27-695"><a href="#cb27-695" aria-hidden="true" tabindex="-1"></a><span class="in">    for index_alpha in range(len_alpha):</span></span>
<span id="cb27-696"><a href="#cb27-696" aria-hidden="true" tabindex="-1"></a><span class="in">        alpha = alphas[index_alpha]</span></span>
<span id="cb27-697"><a href="#cb27-697" aria-hidden="true" tabindex="-1"></a><span class="in">        print(f'alpha value = {alpha}')</span></span>
<span id="cb27-698"><a href="#cb27-698" aria-hidden="true" tabindex="-1"></a><span class="in">        # Parameters and classes</span></span>
<span id="cb27-699"><a href="#cb27-699" aria-hidden="true" tabindex="-1"></a><span class="in">        p = 50     # latent space dimension</span></span>
<span id="cb27-700"><a href="#cb27-700" aria-hidden="true" tabindex="-1"></a><span class="in">        q = 4      # parameter space dimension</span></span>
<span id="cb27-701"><a href="#cb27-701" aria-hidden="true" tabindex="-1"></a><span class="in">        n = 10     # multinomial parameter</span></span>
<span id="cb27-702"><a href="#cb27-702" aria-hidden="true" tabindex="-1"></a><span class="in">        N = 10     # number of data samples</span></span>
<span id="cb27-703"><a href="#cb27-703" aria-hidden="true" tabindex="-1"></a><span class="in">        J = 1000   # nb samples for MC estimation in MI</span></span>
<span id="cb27-704"><a href="#cb27-704" aria-hidden="true" tabindex="-1"></a><span class="in">        T = 50     # nb samples MC marginal likelihood</span></span>
<span id="cb27-705"><a href="#cb27-705" aria-hidden="true" tabindex="-1"></a><span class="in">        Multinom = torch_MultinomialModel(n=n, q=q)  </span></span>
<span id="cb27-706"><a href="#cb27-706" aria-hidden="true" tabindex="-1"></a><span class="in">        input_size = p  </span></span>
<span id="cb27-707"><a href="#cb27-707" aria-hidden="true" tabindex="-1"></a><span class="in">        output_size = q</span></span>
<span id="cb27-708"><a href="#cb27-708" aria-hidden="true" tabindex="-1"></a><span class="in">        n_samples_prior = 10**5</span></span>
<span id="cb27-709"><a href="#cb27-709" aria-hidden="true" tabindex="-1"></a><span class="in">        low = 0.001           # lower bound (must verifiy q * low &lt; 1)</span></span>
<span id="cb27-710"><a href="#cb27-710" aria-hidden="true" tabindex="-1"></a><span class="in">        upp = 1 - low*(q-1)  # upper bound (forced value with low and q)</span></span>
<span id="cb27-711"><a href="#cb27-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-712"><a href="#cb27-712" aria-hidden="true" tabindex="-1"></a><span class="in">        ##### Prior #####</span></span>
<span id="cb27-713"><a href="#cb27-713" aria-hidden="true" tabindex="-1"></a><span class="in">        seed_all(0)</span></span>
<span id="cb27-714"><a href="#cb27-714" aria-hidden="true" tabindex="-1"></a><span class="in">        NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Softmax(dim=-1))</span></span>
<span id="cb27-715"><a href="#cb27-715" aria-hidden="true" tabindex="-1"></a><span class="in">        NN = AffineTransformation(NN, m_low=low, M_upp=upp)</span></span>
<span id="cb27-716"><a href="#cb27-716" aria-hidden="true" tabindex="-1"></a><span class="in">        VA = VA_NeuralNet(neural_net=NN, model=Multinom)</span></span>
<span id="cb27-717"><a href="#cb27-717" aria-hidden="true" tabindex="-1"></a><span class="in">        #Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb27-718"><a href="#cb27-718" aria-hidden="true" tabindex="-1"></a><span class="in">        Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=alpha, use_log_lik=True, use_baseline=False)</span></span>
<span id="cb27-719"><a href="#cb27-719" aria-hidden="true" tabindex="-1"></a><span class="in">        with torch.no_grad():</span></span>
<span id="cb27-720"><a href="#cb27-720" aria-hidden="true" tabindex="-1"></a><span class="in">            theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-721"><a href="#cb27-721" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-722"><a href="#cb27-722" aria-hidden="true" tabindex="-1"></a><span class="in">        num_epochs = 10000</span></span>
<span id="cb27-723"><a href="#cb27-723" aria-hidden="true" tabindex="-1"></a><span class="in">        loss_fct = "LB_MI"</span></span>
<span id="cb27-724"><a href="#cb27-724" aria-hidden="true" tabindex="-1"></a><span class="in">        optimizer = torch_Adam</span></span>
<span id="cb27-725"><a href="#cb27-725" aria-hidden="true" tabindex="-1"></a><span class="in">        num_samples_MI = 200</span></span>
<span id="cb27-726"><a href="#cb27-726" aria-hidden="true" tabindex="-1"></a><span class="in">        freq_MI = 200  </span></span>
<span id="cb27-727"><a href="#cb27-727" aria-hidden="true" tabindex="-1"></a><span class="in">        save_best_param = True</span></span>
<span id="cb27-728"><a href="#cb27-728" aria-hidden="true" tabindex="-1"></a><span class="in">        learning_rate = 0.0025</span></span>
<span id="cb27-729"><a href="#cb27-729" aria-hidden="true" tabindex="-1"></a><span class="in">        ### Training loop</span></span>
<span id="cb27-730"><a href="#cb27-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-731"><a href="#cb27-731" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI = Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span></span>
<span id="cb27-732"><a href="#cb27-732" aria-hidden="true" tabindex="-1"></a><span class="in">                                                                num_samples_MI, freq_MI, save_best_param, learning_rate,</span></span>
<span id="cb27-733"><a href="#cb27-733" aria-hidden="true" tabindex="-1"></a><span class="in">                                                                momentum=True)</span></span>
<span id="cb27-734"><a href="#cb27-734" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb27-735"><a href="#cb27-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-736"><a href="#cb27-736" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample_torch = Div.va.implicit_prior_sampler(n_samples_prior)</span></span>
<span id="cb27-737"><a href="#cb27-737" aria-hidden="true" tabindex="-1"></a><span class="in">        with torch.no_grad():</span></span>
<span id="cb27-738"><a href="#cb27-738" aria-hidden="true" tabindex="-1"></a><span class="in">            theta_sample = theta_sample_torch.numpy()</span></span>
<span id="cb27-739"><a href="#cb27-739" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_sample = Div.model.sample_Jeffreys(n_samples=n_samples_prior)</span></span>
<span id="cb27-740"><a href="#cb27-740" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb27-741"><a href="#cb27-741" aria-hidden="true" tabindex="-1"></a><span class="in">        #MMDvalues[index_alpha, 0] = np.sqrt(compute_MMD2(theta_sample, jeffreys_sample))</span></span>
<span id="cb27-742"><a href="#cb27-742" aria-hidden="true" tabindex="-1"></a><span class="in">        MMDvalues[index_alpha, 0] = np.sqrt(MMD2_rbf(theta_sample[-nb_samples_mmd:,:], jeffreys_sample[-nb_samples_mmd:,:], gamma=0.5))</span></span>
<span id="cb27-743"><a href="#cb27-743" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-744"><a href="#cb27-744" aria-hidden="true" tabindex="-1"></a><span class="in">        ##### Posterior ####</span></span>
<span id="cb27-745"><a href="#cb27-745" aria-hidden="true" tabindex="-1"></a><span class="in">        seed_all(0)</span></span>
<span id="cb27-746"><a href="#cb27-746" aria-hidden="true" tabindex="-1"></a><span class="in">        # Sample data from 'true' parameter</span></span>
<span id="cb27-747"><a href="#cb27-747" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_true = torch.tensor(q*[1/q])  </span></span>
<span id="cb27-748"><a href="#cb27-748" aria-hidden="true" tabindex="-1"></a><span class="in">        N = 10</span></span>
<span id="cb27-749"><a href="#cb27-749" aria-hidden="true" tabindex="-1"></a><span class="in">        D = Multinom.sample(theta_true, N, 1)</span></span>
<span id="cb27-750"><a href="#cb27-750" aria-hidden="true" tabindex="-1"></a><span class="in">        X = D[:, 0]</span></span>
<span id="cb27-751"><a href="#cb27-751" aria-hidden="true" tabindex="-1"></a><span class="in">        # Posterior samples using Jeffreys prior</span></span>
<span id="cb27-752"><a href="#cb27-752" aria-hidden="true" tabindex="-1"></a><span class="in">        n_samples_post = 5*10**4</span></span>
<span id="cb27-753"><a href="#cb27-753" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_post = Multinom.sample_post_Jeffreys(X, n_samples_post)</span></span>
<span id="cb27-754"><a href="#cb27-754" aria-hidden="true" tabindex="-1"></a><span class="in">        T_mcmc = 10**5 + 1</span></span>
<span id="cb27-755"><a href="#cb27-755" aria-hidden="true" tabindex="-1"></a><span class="in">        sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-756"><a href="#cb27-756" aria-hidden="true" tabindex="-1"></a><span class="in">        eps_0 = torch.randn(p)</span></span>
<span id="cb27-757"><a href="#cb27-757" aria-hidden="true" tabindex="-1"></a><span class="in">        eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap=True, Cov=True)</span></span>
<span id="cb27-758"><a href="#cb27-758" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = NN(eps_MH)</span></span>
<span id="cb27-759"><a href="#cb27-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-760"><a href="#cb27-760" aria-hidden="true" tabindex="-1"></a><span class="in">        with torch.no_grad():</span></span>
<span id="cb27-761"><a href="#cb27-761" aria-hidden="true" tabindex="-1"></a><span class="in">            theta_MH = theta_MH.numpy()</span></span>
<span id="cb27-762"><a href="#cb27-762" aria-hidden="true" tabindex="-1"></a><span class="in">            jeffreys_post = jeffreys_post.numpy()</span></span>
<span id="cb27-763"><a href="#cb27-763" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post = theta_MH[-n_samples_post:,:]</span></span>
<span id="cb27-764"><a href="#cb27-764" aria-hidden="true" tabindex="-1"></a><span class="in">        </span></span>
<span id="cb27-765"><a href="#cb27-765" aria-hidden="true" tabindex="-1"></a><span class="in">        #MMDvalues[index_alpha, 1] = np.sqrt(compute_MMD2(theta_post, jeffreys_post))</span></span>
<span id="cb27-766"><a href="#cb27-766" aria-hidden="true" tabindex="-1"></a><span class="in">        MMDvalues[index_alpha, 1] = np.sqrt(MMD2_rbf(theta_post[-nb_samples_mmd:,:], jeffreys_post[-nb_samples_mmd:,:], gamma=0.5))</span></span>
<span id="cb27-767"><a href="#cb27-767" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-768"><a href="#cb27-768" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(MMDvalues, file)</span></span>
<span id="cb27-769"><a href="#cb27-769" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-770"><a href="#cb27-770" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-771"><a href="#cb27-771" aria-hidden="true" tabindex="-1"></a><span class="in">        MMDvalues = pickle.load(file)</span></span>
<span id="cb27-772"><a href="#cb27-772" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-773"><a href="#cb27-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-774"><a href="#cb27-774" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-775"><a href="#cb27-775" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-mmd_multinom</span></span>
<span id="cb27-776"><a href="#cb27-776" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "MMD values for different $\\alpha$-divergences at prior and posterior levels. As a reference on the prior level, when computing the criterion between two independent Dirichlet Dir$(\\frac{1}{2},\\frac{1}{2},\\frac{1}{2},\\frac{1}{2})$ distributions (ie the Jeffreys prior) on $2 \\cdot 10^4$ samples, we obtain an order of magnitude of $10^{-3}$."</span></span>
<span id="cb27-777"><a href="#cb27-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-778"><a href="#cb27-778" aria-hidden="true" tabindex="-1"></a><span class="in">data = np.column_stack((alphas, MMDvalues))</span></span>
<span id="cb27-779"><a href="#cb27-779" aria-hidden="true" tabindex="-1"></a><span class="in">#headers = ["α", "Prior", "Posterior"]</span></span>
<span id="cb27-780"><a href="#cb27-780" aria-hidden="true" tabindex="-1"></a><span class="in">headers = [r"$\alpha$", "Prior", "Posterior"]</span></span>
<span id="cb27-781"><a href="#cb27-781" aria-hidden="true" tabindex="-1"></a><span class="in">def format_scientific(value, decimals=2):</span></span>
<span id="cb27-782"><a href="#cb27-782" aria-hidden="true" tabindex="-1"></a><span class="in">    """Format a float in LaTeX-style scientific notation."""</span></span>
<span id="cb27-783"><a href="#cb27-783" aria-hidden="true" tabindex="-1"></a><span class="in">    formatted = f"{value:.{decimals}e}"  </span></span>
<span id="cb27-784"><a href="#cb27-784" aria-hidden="true" tabindex="-1"></a><span class="in">    base, exp = formatted.split("e")  </span></span>
<span id="cb27-785"><a href="#cb27-785" aria-hidden="true" tabindex="-1"></a><span class="in">    return f"${base} \\times 10^{{{int(exp)}}}$"</span></span>
<span id="cb27-786"><a href="#cb27-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-787"><a href="#cb27-787" aria-hidden="true" tabindex="-1"></a><span class="in">formatted_data = [</span></span>
<span id="cb27-788"><a href="#cb27-788" aria-hidden="true" tabindex="-1"></a><span class="in">    [f"${row[0]:.2f}$", format_scientific(row[1]), format_scientific(row[2])]</span></span>
<span id="cb27-789"><a href="#cb27-789" aria-hidden="true" tabindex="-1"></a><span class="in">    for row in data</span></span>
<span id="cb27-790"><a href="#cb27-790" aria-hidden="true" tabindex="-1"></a><span class="in">]</span></span>
<span id="cb27-791"><a href="#cb27-791" aria-hidden="true" tabindex="-1"></a><span class="in"># formatted_data = [</span></span>
<span id="cb27-792"><a href="#cb27-792" aria-hidden="true" tabindex="-1"></a><span class="in">#     [f"{row[0]:.2f}", f"{row[1]:.2e}", f"{row[2]:.3e}"] for row in data</span></span>
<span id="cb27-793"><a href="#cb27-793" aria-hidden="true" tabindex="-1"></a><span class="in"># ]</span></span>
<span id="cb27-794"><a href="#cb27-794" aria-hidden="true" tabindex="-1"></a><span class="in">table_data = [headers] + formatted_data</span></span>
<span id="cb27-795"><a href="#cb27-795" aria-hidden="true" tabindex="-1"></a><span class="in">fig, ax = plt.subplots()</span></span>
<span id="cb27-796"><a href="#cb27-796" aria-hidden="true" tabindex="-1"></a><span class="in">ax.axis("off")  </span></span>
<span id="cb27-797"><a href="#cb27-797" aria-hidden="true" tabindex="-1"></a><span class="in">table = ax.table(</span></span>
<span id="cb27-798"><a href="#cb27-798" aria-hidden="true" tabindex="-1"></a><span class="in">    cellText=table_data,  </span></span>
<span id="cb27-799"><a href="#cb27-799" aria-hidden="true" tabindex="-1"></a><span class="in">    colLabels=None,      </span></span>
<span id="cb27-800"><a href="#cb27-800" aria-hidden="true" tabindex="-1"></a><span class="in">    loc="center",        </span></span>
<span id="cb27-801"><a href="#cb27-801" aria-hidden="true" tabindex="-1"></a><span class="in">    cellLoc="center",   </span></span>
<span id="cb27-802"><a href="#cb27-802" aria-hidden="true" tabindex="-1"></a><span class="in">)</span></span>
<span id="cb27-803"><a href="#cb27-803" aria-hidden="true" tabindex="-1"></a><span class="in">for (i, key) in enumerate(headers):</span></span>
<span id="cb27-804"><a href="#cb27-804" aria-hidden="true" tabindex="-1"></a><span class="in">    cell = table[0, i]  </span></span>
<span id="cb27-805"><a href="#cb27-805" aria-hidden="true" tabindex="-1"></a><span class="in">    cell.set_text_props(fontweight="bold")  </span></span>
<span id="cb27-806"><a href="#cb27-806" aria-hidden="true" tabindex="-1"></a><span class="in">    cell.set_facecolor("lightgray")  </span></span>
<span id="cb27-807"><a href="#cb27-807" aria-hidden="true" tabindex="-1"></a><span class="in">table.scale(1, 2.5)  </span></span>
<span id="cb27-808"><a href="#cb27-808" aria-hidden="true" tabindex="-1"></a><span class="in">table.auto_set_font_size(False)</span></span>
<span id="cb27-809"><a href="#cb27-809" aria-hidden="true" tabindex="-1"></a><span class="in">table.set_fontsize(12)</span></span>
<span id="cb27-810"><a href="#cb27-810" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-811"><a href="#cb27-811" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-812"><a href="#cb27-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-813"><a href="#cb27-813" aria-hidden="true" tabindex="-1"></a>According to @fig-mmd_multinom, the difference between $\alpha$ values in terms of the MMD criterion is essentially inconsequential. One remark is that the mutual information tends to be more unstable as $\alpha$ gets closer to $1$. The explanation is that when $\alpha$ tends to $1$, we have the approximation :</span>
<span id="cb27-814"><a href="#cb27-814" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-815"><a href="#cb27-815" aria-hidden="true" tabindex="-1"></a>\hat{f}_{\alpha}(x) \approx \frac{x-1}{\alpha(\alpha-1)} + \frac{x\log(x)}{\alpha},    </span>
<span id="cb27-816"><a href="#cb27-816" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-817"><a href="#cb27-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-818"><a href="#cb27-818" aria-hidden="true" tabindex="-1"></a>which diverges for all $x$ because of the first term. Hence, we advise the user to avoid $\alpha$ values that are too close to $1$. In the following, we use $\alpha = 0.5$ for the divergence.</span>
<span id="cb27-819"><a href="#cb27-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-820"><a href="#cb27-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-821"><a href="#cb27-821" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-822"><a href="#cb27-822" aria-hidden="true" tabindex="-1"></a><span class="in">compute_mmd_ref_value = False</span></span>
<span id="cb27-823"><a href="#cb27-823" aria-hidden="true" tabindex="-1"></a><span class="in">if compute_mmd_ref_value : </span></span>
<span id="cb27-824"><a href="#cb27-824" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-825"><a href="#cb27-825" aria-hidden="true" tabindex="-1"></a><span class="in">    dir1 = Multinom.sample_Jeffreys(n_samples=nb_samples_mmd)</span></span>
<span id="cb27-826"><a href="#cb27-826" aria-hidden="true" tabindex="-1"></a><span class="in">    dir2 = Multinom.sample_Jeffreys(n_samples=nb_samples_mmd)</span></span>
<span id="cb27-827"><a href="#cb27-827" aria-hidden="true" tabindex="-1"></a><span class="in">    print(f'MMD reference value : {np.sqrt(MMD2_rbf(dir1, dir2, gamma=0.5))}')</span></span>
<span id="cb27-828"><a href="#cb27-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-829"><a href="#cb27-829" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-830"><a href="#cb27-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-831"><a href="#cb27-831" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probit model  {#sec-probit_model}</span></span>
<span id="cb27-832"><a href="#cb27-832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-833"><a href="#cb27-833" aria-hidden="true" tabindex="-1"></a>We present in this section the probit model used to estimate seismic fragility curves, which was introduced by @kennedy1980, it is also referred as the log-normal model in the literature. A seismic fragility curve is the probability of failure $P_f(a)$ of a mechanical</span>
<span id="cb27-834"><a href="#cb27-834" aria-hidden="true" tabindex="-1"></a>structure subjected to a seism as a function of a scalar value $a$ derived from the seismic ground motion. The properties of the Jeffreys prior for this model are highlighted by @van2024reference.</span>
<span id="cb27-835"><a href="#cb27-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-836"><a href="#cb27-836" aria-hidden="true" tabindex="-1"></a>The model is defined by the observation of an i.i.d. sample $\bX=(X_1,\dots,X_N)$ where for any $i$, $X_i\sim(Z,a)\in\mathcal{X}=<span class="sc">\{</span>0,1<span class="sc">\}</span>\times(0,\infty)$. The distribution of the r.v. $(Z,a)$ is parameterized by $\theta=(\theta_1,\theta_2)\in(0,\infty)^2$ as:</span>
<span id="cb27-837"><a href="#cb27-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-838"><a href="#cb27-838" aria-hidden="true" tabindex="-1"></a>$$    </span>
<span id="cb27-839"><a href="#cb27-839" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb27-840"><a href="#cb27-840" aria-hidden="true" tabindex="-1"></a>   \displaystyle a \sim \text{Log}\text{-}\mathcal{N}(\mu_a, \sigma^2_a) <span class="sc">\\</span></span>
<span id="cb27-841"><a href="#cb27-841" aria-hidden="true" tabindex="-1"></a>    P_f(a) = \displaystyle \Phi \left( \frac{\log a - \log \theta_1}{\theta_2} \right) <span class="sc">\\</span></span>
<span id="cb27-842"><a href="#cb27-842" aria-hidden="true" tabindex="-1"></a>    Z \sim \text{Bernoulli}(P_f(a)),</span>
<span id="cb27-843"><a href="#cb27-843" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-844"><a href="#cb27-844" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-845"><a href="#cb27-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-846"><a href="#cb27-846" aria-hidden="true" tabindex="-1"></a>where $\Phi$ is the cumulative distribution function of the standard Gaussian. The probit function is the inverse of $\Phi$. The likelihood is of the form : </span>
<span id="cb27-847"><a href="#cb27-847" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-848"><a href="#cb27-848" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb27-849"><a href="#cb27-849" aria-hidden="true" tabindex="-1"></a>    L_N(\bX \, | \, \theta) = \displaystyle \prod_{i=1}^N p(a_i) \prod_{i=1}^N P_f(a_i)^{Z_i} (1-P_f(a_i))^{1-Z_i} \propto \prod_{i=1}^N  P_f(a_i)^{Z_i} (1-P_f(a_i))^{1-Z_i} <span class="sc">\\</span></span>
<span id="cb27-850"><a href="#cb27-850" aria-hidden="true" tabindex="-1"></a>    p(a_i) = \displaystyle \frac{1}{a_i \sqrt{2\pi \sigma^2_a}} \exp \left( - \frac{1}{2\sigma_a^2}(\log a_i - \mu_a)^2 \right).</span>
<span id="cb27-851"><a href="#cb27-851" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-852"><a href="#cb27-852" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-853"><a href="#cb27-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-854"><a href="#cb27-854" aria-hidden="true" tabindex="-1"></a>For simplicity, we denote : $\gamma_i = \displaystyle  \frac{\log a_i - \log \theta_1}{\theta_2} = \Phi^{-1}(P_f(a_i)) = \text{probit}(P_f(a_i))$, the gradient of the log-likelihood is the following : </span>
<span id="cb27-855"><a href="#cb27-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-856"><a href="#cb27-856" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-857"><a href="#cb27-857" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb27-858"><a href="#cb27-858" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\partial \log L_N}{\partial \theta_1}(\bX \,|\,\theta)  = \sum_{i=1}^N  \frac{1}{\theta_1 \theta_2} \left( (-Z_i)\frac{\Phi'(\gamma_i)}{\Phi(\gamma_i)} + (1-Z_i)\frac{\Phi'(\gamma_i)}{1-\Phi(\gamma_i)}  \right) <span class="sc">\\</span></span>
<span id="cb27-859"><a href="#cb27-859" aria-hidden="true" tabindex="-1"></a>\displaystyle \frac{\partial \log L_N}{\partial \theta_2}(\bX\,|\,\theta)  =   \sum_{i=1}^N \frac{\gamma_i}{\theta_2} \left( (-Z_i)\frac{\Phi'(\gamma_i)}{\Phi(\gamma_i)} + (1-Z_i)\frac{\Phi'(\gamma_i)}{1-\Phi(\gamma_i)}  \right). </span>
<span id="cb27-860"><a href="#cb27-860" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-861"><a href="#cb27-861" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-862"><a href="#cb27-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-863"><a href="#cb27-863" aria-hidden="true" tabindex="-1"></a>There is no explicit formula for the MLE, so it has to be approximated using samples. This statistical model is a more difficult case than the previous one, since no explicit formula for the Jeffreys prior is available either but it has been shown by @van2024reference that it is improper in $\theta_2$ and some asymptotic rates where derived. More precisely, when $\theta_1 &gt; 0$ is fixed,</span>
<span id="cb27-864"><a href="#cb27-864" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-865"><a href="#cb27-865" aria-hidden="true" tabindex="-1"></a>\begin{cases} \displaystyle</span>
<span id="cb27-866"><a href="#cb27-866" aria-hidden="true" tabindex="-1"></a>J(\theta) \propto 1/\theta_2 \quad \text{as} \quad  \theta_2 \longrightarrow 0 <span class="sc">\\</span></span>
<span id="cb27-867"><a href="#cb27-867" aria-hidden="true" tabindex="-1"></a>J(\theta) \propto 1/\theta_2^3 \quad \text{as} \quad  \theta_2 \longrightarrow +\infty.</span>
<span id="cb27-868"><a href="#cb27-868" aria-hidden="true" tabindex="-1"></a>\end{cases} </span>
<span id="cb27-869"><a href="#cb27-869" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-870"><a href="#cb27-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-871"><a href="#cb27-871" aria-hidden="true" tabindex="-1"></a>If we fix $\theta_2 &gt; 0$, the prior is proper for the variable $\theta_1$ :</span>
<span id="cb27-872"><a href="#cb27-872" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-873"><a href="#cb27-873" aria-hidden="true" tabindex="-1"></a>  J(\theta) \propto \frac{|\log \theta_1|}{\theta_1} \exp \left( -\frac{(\log \theta_1 - \mu_a)^2}{2\theta_2 + 2\sigma_a^2} \right)  \quad \text{when} \quad  |\log \theta_1| \longrightarrow +\infty. </span>
<span id="cb27-874"><a href="#cb27-874" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb27-875"><a href="#cb27-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-876"><a href="#cb27-876" aria-hidden="true" tabindex="-1"></a>which resembles a log-normal distribution except for the $|\log \theta_1|$ factor. Since the density of the Jeffreys prior is not explicit and can not be computed directly, the Fisher information matrix is computed in @van2024reference using numerical integration with Simpson's rule on a specific grid and then an interpolation is applied. We use this computation as the reference to evaluate the quality of the output of our algorithm. In the mentioned article, the posterior distribution is also computed with an adaptive Metropolis-Hastings algorithm on the variable $\theta$, we refer to this algorithm as MH($\theta$) since it is different from the one mentioned in @sec-posterio-sampling. More details on MH($\theta$) are given in @gauchy_thesis. We take $\mu_a = 0$, $\sigma^2_a = 1$, $N=500$ and $U=500$ for the computation of the prior.</span>
<span id="cb27-877"><a href="#cb27-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-878"><a href="#cb27-878" aria-hidden="true" tabindex="-1"></a>As for the neural network, we use a one-layer network with an $\exp$ activation for $\theta_1$ and a Softplus activation for $\theta_2$. We have that : </span>
<span id="cb27-879"><a href="#cb27-879" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-880"><a href="#cb27-880" aria-hidden="true" tabindex="-1"></a> \theta = \begin{pmatrix}</span>
<span id="cb27-881"><a href="#cb27-881" aria-hidden="true" tabindex="-1"></a>           \theta_1 <span class="sc">\\</span></span>
<span id="cb27-882"><a href="#cb27-882" aria-hidden="true" tabindex="-1"></a>           \theta_2 <span class="sc">\\</span></span>
<span id="cb27-883"><a href="#cb27-883" aria-hidden="true" tabindex="-1"></a>         \end{pmatrix} = </span>
<span id="cb27-884"><a href="#cb27-884" aria-hidden="true" tabindex="-1"></a>\begin{pmatrix}</span>
<span id="cb27-885"><a href="#cb27-885" aria-hidden="true" tabindex="-1"></a>           \exp(w_1^{\top}\varepsilon + b_1) <span class="sc">\\</span></span>
<span id="cb27-886"><a href="#cb27-886" aria-hidden="true" tabindex="-1"></a>           \log\left(1 + \exp(w_2^{\top} \varepsilon + b_2)\right) <span class="sc">\\</span></span>
<span id="cb27-887"><a href="#cb27-887" aria-hidden="true" tabindex="-1"></a>         \end{pmatrix},  </span>
<span id="cb27-888"><a href="#cb27-888" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-889"><a href="#cb27-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-890"><a href="#cb27-890" aria-hidden="true" tabindex="-1"></a>with $w_1, w_2 \in \mathbb{R}^p$ the weight vectors and $b_1, b_2 \in \mathbb{R}$ the biases, thus we have $\lambda = (w_1, w_2, b_1,b_2)$. </span>
<span id="cb27-891"><a href="#cb27-891" aria-hidden="true" tabindex="-1"></a>Because this architecture remains simple, it is possible to elucidate the resulting marginal distributions of $\theta_1$ and $\theta_2$.</span>
<span id="cb27-892"><a href="#cb27-892" aria-hidden="true" tabindex="-1"></a>The first component $\theta_1$ follows a  $\text{Log-}\mathcal{N}(b_1, ||w_1||_2^2)$ distribution and $\theta_2$ has an explicit density function : </span>
<span id="cb27-893"><a href="#cb27-893" aria-hidden="true" tabindex="-1"></a>\begin{equation*} </span>
<span id="cb27-894"><a href="#cb27-894" aria-hidden="true" tabindex="-1"></a>    p(\theta_2) = \frac{1}{\sqrt{2\pi||w_2||_2^2}(1-e^{-\theta_2})} \exp \left(-\frac{1}{2||w_2||_2^2}\left(\log(e^{\theta_2}-1)-b_2 \right)^2 \right).</span>
<span id="cb27-895"><a href="#cb27-895" aria-hidden="true" tabindex="-1"></a>\end{equation*} </span>
<span id="cb27-896"><a href="#cb27-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-897"><a href="#cb27-897" aria-hidden="true" tabindex="-1"></a>These expressions describe the parameterized set $\mathcal{P}_\Lambda$ of priors considered in the optimization problem. This set is restrictive, so that the resulting VA-RP must be interpreted as the most objective ---according to the mutual information criterion--- prior among the ones in $\mathcal{P}_\Lambda$. Since we do not know any explicit expression of the Jeffreys prior for this prior, we cannot provide a precise comparison between the parameterized VA-RP elucidated above and the target.</span>
<span id="cb27-898"><a href="#cb27-898" aria-hidden="true" tabindex="-1"></a>However, the form of the distribution of $\theta_1$ qualitatively resembles its theoretical target.</span>
<span id="cb27-899"><a href="#cb27-899" aria-hidden="true" tabindex="-1"></a>In the case of $\theta_2$, the asymptotic decay rates of its density function can be derived:</span>
<span id="cb27-900"><a href="#cb27-900" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-901"><a href="#cb27-901" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb27-902"><a href="#cb27-902" aria-hidden="true" tabindex="-1"></a>    p(\theta_2) \aseq{\theta_2\rightarrow0} \frac{1}{\theta_2\sqrt{2\pi}\|w_2\|_2}\exp\left(-\frac{(\log\theta_2-b_2)^2}{2\|w_2\|_2^2}\right); <span class="sc">\\</span> </span>
<span id="cb27-903"><a href="#cb27-903" aria-hidden="true" tabindex="-1"></a>    p(\theta_2) \aseq{\theta_2\rightarrow\infty} \frac{1}{\sqrt{2\pi}\|w_2\|_2}\exp\left(-\frac{(\theta_2-b_2)^2}{2\|w_2\|_2^2} \right).</span>
<span id="cb27-904"><a href="#cb27-904" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb27-905"><a href="#cb27-905" aria-hidden="true" tabindex="-1"></a>$$ {#eq-decay_rates_ptheta2}</span>
<span id="cb27-906"><a href="#cb27-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-907"><a href="#cb27-907" aria-hidden="true" tabindex="-1"></a>While $\|w_2\|_2$ does not tend toward $\infty$, these decay rates strongly differ from the ones of the Jeffreys prior w.r.t. $\theta_2$. Otherwise, the decay rates resemble to something proportional to $(\theta_2+1)^{-1}$ in both directions. In our numerical computations, the optimization process yielded a VA-RP with parameters $w_2$ and $b_2$ that did not diverge to extreme values.</span>
<span id="cb27-908"><a href="#cb27-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-909"><a href="#cb27-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-910"><a href="#cb27-910" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-911"><a href="#cb27-911" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameters and classes</span></span>
<span id="cb27-912"><a href="#cb27-912" aria-hidden="true" tabindex="-1"></a><span class="in">p = 50   </span></span>
<span id="cb27-913"><a href="#cb27-913" aria-hidden="true" tabindex="-1"></a><span class="in">q = 2      </span></span>
<span id="cb27-914"><a href="#cb27-914" aria-hidden="true" tabindex="-1"></a><span class="in">N = 500     </span></span>
<span id="cb27-915"><a href="#cb27-915" aria-hidden="true" tabindex="-1"></a><span class="in">J = 500   </span></span>
<span id="cb27-916"><a href="#cb27-916" aria-hidden="true" tabindex="-1"></a><span class="in">T = 50     </span></span>
<span id="cb27-917"><a href="#cb27-917" aria-hidden="true" tabindex="-1"></a><span class="in">input_size = p  </span></span>
<span id="cb27-918"><a href="#cb27-918" aria-hidden="true" tabindex="-1"></a><span class="in">output_size = q</span></span>
<span id="cb27-919"><a href="#cb27-919" aria-hidden="true" tabindex="-1"></a><span class="in">low = 0.0001        </span></span>
<span id="cb27-920"><a href="#cb27-920" aria-hidden="true" tabindex="-1"></a><span class="in">upp = 1 + low</span></span>
<span id="cb27-921"><a href="#cb27-921" aria-hidden="true" tabindex="-1"></a><span class="in">mu_a, sigma2_a = 0, 1</span></span>
<span id="cb27-922"><a href="#cb27-922" aria-hidden="true" tabindex="-1"></a><span class="in">#mu_a, sigma2_a = 8.7 * 10**-3, 1.03</span></span>
<span id="cb27-923"><a href="#cb27-923" aria-hidden="true" tabindex="-1"></a><span class="in">Probit = torch_ProbitModel(use_log_normal=True, mu_a=mu_a, sigma2_a=sigma2_a, set_beta=None, alt_scaling=True)</span></span>
<span id="cb27-924"><a href="#cb27-924" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**6</span></span>
<span id="cb27-925"><a href="#cb27-925" aria-hidden="true" tabindex="-1"></a><span class="in">alpha = 0.5</span></span>
<span id="cb27-926"><a href="#cb27-926" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Probit_results_unconstrained.pkl'</span></span>
<span id="cb27-927"><a href="#cb27-927" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-928"><a href="#cb27-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-929"><a href="#cb27-929" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(0)</span></span>
<span id="cb27-930"><a href="#cb27-930" aria-hidden="true" tabindex="-1"></a><span class="in">NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Identity())</span></span>
<span id="cb27-931"><a href="#cb27-931" aria-hidden="true" tabindex="-1"></a><span class="in">NN = DifferentActivations(NN, [torch.exp, nn.Softplus()])</span></span>
<span id="cb27-932"><a href="#cb27-932" aria-hidden="true" tabindex="-1"></a><span class="in">NN = AffineTransformation(NN, low, upp)</span></span>
<span id="cb27-933"><a href="#cb27-933" aria-hidden="true" tabindex="-1"></a><span class="in">VA = VA_NeuralNet(neural_net=NN, model=Probit)</span></span>
<span id="cb27-934"><a href="#cb27-934" aria-hidden="true" tabindex="-1"></a><span class="in">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb27-935"><a href="#cb27-935" aria-hidden="true" tabindex="-1"></a><span class="in">Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=alpha, use_log_lik=True)</span></span>
<span id="cb27-936"><a href="#cb27-936" aria-hidden="true" tabindex="-1"></a><span class="in">with torch.no_grad():</span></span>
<span id="cb27-937"><a href="#cb27-937" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-938"><a href="#cb27-938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-939"><a href="#cb27-939" aria-hidden="true" tabindex="-1"></a><span class="in">num_epochs = 10000</span></span>
<span id="cb27-940"><a href="#cb27-940" aria-hidden="true" tabindex="-1"></a><span class="in">loss_fct = "LB_MI"</span></span>
<span id="cb27-941"><a href="#cb27-941" aria-hidden="true" tabindex="-1"></a><span class="in">optimizer = torch_Adam</span></span>
<span id="cb27-942"><a href="#cb27-942" aria-hidden="true" tabindex="-1"></a><span class="in">num_samples_MI = 100</span></span>
<span id="cb27-943"><a href="#cb27-943" aria-hidden="true" tabindex="-1"></a><span class="in">freq_MI = 500</span></span>
<span id="cb27-944"><a href="#cb27-944" aria-hidden="true" tabindex="-1"></a><span class="in">save_best_param = True</span></span>
<span id="cb27-945"><a href="#cb27-945" aria-hidden="true" tabindex="-1"></a><span class="in">learning_rate = 0.001</span></span>
<span id="cb27-946"><a href="#cb27-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-947"><a href="#cb27-947" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_Probit_nocstr : </span></span>
<span id="cb27-948"><a href="#cb27-948" aria-hidden="true" tabindex="-1"></a><span class="in">    MI, range_MI, lower_MI, upper_MI = Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, num_samples_MI, </span></span>
<span id="cb27-949"><a href="#cb27-949" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            freq_MI, save_best_param, learning_rate, momentum=True)</span></span>
<span id="cb27-950"><a href="#cb27-950" aria-hidden="true" tabindex="-1"></a><span class="in">    all_params = torch.cat([param.view(-1) for param in NN.parameters()])</span></span>
<span id="cb27-951"><a href="#cb27-951" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-952"><a href="#cb27-952" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-953"><a href="#cb27-953" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample = Div.va.implicit_prior_sampler(n_samples_prior)</span></span>
<span id="cb27-954"><a href="#cb27-954" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-955"><a href="#cb27-955" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample_prior = theta_sample.numpy()</span></span>
<span id="cb27-956"><a href="#cb27-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-957"><a href="#cb27-957" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(tirages_path, 'rb') as file:</span></span>
<span id="cb27-958"><a href="#cb27-958" aria-hidden="true" tabindex="-1"></a><span class="in">        data = pickle.load(file)</span></span>
<span id="cb27-959"><a href="#cb27-959" aria-hidden="true" tabindex="-1"></a><span class="in">    data_A, data_Z = data[0], data[1]</span></span>
<span id="cb27-960"><a href="#cb27-960" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_true = np.array([3.37610525, 0.43304097])</span></span>
<span id="cb27-961"><a href="#cb27-961" aria-hidden="true" tabindex="-1"></a><span class="in">    N = 50  # non-degenerate</span></span>
<span id="cb27-962"><a href="#cb27-962" aria-hidden="true" tabindex="-1"></a><span class="in">    i = 2</span></span>
<span id="cb27-963"><a href="#cb27-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-964"><a href="#cb27-964" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-965"><a href="#cb27-965" aria-hidden="true" tabindex="-1"></a><span class="in">    Xstack = np.stack((data_Z[:N, i],data_A[:N, i]),axis=1)</span></span>
<span id="cb27-966"><a href="#cb27-966" aria-hidden="true" tabindex="-1"></a><span class="in">    X = torch.tensor(Xstack)</span></span>
<span id="cb27-967"><a href="#cb27-967" aria-hidden="true" tabindex="-1"></a><span class="in">    D = X.unsqueeze(1)</span></span>
<span id="cb27-968"><a href="#cb27-968" aria-hidden="true" tabindex="-1"></a><span class="in">    Probit.data = D</span></span>
<span id="cb27-969"><a href="#cb27-969" aria-hidden="true" tabindex="-1"></a><span class="in">    n_samples_post = 5000</span></span>
<span id="cb27-970"><a href="#cb27-970" aria-hidden="true" tabindex="-1"></a><span class="in">    T_mcmc = 5*10**4 + 1 </span></span>
<span id="cb27-971"><a href="#cb27-971" aria-hidden="true" tabindex="-1"></a><span class="in">    sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-972"><a href="#cb27-972" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_0 = torch.randn(p)</span></span>
<span id="cb27-973"><a href="#cb27-973" aria-hidden="true" tabindex="-1"></a><span class="in">    #eps_0 = 10 * torch.ones(p)</span></span>
<span id="cb27-974"><a href="#cb27-974" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept=0.4, adap=True, Cov=True, disable_tqdm=False)</span></span>
<span id="cb27-975"><a href="#cb27-975" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_MH = NN(eps_MH)</span></span>
<span id="cb27-976"><a href="#cb27-976" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-977"><a href="#cb27-977" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = theta_MH.detach().numpy()</span></span>
<span id="cb27-978"><a href="#cb27-978" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post_nocstr = theta_MH[-n_samples_post:,-n_samples_post:]</span></span>
<span id="cb27-979"><a href="#cb27-979" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-980"><a href="#cb27-980" aria-hidden="true" tabindex="-1"></a><span class="in">    # Saves all relevant quantities</span></span>
<span id="cb27-981"><a href="#cb27-981" aria-hidden="true" tabindex="-1"></a><span class="in">    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI}</span></span>
<span id="cb27-982"><a href="#cb27-982" aria-hidden="true" tabindex="-1"></a><span class="in">    Prior = {'samples' : theta_sample_prior, 'jeffreys' : None, 'params' : all_params}</span></span>
<span id="cb27-983"><a href="#cb27-983" aria-hidden="true" tabindex="-1"></a><span class="in">    Posterior = {'samples' : theta_post_nocstr, 'jeffreys' : None}</span></span>
<span id="cb27-984"><a href="#cb27-984" aria-hidden="true" tabindex="-1"></a><span class="in">    Probit_results_nocstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}</span></span>
<span id="cb27-985"><a href="#cb27-985" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-986"><a href="#cb27-986" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(Probit_results_nocstr, file)</span></span>
<span id="cb27-987"><a href="#cb27-987" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-988"><a href="#cb27-988" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-989"><a href="#cb27-989" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-990"><a href="#cb27-990" aria-hidden="true" tabindex="-1"></a><span class="in">        res = pickle.load(file)</span></span>
<span id="cb27-991"><a href="#cb27-991" aria-hidden="true" tabindex="-1"></a><span class="in">        MI_dic = res['MI']</span></span>
<span id="cb27-992"><a href="#cb27-992" aria-hidden="true" tabindex="-1"></a><span class="in">        Prior = res['prior'] </span></span>
<span id="cb27-993"><a href="#cb27-993" aria-hidden="true" tabindex="-1"></a><span class="in">        Posterior = res['post']</span></span>
<span id="cb27-994"><a href="#cb27-994" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper']</span></span>
<span id="cb27-995"><a href="#cb27-995" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample_prior, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']</span></span>
<span id="cb27-996"><a href="#cb27-996" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post_nocstr, jeffreys_post = Posterior['samples'], Posterior['jeffreys']    </span></span>
<span id="cb27-997"><a href="#cb27-997" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-998"><a href="#cb27-998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-999"><a href="#cb27-999" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1000"><a href="#cb27-1000" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_mi</span></span>
<span id="cb27-1001"><a href="#cb27-1001" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap:  "Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 100 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. The learning rate used in the optimization is $0.001$."</span></span>
<span id="cb27-1002"><a href="#cb27-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1003"><a href="#cb27-1003" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(4, 3))</span></span>
<span id="cb27-1004"><a href="#cb27-1004" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '-', color=rougeCEA)</span></span>
<span id="cb27-1005"><a href="#cb27-1005" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '*', color=rougeCEA)</span></span>
<span id="cb27-1006"><a href="#cb27-1006" aria-hidden="true" tabindex="-1"></a><span class="in"># for i in range(len(range_MI)):</span></span>
<span id="cb27-1007"><a href="#cb27-1007" aria-hidden="true" tabindex="-1"></a><span class="in">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')</span></span>
<span id="cb27-1008"><a href="#cb27-1008" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, upper_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1009"><a href="#cb27-1009" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, lower_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1010"><a href="#cb27-1010" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)</span></span>
<span id="cb27-1011"><a href="#cb27-1011" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"Epochs")</span></span>
<span id="cb27-1012"><a href="#cb27-1012" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel(r"Generalized mutual information")</span></span>
<span id="cb27-1013"><a href="#cb27-1013" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1014"><a href="#cb27-1014" aria-hidden="true" tabindex="-1"></a><span class="in">#plt.yscale('log')</span></span>
<span id="cb27-1015"><a href="#cb27-1015" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1016"><a href="#cb27-1016" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1017"><a href="#cb27-1017" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1018"><a href="#cb27-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1019"><a href="#cb27-1019" aria-hidden="true" tabindex="-1"></a>In @fig-probit_mi is shown the evolution of the mutual information through the optimization of the VA-RP for the probit model. </span>
<span id="cb27-1020"><a href="#cb27-1020" aria-hidden="true" tabindex="-1"></a>We perceive high mutual information values at the initialization, which we interpret as a result of the fact that the parametric prior on $\theta_1$ is already quite close to its target distribution.</span>
<span id="cb27-1021"><a href="#cb27-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1022"><a href="#cb27-1022" aria-hidden="true" tabindex="-1"></a>With $\alpha$-divergences, using a moment constraint of the form $a(\theta_2) =\theta_2^{\kappa}$ for the second component is relevant here as long as $\kappa \in \left(0, \frac{2}{1+1/\alpha}\right)$, to ensure that the resulting constrained prior is indeed proper. With $\alpha=0.5$, we take the value $\kappa=1/8$ and we use the same neural network.</span>
<span id="cb27-1023"><a href="#cb27-1023" aria-hidden="true" tabindex="-1"></a>The evolution of the mutual information through the optimization of the constrained VA-RP is proposed in @fig-probit_constr_mi. In @fig-probit_constr_gap is presented the evolution of the constrained gap: the difference between the target and current values for the constraint.</span>
<span id="cb27-1024"><a href="#cb27-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1025"><a href="#cb27-1025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1026"><a href="#cb27-1026" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-1027"><a href="#cb27-1027" aria-hidden="true" tabindex="-1"></a><span class="in">kappa = 1/8</span></span>
<span id="cb27-1028"><a href="#cb27-1028" aria-hidden="true" tabindex="-1"></a><span class="in">K_val = np.mean((theta_sample_prior[:,1]**kappa)**(1/alpha))</span></span>
<span id="cb27-1029"><a href="#cb27-1029" aria-hidden="true" tabindex="-1"></a><span class="in">c_val = np.mean((theta_sample_prior[:,1]**kappa)**(1+1/alpha))</span></span>
<span id="cb27-1030"><a href="#cb27-1030" aria-hidden="true" tabindex="-1"></a><span class="in">constr_val = c_val / K_val</span></span>
<span id="cb27-1031"><a href="#cb27-1031" aria-hidden="true" tabindex="-1"></a><span class="in">alpha_constr = np.mean((theta_sample_prior[:,0]**-kappa + 1)**-1)</span></span>
<span id="cb27-1032"><a href="#cb27-1032" aria-hidden="true" tabindex="-1"></a><span class="in">#print(f'Constraint value estimation : {constr_val}')  # = 0.839594841003418 </span></span>
<span id="cb27-1033"><a href="#cb27-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1034"><a href="#cb27-1034" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameters and classes</span></span>
<span id="cb27-1035"><a href="#cb27-1035" aria-hidden="true" tabindex="-1"></a><span class="in">p = 50     # latent space dimension</span></span>
<span id="cb27-1036"><a href="#cb27-1036" aria-hidden="true" tabindex="-1"></a><span class="in">q = 2      # parameter space dimension</span></span>
<span id="cb27-1037"><a href="#cb27-1037" aria-hidden="true" tabindex="-1"></a><span class="in">N = 500    # number of data samples</span></span>
<span id="cb27-1038"><a href="#cb27-1038" aria-hidden="true" tabindex="-1"></a><span class="in">J = 500    # nb samples for MC estimation in MI</span></span>
<span id="cb27-1039"><a href="#cb27-1039" aria-hidden="true" tabindex="-1"></a><span class="in">T = 50     # nb samples MC marginal likelihood</span></span>
<span id="cb27-1040"><a href="#cb27-1040" aria-hidden="true" tabindex="-1"></a><span class="in">alpha = 0.5</span></span>
<span id="cb27-1041"><a href="#cb27-1041" aria-hidden="true" tabindex="-1"></a><span class="in">input_size = p  </span></span>
<span id="cb27-1042"><a href="#cb27-1042" aria-hidden="true" tabindex="-1"></a><span class="in">output_size = q</span></span>
<span id="cb27-1043"><a href="#cb27-1043" aria-hidden="true" tabindex="-1"></a><span class="in">low = 0.0001          # lower bound </span></span>
<span id="cb27-1044"><a href="#cb27-1044" aria-hidden="true" tabindex="-1"></a><span class="in">upp = 1 + low</span></span>
<span id="cb27-1045"><a href="#cb27-1045" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**6</span></span>
<span id="cb27-1046"><a href="#cb27-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1047"><a href="#cb27-1047" aria-hidden="true" tabindex="-1"></a><span class="in">mu_a, sigma2_a = 0, 1</span></span>
<span id="cb27-1048"><a href="#cb27-1048" aria-hidden="true" tabindex="-1"></a><span class="in">#mu_a, sigma2_a = 8.7 * 10**-3, 1.03</span></span>
<span id="cb27-1049"><a href="#cb27-1049" aria-hidden="true" tabindex="-1"></a><span class="in">Probit = torch_ProbitModel(use_log_normal=True, mu_a=mu_a, sigma2_a=sigma2_a, set_beta=None, alt_scaling=True)</span></span>
<span id="cb27-1050"><a href="#cb27-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1051"><a href="#cb27-1051" aria-hidden="true" tabindex="-1"></a><span class="in"># Constraints</span></span>
<span id="cb27-1052"><a href="#cb27-1052" aria-hidden="true" tabindex="-1"></a><span class="in">beta = torch.tensor([kappa])</span></span>
<span id="cb27-1053"><a href="#cb27-1053" aria-hidden="true" tabindex="-1"></a><span class="in">b = torch.tensor([[alpha_constr,constr_val]]) </span></span>
<span id="cb27-1054"><a href="#cb27-1054" aria-hidden="true" tabindex="-1"></a><span class="in">T_cstr = 100000</span></span>
<span id="cb27-1055"><a href="#cb27-1055" aria-hidden="true" tabindex="-1"></a><span class="in">eta_augm = torch.tensor([[0.,1.]])</span></span>
<span id="cb27-1056"><a href="#cb27-1056" aria-hidden="true" tabindex="-1"></a><span class="in">eta = torch.tensor([[0.,1.]])</span></span>
<span id="cb27-1057"><a href="#cb27-1057" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Probit_results_constrained.pkl'</span></span>
<span id="cb27-1058"><a href="#cb27-1058" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-1059"><a href="#cb27-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1060"><a href="#cb27-1060" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(0)</span></span>
<span id="cb27-1061"><a href="#cb27-1061" aria-hidden="true" tabindex="-1"></a><span class="in">NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Identity())</span></span>
<span id="cb27-1062"><a href="#cb27-1062" aria-hidden="true" tabindex="-1"></a><span class="in">NN = DifferentActivations(NN, [torch.exp, nn.Softplus()])</span></span>
<span id="cb27-1063"><a href="#cb27-1063" aria-hidden="true" tabindex="-1"></a><span class="in">NN = AffineTransformation(NN, low, upp)</span></span>
<span id="cb27-1064"><a href="#cb27-1064" aria-hidden="true" tabindex="-1"></a><span class="in">VA = VA_NeuralNet(neural_net=NN, model=Probit)</span></span>
<span id="cb27-1065"><a href="#cb27-1065" aria-hidden="true" tabindex="-1"></a><span class="in">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb27-1066"><a href="#cb27-1066" aria-hidden="true" tabindex="-1"></a><span class="in">Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=0.5, use_log_lik=True)</span></span>
<span id="cb27-1067"><a href="#cb27-1067" aria-hidden="true" tabindex="-1"></a><span class="in">Constr = Constraints_NeuralNet(div=Div, betas=beta, b=b, T_cstr=T_cstr, </span></span>
<span id="cb27-1068"><a href="#cb27-1068" aria-hidden="true" tabindex="-1"></a><span class="in">                               objective='LB_MI', lag_method='augmented', eta_augm=eta_augm, rule='SGD')</span></span>
<span id="cb27-1069"><a href="#cb27-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1070"><a href="#cb27-1070" aria-hidden="true" tabindex="-1"></a><span class="in">with torch.no_grad():</span></span>
<span id="cb27-1071"><a href="#cb27-1071" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-1072"><a href="#cb27-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1073"><a href="#cb27-1073" aria-hidden="true" tabindex="-1"></a><span class="in">num_epochs = 10000</span></span>
<span id="cb27-1074"><a href="#cb27-1074" aria-hidden="true" tabindex="-1"></a><span class="in">optimizer = torch_Adam</span></span>
<span id="cb27-1075"><a href="#cb27-1075" aria-hidden="true" tabindex="-1"></a><span class="in">num_samples_MI = 100</span></span>
<span id="cb27-1076"><a href="#cb27-1076" aria-hidden="true" tabindex="-1"></a><span class="in">freq_MI = 500</span></span>
<span id="cb27-1077"><a href="#cb27-1077" aria-hidden="true" tabindex="-1"></a><span class="in">save_best_param = False</span></span>
<span id="cb27-1078"><a href="#cb27-1078" aria-hidden="true" tabindex="-1"></a><span class="in">learning_rate = 0.0005</span></span>
<span id="cb27-1079"><a href="#cb27-1079" aria-hidden="true" tabindex="-1"></a><span class="in">freq_augm = 100</span></span>
<span id="cb27-1080"><a href="#cb27-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1081"><a href="#cb27-1081" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_Probit_cstr :</span></span>
<span id="cb27-1082"><a href="#cb27-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1083"><a href="#cb27-1083" aria-hidden="true" tabindex="-1"></a><span class="in">    ### Training loop</span></span>
<span id="cb27-1084"><a href="#cb27-1084" aria-hidden="true" tabindex="-1"></a><span class="in">    MI, constr_values, range_MI, lower_MI, upper_MI = Constr.Partial_autograd(J, N, eta, num_epochs, optimizer, num_samples_MI, </span></span>
<span id="cb27-1085"><a href="#cb27-1085" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            freq_MI, save_best_param, learning_rate, momentum=True,</span></span>
<span id="cb27-1086"><a href="#cb27-1086" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            freq_augm=freq_augm, max_violation=0.005, update_eta_augm=2.)</span></span>
<span id="cb27-1087"><a href="#cb27-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1088"><a href="#cb27-1088" aria-hidden="true" tabindex="-1"></a><span class="in">    constr_values = torch.stack(constr_values).numpy()</span></span>
<span id="cb27-1089"><a href="#cb27-1089" aria-hidden="true" tabindex="-1"></a><span class="in">    all_params = torch.cat([param.view(-1) for param in NN.parameters()])</span></span>
<span id="cb27-1090"><a href="#cb27-1090" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-1091"><a href="#cb27-1091" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1092"><a href="#cb27-1092" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample = Constr.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-1093"><a href="#cb27-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1094"><a href="#cb27-1094" aria-hidden="true" tabindex="-1"></a><span class="in">    print(f'Moment of order {beta.item()}, estimation : {np.mean(theta_sample**beta.item(),axis=0)}, wanted value : {b}')</span></span>
<span id="cb27-1095"><a href="#cb27-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1096"><a href="#cb27-1096" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-1097"><a href="#cb27-1097" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(tirages_path, 'rb') as file:</span></span>
<span id="cb27-1098"><a href="#cb27-1098" aria-hidden="true" tabindex="-1"></a><span class="in">        data = pickle.load(file)</span></span>
<span id="cb27-1099"><a href="#cb27-1099" aria-hidden="true" tabindex="-1"></a><span class="in">    data_A, data_Z = data[0], data[1]</span></span>
<span id="cb27-1100"><a href="#cb27-1100" aria-hidden="true" tabindex="-1"></a><span class="in">    N = 50  # non-degenerate</span></span>
<span id="cb27-1101"><a href="#cb27-1101" aria-hidden="true" tabindex="-1"></a><span class="in">    i = 2</span></span>
<span id="cb27-1102"><a href="#cb27-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1103"><a href="#cb27-1103" aria-hidden="true" tabindex="-1"></a><span class="in">    Xstack = np.stack((data_Z[:N, i],data_A[:N, i]),axis=1)</span></span>
<span id="cb27-1104"><a href="#cb27-1104" aria-hidden="true" tabindex="-1"></a><span class="in">    X = torch.tensor(Xstack)</span></span>
<span id="cb27-1105"><a href="#cb27-1105" aria-hidden="true" tabindex="-1"></a><span class="in">    D = X.unsqueeze(1)</span></span>
<span id="cb27-1106"><a href="#cb27-1106" aria-hidden="true" tabindex="-1"></a><span class="in">    Probit.data = D</span></span>
<span id="cb27-1107"><a href="#cb27-1107" aria-hidden="true" tabindex="-1"></a><span class="in">    n_samples_post = 5000</span></span>
<span id="cb27-1108"><a href="#cb27-1108" aria-hidden="true" tabindex="-1"></a><span class="in">    T_mcmc = 5*10**4 + 1 </span></span>
<span id="cb27-1109"><a href="#cb27-1109" aria-hidden="true" tabindex="-1"></a><span class="in">    sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-1110"><a href="#cb27-1110" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_0 = torch.randn(p)</span></span>
<span id="cb27-1111"><a href="#cb27-1111" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, target_accept=0.4, adap=True, Cov=True, disable_tqdm=False)</span></span>
<span id="cb27-1112"><a href="#cb27-1112" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_MH = NN(eps_MH)</span></span>
<span id="cb27-1113"><a href="#cb27-1113" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1114"><a href="#cb27-1114" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = theta_MH.detach().numpy()</span></span>
<span id="cb27-1115"><a href="#cb27-1115" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post_cstr = theta_MH[-n_samples_post:,-n_samples_post:]</span></span>
<span id="cb27-1116"><a href="#cb27-1116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1117"><a href="#cb27-1117" aria-hidden="true" tabindex="-1"></a><span class="in">    # Saves all relevant quantities</span></span>
<span id="cb27-1118"><a href="#cb27-1118" aria-hidden="true" tabindex="-1"></a><span class="in">    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI, 'constr' : constr_values}</span></span>
<span id="cb27-1119"><a href="#cb27-1119" aria-hidden="true" tabindex="-1"></a><span class="in">    Prior = {'samples' : theta_sample_prior, 'jeffreys' : None, 'params' : all_params}</span></span>
<span id="cb27-1120"><a href="#cb27-1120" aria-hidden="true" tabindex="-1"></a><span class="in">    Posterior = {'samples' : theta_post_cstr, 'jeffreys' : None}</span></span>
<span id="cb27-1121"><a href="#cb27-1121" aria-hidden="true" tabindex="-1"></a><span class="in">    Probit_results_cstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}</span></span>
<span id="cb27-1122"><a href="#cb27-1122" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-1123"><a href="#cb27-1123" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(Probit_results_cstr, file)</span></span>
<span id="cb27-1124"><a href="#cb27-1124" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1125"><a href="#cb27-1125" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-1126"><a href="#cb27-1126" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1127"><a href="#cb27-1127" aria-hidden="true" tabindex="-1"></a><span class="in">        res = pickle.load(file)</span></span>
<span id="cb27-1128"><a href="#cb27-1128" aria-hidden="true" tabindex="-1"></a><span class="in">        MI_dic = res['MI']</span></span>
<span id="cb27-1129"><a href="#cb27-1129" aria-hidden="true" tabindex="-1"></a><span class="in">        Prior = res['prior'] </span></span>
<span id="cb27-1130"><a href="#cb27-1130" aria-hidden="true" tabindex="-1"></a><span class="in">        Posterior = res['post']</span></span>
<span id="cb27-1131"><a href="#cb27-1131" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI, constr_values = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper'], MI_dic['constr']</span></span>
<span id="cb27-1132"><a href="#cb27-1132" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample_prior, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']</span></span>
<span id="cb27-1133"><a href="#cb27-1133" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post_cstr, jeffreys_post = Posterior['samples'], Posterior['jeffreys']</span></span>
<span id="cb27-1134"><a href="#cb27-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1135"><a href="#cb27-1135" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1136"><a href="#cb27-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1137"><a href="#cb27-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1138"><a href="#cb27-1138" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1139"><a href="#cb27-1139" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_constr_mi</span></span>
<span id="cb27-1140"><a href="#cb27-1140" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 100 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. The learning rate used in the optimization is $0.0005$."</span></span>
<span id="cb27-1141"><a href="#cb27-1141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1142"><a href="#cb27-1142" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1143"><a href="#cb27-1143" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '-', color=rougeCEA)</span></span>
<span id="cb27-1144"><a href="#cb27-1144" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, MI, '*', color=rougeCEA)</span></span>
<span id="cb27-1145"><a href="#cb27-1145" aria-hidden="true" tabindex="-1"></a><span class="in"># for i in range(len(range_MI)):</span></span>
<span id="cb27-1146"><a href="#cb27-1146" aria-hidden="true" tabindex="-1"></a><span class="in">#     plt.plot([range_MI[i], range_MI[i]], [lower_MI[i], upper_MI[i]], color='black')</span></span>
<span id="cb27-1147"><a href="#cb27-1147" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, upper_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1148"><a href="#cb27-1148" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, lower_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1149"><a href="#cb27-1149" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)</span></span>
<span id="cb27-1150"><a href="#cb27-1150" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"Epochs")</span></span>
<span id="cb27-1151"><a href="#cb27-1151" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel(r"Generalized mutual information")</span></span>
<span id="cb27-1152"><a href="#cb27-1152" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1153"><a href="#cb27-1153" aria-hidden="true" tabindex="-1"></a><span class="in">#plt.yscale('log')</span></span>
<span id="cb27-1154"><a href="#cb27-1154" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1155"><a href="#cb27-1155" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1156"><a href="#cb27-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1157"><a href="#cb27-1157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1158"><a href="#cb27-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1159"><a href="#cb27-1159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1160"><a href="#cb27-1160" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_constr_gap</span></span>
<span id="cb27-1161"><a href="#cb27-1161" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Evolution of the constraint value gap during training. It corresponds to the difference between the target and current values for the constraint (in absolute value)"</span></span>
<span id="cb27-1162"><a href="#cb27-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1163"><a href="#cb27-1163" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1164"><a href="#cb27-1164" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, constr_values[:,0,1], '-', color=vertCEA)</span></span>
<span id="cb27-1165"><a href="#cb27-1165" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(range_MI, constr_values[:,0,1], '*', color=vertCEA)</span></span>
<span id="cb27-1166"><a href="#cb27-1166" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"Epochs")</span></span>
<span id="cb27-1167"><a href="#cb27-1167" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel(r"Constraint gap")</span></span>
<span id="cb27-1168"><a href="#cb27-1168" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1169"><a href="#cb27-1169" aria-hidden="true" tabindex="-1"></a><span class="in">#plt.yscale('log')</span></span>
<span id="cb27-1170"><a href="#cb27-1170" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1171"><a href="#cb27-1171" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1172"><a href="#cb27-1172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1173"><a href="#cb27-1173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1174"><a href="#cb27-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1175"><a href="#cb27-1175" aria-hidden="true" tabindex="-1"></a>For the posterior, we take as dataset $50$ samples from the probit model. For computational reasons, the Metropolis-Hastings algorithm is applied for only $5\cdot10^4$ iterations. An important remark is that if the size of the dataset is rather small, the probability that the data is degenerate is not negligible. By degenerate data, we refer to situations when the data points are partitioned into two disjoint subsets when classified according to $a$ values, the posterior becomes improper because the likelihood is constant (@van2024reference). In such cases, the convergence of the Markov chains is less apparent, the plots for this section are obtained with non-degenerate datasets.   </span>
<span id="cb27-1176"><a href="#cb27-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1177"><a href="#cb27-1177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1178"><a href="#cb27-1178" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1179"><a href="#cb27-1179" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_post_scatterhist</span></span>
<span id="cb27-1180"><a href="#cb27-1180" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Scatter histogram of the unconstrained fitted posterior and the Jeffreys posterior distributions obtained from $5000$ samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels."</span></span>
<span id="cb27-1181"><a href="#cb27-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1182"><a href="#cb27-1182" aria-hidden="true" tabindex="-1"></a><span class="in">N = 50  # non-degenerate</span></span>
<span id="cb27-1183"><a href="#cb27-1183" aria-hidden="true" tabindex="-1"></a><span class="in">i = 2</span></span>
<span id="cb27-1184"><a href="#cb27-1184" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(int_jeffreys_path, f'model_J_{i}')</span></span>
<span id="cb27-1185"><a href="#cb27-1185" aria-hidden="true" tabindex="-1"></a><span class="in">with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1186"><a href="#cb27-1186" aria-hidden="true" tabindex="-1"></a><span class="in">    model_J = pickle.load(file)</span></span>
<span id="cb27-1187"><a href="#cb27-1187" aria-hidden="true" tabindex="-1"></a><span class="in">theta_J = model_J['logs']['post'][N]</span></span>
<span id="cb27-1188"><a href="#cb27-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1189"><a href="#cb27-1189" aria-hidden="true" tabindex="-1"></a><span class="in">x1 = theta_post_nocstr[:, 0]</span></span>
<span id="cb27-1190"><a href="#cb27-1190" aria-hidden="true" tabindex="-1"></a><span class="in">y1 = theta_post_nocstr[:, 1]</span></span>
<span id="cb27-1191"><a href="#cb27-1191" aria-hidden="true" tabindex="-1"></a><span class="in">x2 = theta_J[:, 0]</span></span>
<span id="cb27-1192"><a href="#cb27-1192" aria-hidden="true" tabindex="-1"></a><span class="in">y2 = theta_J[:, 1]</span></span>
<span id="cb27-1193"><a href="#cb27-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1194"><a href="#cb27-1194" aria-hidden="true" tabindex="-1"></a><span class="in">kde_x1 = gaussian_kde(x1)</span></span>
<span id="cb27-1195"><a href="#cb27-1195" aria-hidden="true" tabindex="-1"></a><span class="in">kde_y1 = gaussian_kde(y1)</span></span>
<span id="cb27-1196"><a href="#cb27-1196" aria-hidden="true" tabindex="-1"></a><span class="in">kde_x2 = gaussian_kde(x2)</span></span>
<span id="cb27-1197"><a href="#cb27-1197" aria-hidden="true" tabindex="-1"></a><span class="in">kde_y2 = gaussian_kde(y2)</span></span>
<span id="cb27-1198"><a href="#cb27-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1199"><a href="#cb27-1199" aria-hidden="true" tabindex="-1"></a><span class="in"># Create figure and gridspec layout</span></span>
<span id="cb27-1200"><a href="#cb27-1200" aria-hidden="true" tabindex="-1"></a><span class="in">fig = plt.figure(figsize=(8, 6))</span></span>
<span id="cb27-1201"><a href="#cb27-1201" aria-hidden="true" tabindex="-1"></a><span class="in">gs = gridspec.GridSpec(4, 5) </span></span>
<span id="cb27-1202"><a href="#cb27-1202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1203"><a href="#cb27-1203" aria-hidden="true" tabindex="-1"></a><span class="in"># Main scatter plot </span></span>
<span id="cb27-1204"><a href="#cb27-1204" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main = fig.add_subplot(gs[1:4, 0:3])</span></span>
<span id="cb27-1205"><a href="#cb27-1205" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.scatter(x1, y1, alpha=0.5, s=20, marker='.', label='Fitted posterior',zorder=2)</span></span>
<span id="cb27-1206"><a href="#cb27-1206" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.scatter(x2, y2, alpha=0.5, s=20, marker='.', label='Jeffreys posterior',zorder=1, color=vertCEA)</span></span>
<span id="cb27-1207"><a href="#cb27-1207" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.set_xlabel(r'$\theta_1$')</span></span>
<span id="cb27-1208"><a href="#cb27-1208" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.set_ylabel(r'$\theta_2$')</span></span>
<span id="cb27-1209"><a href="#cb27-1209" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.legend(loc='upper left', fontsize=11)</span></span>
<span id="cb27-1210"><a href="#cb27-1210" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.grid()</span></span>
<span id="cb27-1211"><a href="#cb27-1211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1212"><a href="#cb27-1212" aria-hidden="true" tabindex="-1"></a><span class="in"># Marginal histogram / KDE for alpha</span></span>
<span id="cb27-1213"><a href="#cb27-1213" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist = fig.add_subplot(gs[0, 0:3], sharex=ax_main)</span></span>
<span id="cb27-1214"><a href="#cb27-1214" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.hist(x1, bins='rice', alpha=0.4, label='Fitted histogram',density=True)</span></span>
<span id="cb27-1215"><a href="#cb27-1215" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.hist(x2, bins='rice', alpha=0.4, label='Jeffreys histogram',density=True,color=vertCEA)</span></span>
<span id="cb27-1216"><a href="#cb27-1216" aria-hidden="true" tabindex="-1"></a><span class="in">x_vals = np.linspace(min(x1.min(), x2.min()), max(x1.max(), x2.max()), 100)</span></span>
<span id="cb27-1217"><a href="#cb27-1217" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.plot(x_vals, kde_x1(x_vals), color='blue', lw=2, label='Fitted KDE')</span></span>
<span id="cb27-1218"><a href="#cb27-1218" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.plot(x_vals, kde_x2(x_vals), color='green', lw=2, label='Jeffreys KDE')</span></span>
<span id="cb27-1219"><a href="#cb27-1219" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.set_ylabel(r'Marginal $\theta_1$')</span></span>
<span id="cb27-1220"><a href="#cb27-1220" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.tick_params(axis='x', labelbottom=False)</span></span>
<span id="cb27-1221"><a href="#cb27-1221" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.legend()</span></span>
<span id="cb27-1222"><a href="#cb27-1222" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.grid()</span></span>
<span id="cb27-1223"><a href="#cb27-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1224"><a href="#cb27-1224" aria-hidden="true" tabindex="-1"></a><span class="in"># Marginal histogram / KDE for beta</span></span>
<span id="cb27-1225"><a href="#cb27-1225" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist = fig.add_subplot(gs[1:4, 3:5], sharey=ax_main)</span></span>
<span id="cb27-1226"><a href="#cb27-1226" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.hist(y1, bins='rice', orientation='horizontal', alpha=0.4, label='Fitted histogram',density=True)</span></span>
<span id="cb27-1227"><a href="#cb27-1227" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.hist(y2, bins='rice', orientation='horizontal', alpha=0.4, label='Jeffreys histogram',density=True, color=vertCEA)</span></span>
<span id="cb27-1228"><a href="#cb27-1228" aria-hidden="true" tabindex="-1"></a><span class="in">y_vals = np.linspace(min(y1.min(), y2.min()), max(y1.max(), y2.max()), 100)</span></span>
<span id="cb27-1229"><a href="#cb27-1229" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.plot(kde_y1(y_vals), y_vals, color='blue', lw=2, label='Fitted KDE')</span></span>
<span id="cb27-1230"><a href="#cb27-1230" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.plot(kde_y2(y_vals), y_vals, color='green', lw=2, label='Jeffreys KDE')</span></span>
<span id="cb27-1231"><a href="#cb27-1231" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.set_xlabel(r'Marginal $\theta_2$')</span></span>
<span id="cb27-1232"><a href="#cb27-1232" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.tick_params(axis='y', labelleft=False)</span></span>
<span id="cb27-1233"><a href="#cb27-1233" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.legend()</span></span>
<span id="cb27-1234"><a href="#cb27-1234" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.grid()</span></span>
<span id="cb27-1235"><a href="#cb27-1235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1236"><a href="#cb27-1236" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1237"><a href="#cb27-1237" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1238"><a href="#cb27-1238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1239"><a href="#cb27-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1240"><a href="#cb27-1240" aria-hidden="true" tabindex="-1"></a>As @fig-probit_post_scatterhist shows, we obtain a relevant approximation of the true Jeffreys posterior especially on the variable $\theta_1$, whereas a small difference is present for the tail of the distribution on $\theta_2$. The latter remark was expected regarding the analytical study of the marginal distribution of $\pi_\lambda$ w.r.t. $\theta_2$  given the architecture considered for the VA-RP (see @eq-decay_rates_ptheta2).</span>
<span id="cb27-1241"><a href="#cb27-1241" aria-hidden="true" tabindex="-1"></a>It is interesting to see that the difference between the posteriors is harder to discern in the neighborhood of $\theta_2=0$. Indeed, in such case where the data are not degenerate, the likelihood provides a strong decay rate when $\theta_2\rightarrow0$ that makes the influence of the prior negligible (see @van2024reference):</span>
<span id="cb27-1242"><a href="#cb27-1242" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1243"><a href="#cb27-1243" aria-hidden="true" tabindex="-1"></a>L_N(\bX\,|\,\theta) \aseq{\theta_2\rightarrow0} \theta_2^{\|\chi\|_2^2}\exp\left(-\frac{1}{2\theta_2^2}\sum_{i=1}^N\chi_i(\log a_i-\log\theta_1)^2 \right),</span>
<span id="cb27-1244"><a href="#cb27-1244" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1245"><a href="#cb27-1245" aria-hidden="true" tabindex="-1"></a>where $\chi\in<span class="sc">\{</span>0,1<span class="sc">\}</span>^N$ is a non-null vector that depends on $\bX$.</span>
<span id="cb27-1246"><a href="#cb27-1246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1247"><a href="#cb27-1247" aria-hidden="true" tabindex="-1"></a>When $\theta_2\rightarrow\infty$, however, the likelihood does not reduce the influence of the prior has it remains asymptotically constant: $L_N(\bX \,|\,\theta) \conv{\theta_2\rightarrow\infty}2^{-N}$.</span>
<span id="cb27-1248"><a href="#cb27-1248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1249"><a href="#cb27-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1250"><a href="#cb27-1250" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1251"><a href="#cb27-1251" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_post_constr_scatterhist</span></span>
<span id="cb27-1252"><a href="#cb27-1252" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Scatter histogram of the constrained fitted posterior and the target posterior distributions obtained from $5000$ samples. Kernel density estimation is used on the marginal distributions in order to approximate their density functions with Gaussian kernels."</span></span>
<span id="cb27-1253"><a href="#cb27-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1254"><a href="#cb27-1254" aria-hidden="true" tabindex="-1"></a><span class="in">N = 50  # non-degenerate</span></span>
<span id="cb27-1255"><a href="#cb27-1255" aria-hidden="true" tabindex="-1"></a><span class="in">i = 2</span></span>
<span id="cb27-1256"><a href="#cb27-1256" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(int_jeffreys_path, f'model_J_constraint_{i}')</span></span>
<span id="cb27-1257"><a href="#cb27-1257" aria-hidden="true" tabindex="-1"></a><span class="in">with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1258"><a href="#cb27-1258" aria-hidden="true" tabindex="-1"></a><span class="in">    model_J = pickle.load(file)</span></span>
<span id="cb27-1259"><a href="#cb27-1259" aria-hidden="true" tabindex="-1"></a><span class="in">theta_J = model_J['logs']['post'][N]</span></span>
<span id="cb27-1260"><a href="#cb27-1260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1261"><a href="#cb27-1261" aria-hidden="true" tabindex="-1"></a><span class="in">x1 = theta_post_cstr[:, 0]</span></span>
<span id="cb27-1262"><a href="#cb27-1262" aria-hidden="true" tabindex="-1"></a><span class="in">y1 = theta_post_cstr[:, 1]</span></span>
<span id="cb27-1263"><a href="#cb27-1263" aria-hidden="true" tabindex="-1"></a><span class="in">x2 = theta_J[:, 0]</span></span>
<span id="cb27-1264"><a href="#cb27-1264" aria-hidden="true" tabindex="-1"></a><span class="in">y2 = theta_J[:, 1]</span></span>
<span id="cb27-1265"><a href="#cb27-1265" aria-hidden="true" tabindex="-1"></a><span class="in">kde_x1 = gaussian_kde(x1)</span></span>
<span id="cb27-1266"><a href="#cb27-1266" aria-hidden="true" tabindex="-1"></a><span class="in">kde_y1 = gaussian_kde(y1)</span></span>
<span id="cb27-1267"><a href="#cb27-1267" aria-hidden="true" tabindex="-1"></a><span class="in">kde_x2 = gaussian_kde(x2)</span></span>
<span id="cb27-1268"><a href="#cb27-1268" aria-hidden="true" tabindex="-1"></a><span class="in">kde_y2 = gaussian_kde(y2)</span></span>
<span id="cb27-1269"><a href="#cb27-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1270"><a href="#cb27-1270" aria-hidden="true" tabindex="-1"></a><span class="in"># Create figure and gridspec layout</span></span>
<span id="cb27-1271"><a href="#cb27-1271" aria-hidden="true" tabindex="-1"></a><span class="in">fig = plt.figure(figsize=(8, 6))</span></span>
<span id="cb27-1272"><a href="#cb27-1272" aria-hidden="true" tabindex="-1"></a><span class="in">gs = gridspec.GridSpec(4, 5) </span></span>
<span id="cb27-1273"><a href="#cb27-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1274"><a href="#cb27-1274" aria-hidden="true" tabindex="-1"></a><span class="in"># Main scatter plot </span></span>
<span id="cb27-1275"><a href="#cb27-1275" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main = fig.add_subplot(gs[1:4, 0:3])</span></span>
<span id="cb27-1276"><a href="#cb27-1276" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.scatter(x1, y1, alpha=0.5, s=20, marker='.', label='Fitted posterior',zorder=2)</span></span>
<span id="cb27-1277"><a href="#cb27-1277" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.scatter(x2, y2, alpha=0.5, s=20, marker='.', label='Jeffreys posterior',zorder=1, color=vertCEA)</span></span>
<span id="cb27-1278"><a href="#cb27-1278" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.set_xlabel(r'$\theta_1$')</span></span>
<span id="cb27-1279"><a href="#cb27-1279" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.set_ylabel(r'$\theta_2$')</span></span>
<span id="cb27-1280"><a href="#cb27-1280" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.legend(loc='upper left',fontsize=11)</span></span>
<span id="cb27-1281"><a href="#cb27-1281" aria-hidden="true" tabindex="-1"></a><span class="in">ax_main.grid()</span></span>
<span id="cb27-1282"><a href="#cb27-1282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1283"><a href="#cb27-1283" aria-hidden="true" tabindex="-1"></a><span class="in"># Marginal histogram / KDE for alpha</span></span>
<span id="cb27-1284"><a href="#cb27-1284" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist = fig.add_subplot(gs[0, 0:3], sharex=ax_main)</span></span>
<span id="cb27-1285"><a href="#cb27-1285" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.hist(x1, bins='rice', alpha=0.4, label='Fitted histogram',density=True)</span></span>
<span id="cb27-1286"><a href="#cb27-1286" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.hist(x2, bins='rice', alpha=0.4, label='Jeffreys histogram',density=True,color=vertCEA)</span></span>
<span id="cb27-1287"><a href="#cb27-1287" aria-hidden="true" tabindex="-1"></a><span class="in">x_vals = np.linspace(min(x1.min(), x2.min()), max(x1.max(), x2.max()), 100)</span></span>
<span id="cb27-1288"><a href="#cb27-1288" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.plot(x_vals, kde_x1(x_vals), color='blue', lw=2, label='Fitted KDE')</span></span>
<span id="cb27-1289"><a href="#cb27-1289" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.plot(x_vals, kde_x2(x_vals), color='green', lw=2, label='Jeffreys KDE')</span></span>
<span id="cb27-1290"><a href="#cb27-1290" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.set_ylabel(r'Marginal $\theta_1$')</span></span>
<span id="cb27-1291"><a href="#cb27-1291" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.tick_params(axis='x', labelbottom=False)</span></span>
<span id="cb27-1292"><a href="#cb27-1292" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.legend()</span></span>
<span id="cb27-1293"><a href="#cb27-1293" aria-hidden="true" tabindex="-1"></a><span class="in">ax_x_hist.grid()</span></span>
<span id="cb27-1294"><a href="#cb27-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1295"><a href="#cb27-1295" aria-hidden="true" tabindex="-1"></a><span class="in"># Marginal histogram / KDE for beta</span></span>
<span id="cb27-1296"><a href="#cb27-1296" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist = fig.add_subplot(gs[1:4, 3:5], sharey=ax_main)</span></span>
<span id="cb27-1297"><a href="#cb27-1297" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.hist(y1, bins='rice', orientation='horizontal', alpha=0.4, label='Fitted histogram',density=True)</span></span>
<span id="cb27-1298"><a href="#cb27-1298" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.hist(y2, bins='rice', orientation='horizontal', alpha=0.4, label='Jeffreys histogram',density=True, color=vertCEA)</span></span>
<span id="cb27-1299"><a href="#cb27-1299" aria-hidden="true" tabindex="-1"></a><span class="in">y_vals = np.linspace(min(y1.min(), y2.min()), max(y1.max(), y2.max()), 100)</span></span>
<span id="cb27-1300"><a href="#cb27-1300" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.plot(kde_y1(y_vals), y_vals, color='blue', lw=2, label='Fitted KDE')</span></span>
<span id="cb27-1301"><a href="#cb27-1301" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.plot(kde_y2(y_vals), y_vals, color='green', lw=2, label='Jeffreys KDE')</span></span>
<span id="cb27-1302"><a href="#cb27-1302" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.set_xlabel(r'Marginal $\theta_2$')</span></span>
<span id="cb27-1303"><a href="#cb27-1303" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.tick_params(axis='y', labelleft=False)</span></span>
<span id="cb27-1304"><a href="#cb27-1304" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.legend()</span></span>
<span id="cb27-1305"><a href="#cb27-1305" aria-hidden="true" tabindex="-1"></a><span class="in">ax_y_hist.grid()</span></span>
<span id="cb27-1306"><a href="#cb27-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1307"><a href="#cb27-1307" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1308"><a href="#cb27-1308" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1309"><a href="#cb27-1309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1310"><a href="#cb27-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1311"><a href="#cb27-1311" aria-hidden="true" tabindex="-1"></a>The result on the constrained case (@fig-probit_post_constr_scatterhist) is very similar to the unconstrained one. </span>
<span id="cb27-1312"><a href="#cb27-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1313"><a href="#cb27-1313" aria-hidden="true" tabindex="-1"></a>Indeed, the priors had already comparable shapes. Altogether, one can observe that the variational inference approach yields close results to the numerical integration approach @van2024reference, with or without constraints, even tough the matching of the decay rates w.r.t. $\theta_2$ remains limited given the simple network that we have used in this case.</span>
<span id="cb27-1314"><a href="#cb27-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1315"><a href="#cb27-1315" aria-hidden="true" tabindex="-1"></a>To ascertain the relevancy of our posterior approximation, we compute the posterior mean euclidean norm difference $\mathbb{E}_{\theta}\left[ ||\theta - \theta_{true}||  \right]$ as a function of the size of the dataset. In each computation, the neural network remains the same but the dataset changes by adding new entries. </span>
<span id="cb27-1316"><a href="#cb27-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1317"><a href="#cb27-1317" aria-hidden="true" tabindex="-1"></a>Furthermore, in order to assess the stability of the stochastic optimization with respect to the random number generator (RNG) seed, we also compute the empirical cumulative distribution functions (ECDFs) for each posterior distribution. For every seed, the parameters of the neural network are expected to be different, we keep the same dataset for the MCMC sampling however.</span>
<span id="cb27-1318"><a href="#cb27-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1319"><a href="#cb27-1319" aria-hidden="true" tabindex="-1"></a>Both types of computations are done in the unconstrained case as well as the constrained one. The different plots and details can be found in @sec-appendix.</span>
<span id="cb27-1320"><a href="#cb27-1320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1321"><a href="#cb27-1321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1322"><a href="#cb27-1322" aria-hidden="true" tabindex="-1"></a><span class="fu"># Conclusion</span></span>
<span id="cb27-1323"><a href="#cb27-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1324"><a href="#cb27-1324" aria-hidden="true" tabindex="-1"></a>In this work, we developed an algorithm to perform variational approximation of reference priors using a generalized definition of mutual information based on $f$-divergences.</span>
<span id="cb27-1325"><a href="#cb27-1325" aria-hidden="true" tabindex="-1"></a>To enhance computational efficiency, we derived </span>
<span id="cb27-1326"><a href="#cb27-1326" aria-hidden="true" tabindex="-1"></a>a lower bound of the generalized mutual information.</span>
<span id="cb27-1327"><a href="#cb27-1327" aria-hidden="true" tabindex="-1"></a>Additionally, because reference priors often yield improper posteriors, we adapted the variational definition of the problem to incorporate constraints that ensure the posteriors are proper. </span>
<span id="cb27-1328"><a href="#cb27-1328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1329"><a href="#cb27-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1330"><a href="#cb27-1330" aria-hidden="true" tabindex="-1"></a>Numerical experiments have been carried out on various test cases of different complexities in order to validate our approach.</span>
<span id="cb27-1331"><a href="#cb27-1331" aria-hidden="true" tabindex="-1"></a>These test cases range from purely toy models to more real-world problems, namely the estimation of seismic fragility curve parameters using a probit statistical model. </span>
<span id="cb27-1332"><a href="#cb27-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1333"><a href="#cb27-1333" aria-hidden="true" tabindex="-1"></a>The results demonstrate the usefulness of our approach in estimating both prior and posterior distributions across various problems.</span>
<span id="cb27-1334"><a href="#cb27-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1335"><a href="#cb27-1335" aria-hidden="true" tabindex="-1"></a>Our development is supported by an open source and flexible implementation, which can be adapted to a wide range of statistical models.</span>
<span id="cb27-1336"><a href="#cb27-1336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1337"><a href="#cb27-1337" aria-hidden="true" tabindex="-1"></a>Looking forward, the approximation of the tails of the reference priors could be improved. That is a complex problem in the field of variational approximation, as well as the stability of the algorithm when using deeper networks. An extension of this work to the approximation of Maximal Data Information (MDI) priors is also appealing, thanks to the fact MDI are proper under certain assumptions precised in @Bousquet2008.  </span>
<span id="cb27-1338"><a href="#cb27-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1339"><a href="#cb27-1339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1340"><a href="#cb27-1340" aria-hidden="true" tabindex="-1"></a><span class="fu"># Acknowledgement {.unnumbered}</span></span>
<span id="cb27-1341"><a href="#cb27-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1342"><a href="#cb27-1342" aria-hidden="true" tabindex="-1"></a>This research was supported by the CEA (French Alternative Energies and Atomic Energy Commission) and the SEISM Institute (https://www.institut-seism.fr/en/).</span>
<span id="cb27-1343"><a href="#cb27-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1344"><a href="#cb27-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1345"><a href="#cb27-1345" aria-hidden="true" tabindex="-1"></a><span class="fu"># Appendix {#sec-appendix} </span></span>
<span id="cb27-1346"><a href="#cb27-1346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1347"><a href="#cb27-1347" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gradient computation of the generalized mutual information  {#sec-grad_comp}</span></span>
<span id="cb27-1348"><a href="#cb27-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1349"><a href="#cb27-1349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1350"><a href="#cb27-1350" aria-hidden="true" tabindex="-1"></a>We recall that $F(x) = f(x)-xf'(x)$ and $p_{\lambda}$ is a shortcut notation for $p_{\pi_{\lambda}, N}$ being the marginal distribution under $\pi_{\lambda}$. The generalized mutual information writes : </span>
<span id="cb27-1351"><a href="#cb27-1351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1352"><a href="#cb27-1352" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-1353"><a href="#cb27-1353" aria-hidden="true" tabindex="-1"></a>    I_{\rD_f}(\pi_{\lambda}; L_N) &amp; = \int_{\Theta}{\rD_f}(p_{\lambda}||L_N(\cdot \, | \, \theta)) \pi_{\lambda}(\theta)d\theta <span class="sc">\\</span></span>
<span id="cb27-1354"><a href="#cb27-1354" aria-hidden="true" tabindex="-1"></a>    &amp; = \int_{\Theta} \int_{\mathcal{X}^N} \pi_{\lambda}(\theta)L_N(\bX \, | \, \theta)f\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \, | \, \theta)}\right) d\bX d\theta.</span>
<span id="cb27-1355"><a href="#cb27-1355" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-1356"><a href="#cb27-1356" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1357"><a href="#cb27-1357" aria-hidden="true" tabindex="-1"></a>For each $l$, taking the derivative with respect to $\lambda_l$ yields : </span>
<span id="cb27-1358"><a href="#cb27-1358" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1359"><a href="#cb27-1359" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-1360"><a href="#cb27-1360" aria-hidden="true" tabindex="-1"></a>\frac{\partial I_{\rD_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) &amp; = \int_{\Theta} \int_{\mathcal{X}^N} \frac{\partial \pi_{\lambda}}{\partial \lambda_l}(\theta)L_N(\bX \, | \, \theta)f\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \, | \, \theta)}\right) d\bX d\theta <span class="sc">\\</span></span>
<span id="cb27-1361"><a href="#cb27-1361" aria-hidden="true" tabindex="-1"></a>&amp; + \int_{\Theta} \int_{\mathcal{X}^N} \pi_{\lambda}(\theta)L_N(\bX \, | \, \theta)\frac{\partial p_{\lambda}}{\partial \lambda_l}\frac{1}{L_N(\bX \, | \, \theta)}(\bX)f'\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \, | \, \theta)}\right) d\bX d\theta, </span>
<span id="cb27-1362"><a href="#cb27-1362" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-1363"><a href="#cb27-1363" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1364"><a href="#cb27-1364" aria-hidden="true" tabindex="-1"></a>or in terms of expectations : </span>
<span id="cb27-1365"><a href="#cb27-1365" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1366"><a href="#cb27-1366" aria-hidden="true" tabindex="-1"></a>\frac{\partial I_{\rD_f}}{\partial \lambda_l}(\pi_{\lambda}; L_N) = \frac{\partial}{\partial \lambda_l} \mathbb{E}_{\theta \sim \pi_{\lambda}} \left<span class="co">[</span><span class="ot"> \tilde{I}(\theta)  \right</span><span class="co">]</span> </span>
<span id="cb27-1367"><a href="#cb27-1367" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>\mathbb{E}_{\theta \sim \pi_{\lambda}}\left[ \mathbb{E}_{\bX \sim L_N(\cdot |\theta)}\left[ \frac{1}{L_N(\bX \,|\,\theta)} \frac{\partial p_{\lambda}}{\partial \lambda_l}(\bX)f'\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right)\right]  \right],</span>
<span id="cb27-1368"><a href="#cb27-1368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1369"><a href="#cb27-1369" aria-hidden="true" tabindex="-1"></a>where : </span>
<span id="cb27-1370"><a href="#cb27-1370" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1371"><a href="#cb27-1371" aria-hidden="true" tabindex="-1"></a> \tilde{I}(\theta) = \int_{\mathcal{X}^N} L_N(\bX \, | \, \theta)f\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \, | \, \theta)}\right) d\bX.</span>
<span id="cb27-1372"><a href="#cb27-1372" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1373"><a href="#cb27-1373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1374"><a href="#cb27-1374" aria-hidden="true" tabindex="-1"></a>We note that the derivative with respect to $\lambda_l$ does not apply to $\tilde{I}$ in the previous equation. Using the chain rule yields :</span>
<span id="cb27-1375"><a href="#cb27-1375" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1376"><a href="#cb27-1376" aria-hidden="true" tabindex="-1"></a> \frac{\partial}{\partial \lambda_l} \mathbb{E}_{\theta \sim \pi_{\lambda}} \left[ \tilde{I}(\theta)  \right] = \frac{\partial}{\partial \lambda_l} \mathbb{E}_{\varepsilon} \left[ \tilde{I}(g(\lambda, \varepsilon)) \right] = \mathbb{E}_{\varepsilon}\left[\sum_{j=1}^q\frac{\partial \tilde{I}}{\partial \theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon)\right]. </span>
<span id="cb27-1377"><a href="#cb27-1377" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1378"><a href="#cb27-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1379"><a href="#cb27-1379" aria-hidden="true" tabindex="-1"></a>We have the following for every $j \in <span class="sc">\{</span>1,...,q<span class="sc">\}</span>$ : </span>
<span id="cb27-1380"><a href="#cb27-1380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1381"><a href="#cb27-1381" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb27-1382"><a href="#cb27-1382" aria-hidden="true" tabindex="-1"></a>\frac{\partial \tilde{I}}{\partial \theta_j}(\theta) &amp; = \int_{\mathcal{X}^N}   \frac{- p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}   \frac{\partial L_N}{\partial \theta_j}(\bX \,|\,\theta)f'\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right) + f\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right) \frac{\partial L_N}{\partial \theta_j}(\bX \,|\,\theta) d\bX  <span class="sc">\\</span></span>
<span id="cb27-1383"><a href="#cb27-1383" aria-hidden="true" tabindex="-1"></a>&amp; = \int_{\mathcal{X}^N}F\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right) \frac{\partial L_N}{\partial \theta_j}(\bX \,|\,\theta) d\bX  <span class="sc">\\</span></span>
<span id="cb27-1384"><a href="#cb27-1384" aria-hidden="true" tabindex="-1"></a>&amp; =  \mathbb{E}_{\bX \sim L_N(\cdot |\theta)}\left[ \frac{\partial \log L_N}{\partial \theta_j}(\bX \,|\,\theta) F\left( \frac{p_{\lambda}(\bX)}{L_N(\bX \,|\,\theta)}\right)\right]. </span>
<span id="cb27-1385"><a href="#cb27-1385" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-1386"><a href="#cb27-1386" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1387"><a href="#cb27-1387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1388"><a href="#cb27-1388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1389"><a href="#cb27-1389" aria-hidden="true" tabindex="-1"></a>Putting everything together, we finally obtain the desired formula. The gradient of the generalized lower bound function is obtained in a very similar manner.</span>
<span id="cb27-1390"><a href="#cb27-1390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1391"><a href="#cb27-1391" aria-hidden="true" tabindex="-1"></a>In what follows, we prove that the gradient of $I_{\rD_f}$, as formulated in @eq-gradientIdf aligns with the form of @eq-compatible_objective_function. </span>
<span id="cb27-1392"><a href="#cb27-1392" aria-hidden="true" tabindex="-1"></a>We write, for $l\in<span class="sc">\{</span>1,\dots,L<span class="sc">\}</span>$:</span>
<span id="cb27-1393"><a href="#cb27-1393" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1394"><a href="#cb27-1394" aria-hidden="true" tabindex="-1"></a>    \frac{\partial I_{\rD_f}}{\partial\lambda_l}(\pi_\lambda;L_N) = \mathbb{E}_\varepsilon \left[\sum_{j=1}^q\frac{\partial\tilde I}{\partial\theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial \lambda_l} (\lambda,\varepsilon) \right] </span>
<span id="cb27-1395"><a href="#cb27-1395" aria-hidden="true" tabindex="-1"></a><span class="ss">        + </span>\mathcal{G}_l, </span>
<span id="cb27-1396"><a href="#cb27-1396" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb27-1397"><a href="#cb27-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1398"><a href="#cb27-1398" aria-hidden="true" tabindex="-1"></a>where </span>
<span id="cb27-1399"><a href="#cb27-1399" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1400"><a href="#cb27-1400" aria-hidden="true" tabindex="-1"></a>    \mathcal{G}_l=\mathbb{E}_{\theta\sim\pi_\lambda}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|\theta)}\left[\frac{1}{L_N(\mathbf{X}|\theta)}\frac{\partial p_\lambda}{\partial \lambda_l}(\mathbf{X})f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|\theta))} \right) \right].</span>
<span id="cb27-1401"><a href="#cb27-1401" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1402"><a href="#cb27-1402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1403"><a href="#cb27-1403" aria-hidden="true" tabindex="-1"></a>We remark that</span>
<span id="cb27-1404"><a href="#cb27-1404" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1405"><a href="#cb27-1405" aria-hidden="true" tabindex="-1"></a>    \frac{\partial p_\lambda}{\partial \lambda_l} (\mathbf{X}) = \mathbb{E}_{\varepsilon_2} \sum_{j=1}^q\frac{\partial L_N}{\partial\theta_j} (\mathbf{X}|g(\lambda,\varepsilon_2))\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2).</span>
<span id="cb27-1406"><a href="#cb27-1406" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1407"><a href="#cb27-1407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1408"><a href="#cb27-1408" aria-hidden="true" tabindex="-1"></a>Thus, we can develop $\mathcal{G}_l$ as:</span>
<span id="cb27-1409"><a href="#cb27-1409" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1410"><a href="#cb27-1410" aria-hidden="true" tabindex="-1"></a> \begin{aligned}</span>
<span id="cb27-1411"><a href="#cb27-1411" aria-hidden="true" tabindex="-1"></a>        \mathcal{G}_l = &amp;\mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\mathbb{E}_{\varepsilon_2}\sum_{j}\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right)\frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)) \frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2)<span class="sc">\\</span></span>
<span id="cb27-1412"><a href="#cb27-1412" aria-hidden="true" tabindex="-1"></a>        =&amp; \mathbb{E}_{\varepsilon_2}\mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\sum_{j}\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right)\frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)) \frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2)<span class="sc">\\</span></span>
<span id="cb27-1413"><a href="#cb27-1413" aria-hidden="true" tabindex="-1"></a>         =&amp; \mathbb{E}_{\varepsilon_2}\sum_{j=1}^q\frac{\partial g_j}{\partial \lambda_l}(\lambda,\varepsilon_2) \mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))} \frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))} f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right) \frac{\partial L_N}{\partial\theta_j}(\mathbf{X}|g(\lambda,\varepsilon_2)).</span>
<span id="cb27-1414"><a href="#cb27-1414" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb27-1415"><a href="#cb27-1415" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1416"><a href="#cb27-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1417"><a href="#cb27-1417" aria-hidden="true" tabindex="-1"></a>Now, calling $\tilde K$ the function defined as follows:</span>
<span id="cb27-1418"><a href="#cb27-1418" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1419"><a href="#cb27-1419" aria-hidden="true" tabindex="-1"></a>    \tilde K:\theta\mapsto\tilde K(\theta) = \mathbb{E}_{\varepsilon_1}\mathbb{E}_{\mathbf{X}\sim L_N(\cdot|g(\lambda,\varepsilon_1))}\left[\frac{1}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}f'\left(\frac{p_\lambda(\mathbf{X})}{L_N(\mathbf{X}|g(\lambda,\varepsilon_1))}  \right) L_N(\mathbf{X}|\theta)\right],</span>
<span id="cb27-1420"><a href="#cb27-1420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1421"><a href="#cb27-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1422"><a href="#cb27-1422" aria-hidden="true" tabindex="-1"></a>we obtain that</span>
<span id="cb27-1423"><a href="#cb27-1423" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1424"><a href="#cb27-1424" aria-hidden="true" tabindex="-1"></a>    \mathcal{G}_l = \mathbb{E}_{\varepsilon_2}\sum_{j=1}^q\frac{\partial g_j}{\partial\lambda_l}(\lambda,\varepsilon_2)\frac{\partial \tilde K}{\partial\theta_j} (g(\lambda,\varepsilon_2)). </span>
<span id="cb27-1425"><a href="#cb27-1425" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1426"><a href="#cb27-1426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1427"><a href="#cb27-1427" aria-hidden="true" tabindex="-1"></a>Eventually, denoting $\tilde{\mathbf{I}}=\tilde K+\tilde I$, we have:</span>
<span id="cb27-1428"><a href="#cb27-1428" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1429"><a href="#cb27-1429" aria-hidden="true" tabindex="-1"></a>    \frac{\partial I_{D_f}}{\partial \lambda_l}(\pi_\lambda;L_N) = \mathbb{E}_\varepsilon\left[\sum_{j=1}^q\frac{\partial\tilde{\mathbf{I}}}{\partial\theta_j}(g(\lambda,\varepsilon))\frac{\partial g_j}{\partial\lambda_l}(\lambda,\varepsilon)\right],</span>
<span id="cb27-1430"><a href="#cb27-1430" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1431"><a href="#cb27-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1432"><a href="#cb27-1432" aria-hidden="true" tabindex="-1"></a>which is compatible with the form of @eq-compatible_objective_function.</span>
<span id="cb27-1433"><a href="#cb27-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1434"><a href="#cb27-1434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1435"><a href="#cb27-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1436"><a href="#cb27-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1437"><a href="#cb27-1437" aria-hidden="true" tabindex="-1"></a><span class="fu">## Gaussian distribution with variance parameter</span></span>
<span id="cb27-1438"><a href="#cb27-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1439"><a href="#cb27-1439" aria-hidden="true" tabindex="-1"></a>We consider a normal distribution where $\theta$ is the variance parameter :  $X_i \sim \mathcal{N}(\mu,\theta)$ with $\mu \in \mathbb{R}$, $\bX \in \mathcal{X}^N = \mathbb{R}^N$ and $\theta \in \mathbb{R}^*_{+}$. We take $\mu=0$. The likelihood and score functions are :</span>
<span id="cb27-1440"><a href="#cb27-1440" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb27-1441"><a href="#cb27-1441" aria-hidden="true" tabindex="-1"></a>L_N(\bX\,|\,\theta) = \prod_{i=1}^N \frac{1}{\sqrt{2\pi \theta}} \exp \left( -\frac{1}{2\theta}(X_i - \mu)^2 \right) </span>
<span id="cb27-1442"><a href="#cb27-1442" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1443"><a href="#cb27-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1444"><a href="#cb27-1444" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1445"><a href="#cb27-1445" aria-hidden="true" tabindex="-1"></a> \frac{\partial \log L_N}{\partial \theta}(\bX\,|\,\theta)  = - \frac{N}{2\theta} + \frac{1}{2\theta^2} \sum_{i=1}^N (X_i - \mu)^2.</span>
<span id="cb27-1446"><a href="#cb27-1446" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1447"><a href="#cb27-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1448"><a href="#cb27-1448" aria-hidden="true" tabindex="-1"></a>The MLE is available : $\hat{\theta}_{MLE} = \frac{1}{N} \sum_{i=1}^N X_i$. However, the Jeffreys prior is an improper distribution in this case : $J(\theta) \propto 1/\theta$. Nevertheless, the Jeffreys posterior is a proper inverse-gamma distribution:</span>
<span id="cb27-1449"><a href="#cb27-1449" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1450"><a href="#cb27-1450" aria-hidden="true" tabindex="-1"></a>J_{post}(\theta \, | \ \bX) = \Gamma^{-1} \left(\theta; \frac{N}{2}, \frac{1}{2} \sum_{i=1}^N(X_i - \mu)^2  \right).</span>
<span id="cb27-1451"><a href="#cb27-1451" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1452"><a href="#cb27-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1453"><a href="#cb27-1453" aria-hidden="true" tabindex="-1"></a>We use a neural network with one layer and a Softplus activation function. The dimension of the latent variable $\varepsilon$ is $p=10$.</span>
<span id="cb27-1454"><a href="#cb27-1454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1455"><a href="#cb27-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1456"><a href="#cb27-1456" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-1457"><a href="#cb27-1457" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameters and classes</span></span>
<span id="cb27-1458"><a href="#cb27-1458" aria-hidden="true" tabindex="-1"></a><span class="in">p = 10    # latent space dimension</span></span>
<span id="cb27-1459"><a href="#cb27-1459" aria-hidden="true" tabindex="-1"></a><span class="in">q = 1     # theta dimension</span></span>
<span id="cb27-1460"><a href="#cb27-1460" aria-hidden="true" tabindex="-1"></a><span class="in">mu = 0    # normal parameter (mean)</span></span>
<span id="cb27-1461"><a href="#cb27-1461" aria-hidden="true" tabindex="-1"></a><span class="in">N = 10    # number of data samples</span></span>
<span id="cb27-1462"><a href="#cb27-1462" aria-hidden="true" tabindex="-1"></a><span class="in">J = 1000  # nb samples for MC estimation in MI</span></span>
<span id="cb27-1463"><a href="#cb27-1463" aria-hidden="true" tabindex="-1"></a><span class="in">T = 50    # nb samples MC marginal likelihood</span></span>
<span id="cb27-1464"><a href="#cb27-1464" aria-hidden="true" tabindex="-1"></a><span class="in">Normal = torch_NormalModel_variance(mu=mu)</span></span>
<span id="cb27-1465"><a href="#cb27-1465" aria-hidden="true" tabindex="-1"></a><span class="in">low = 0.0001        # lower bound</span></span>
<span id="cb27-1466"><a href="#cb27-1466" aria-hidden="true" tabindex="-1"></a><span class="in">upp = 1 + low      # upper bound (not relevant here)</span></span>
<span id="cb27-1467"><a href="#cb27-1467" aria-hidden="true" tabindex="-1"></a><span class="in">input_size = p</span></span>
<span id="cb27-1468"><a href="#cb27-1468" aria-hidden="true" tabindex="-1"></a><span class="in">output_size = 1</span></span>
<span id="cb27-1469"><a href="#cb27-1469" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**5</span></span>
<span id="cb27-1470"><a href="#cb27-1470" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Normal_results_unconstrained.pkl'</span></span>
<span id="cb27-1471"><a href="#cb27-1471" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-1472"><a href="#cb27-1472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1473"><a href="#cb27-1473" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(0)</span></span>
<span id="cb27-1474"><a href="#cb27-1474" aria-hidden="true" tabindex="-1"></a><span class="in">NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=nn.Softplus())</span></span>
<span id="cb27-1475"><a href="#cb27-1475" aria-hidden="true" tabindex="-1"></a><span class="in">NN = AffineTransformation(NN, m_low=low, M_upp=upp)</span></span>
<span id="cb27-1476"><a href="#cb27-1476" aria-hidden="true" tabindex="-1"></a><span class="in">VA = VA_NeuralNet(neural_net=NN, model=Normal)</span></span>
<span id="cb27-1477"><a href="#cb27-1477" aria-hidden="true" tabindex="-1"></a><span class="in">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb27-1478"><a href="#cb27-1478" aria-hidden="true" tabindex="-1"></a><span class="in">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb27-1479"><a href="#cb27-1479" aria-hidden="true" tabindex="-1"></a><span class="in">Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=0.5, use_log_lik=True, use_baseline=False)</span></span>
<span id="cb27-1480"><a href="#cb27-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1481"><a href="#cb27-1481" aria-hidden="true" tabindex="-1"></a><span class="in">with torch.no_grad():</span></span>
<span id="cb27-1482"><a href="#cb27-1482" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample_init = Div.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-1483"><a href="#cb27-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1484"><a href="#cb27-1484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1485"><a href="#cb27-1485" aria-hidden="true" tabindex="-1"></a><span class="in">num_epochs = 7000</span></span>
<span id="cb27-1486"><a href="#cb27-1486" aria-hidden="true" tabindex="-1"></a><span class="in">loss_fct = "LB_MI"</span></span>
<span id="cb27-1487"><a href="#cb27-1487" aria-hidden="true" tabindex="-1"></a><span class="in">optimizer = torch_Adam</span></span>
<span id="cb27-1488"><a href="#cb27-1488" aria-hidden="true" tabindex="-1"></a><span class="in">num_samples_MI = 200</span></span>
<span id="cb27-1489"><a href="#cb27-1489" aria-hidden="true" tabindex="-1"></a><span class="in">freq_MI = 100</span></span>
<span id="cb27-1490"><a href="#cb27-1490" aria-hidden="true" tabindex="-1"></a><span class="in">save_best_param = True</span></span>
<span id="cb27-1491"><a href="#cb27-1491" aria-hidden="true" tabindex="-1"></a><span class="in">learning_rate = 0.025</span></span>
<span id="cb27-1492"><a href="#cb27-1492" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1493"><a href="#cb27-1493" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_Normal_nocstr :</span></span>
<span id="cb27-1494"><a href="#cb27-1494" aria-hidden="true" tabindex="-1"></a><span class="in">    ### Training loop</span></span>
<span id="cb27-1495"><a href="#cb27-1495" aria-hidden="true" tabindex="-1"></a><span class="in">    MI, range_MI, lower_MI, upper_MI = Div.Partial_autograd(J, N, num_epochs, loss_fct, optimizer, </span></span>
<span id="cb27-1496"><a href="#cb27-1496" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            num_samples_MI, freq_MI, save_best_param, </span></span>
<span id="cb27-1497"><a href="#cb27-1497" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            learning_rate, momentum=True)</span></span>
<span id="cb27-1498"><a href="#cb27-1498" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1499"><a href="#cb27-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1500"><a href="#cb27-1500" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_sample = Div.va.implicit_prior_sampler(n_samples_prior)</span></span>
<span id="cb27-1501"><a href="#cb27-1501" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1502"><a href="#cb27-1502" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample = theta_sample.numpy()</span></span>
<span id="cb27-1503"><a href="#cb27-1503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1504"><a href="#cb27-1504" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-1505"><a href="#cb27-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1506"><a href="#cb27-1506" aria-hidden="true" tabindex="-1"></a><span class="in">    # Sample data from 'true' parameter</span></span>
<span id="cb27-1507"><a href="#cb27-1507" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_true = torch.tensor(1.)  </span></span>
<span id="cb27-1508"><a href="#cb27-1508" aria-hidden="true" tabindex="-1"></a><span class="in">    N = 10</span></span>
<span id="cb27-1509"><a href="#cb27-1509" aria-hidden="true" tabindex="-1"></a><span class="in">    D = Normal.sample(theta_true, N, 1)</span></span>
<span id="cb27-1510"><a href="#cb27-1510" aria-hidden="true" tabindex="-1"></a><span class="in">    X = D[:, 0]</span></span>
<span id="cb27-1511"><a href="#cb27-1511" aria-hidden="true" tabindex="-1"></a><span class="in">    # Posterior samples using Jeffreys prior</span></span>
<span id="cb27-1512"><a href="#cb27-1512" aria-hidden="true" tabindex="-1"></a><span class="in">    n_samples_post = 5*10**4</span></span>
<span id="cb27-1513"><a href="#cb27-1513" aria-hidden="true" tabindex="-1"></a><span class="in">    jeffreys_post = Normal.sample_post_Jeffreys(X, n_samples_post)</span></span>
<span id="cb27-1514"><a href="#cb27-1514" aria-hidden="true" tabindex="-1"></a><span class="in">    jeffreys_post = np.reshape(jeffreys_post, (n_samples_post,q))</span></span>
<span id="cb27-1515"><a href="#cb27-1515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1516"><a href="#cb27-1516" aria-hidden="true" tabindex="-1"></a><span class="in">    T_mcmc = 10**5 + 1</span></span>
<span id="cb27-1517"><a href="#cb27-1517" aria-hidden="true" tabindex="-1"></a><span class="in">    sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-1518"><a href="#cb27-1518" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_0 = torch.randn(p)</span></span>
<span id="cb27-1519"><a href="#cb27-1519" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap=True, Cov=False)</span></span>
<span id="cb27-1520"><a href="#cb27-1520" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_MH = NN(eps_MH)</span></span>
<span id="cb27-1521"><a href="#cb27-1521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1522"><a href="#cb27-1522" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1523"><a href="#cb27-1523" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = theta_MH.numpy()</span></span>
<span id="cb27-1524"><a href="#cb27-1524" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_post = jeffreys_post.numpy()</span></span>
<span id="cb27-1525"><a href="#cb27-1525" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post = theta_MH[-n_samples_post:]</span></span>
<span id="cb27-1526"><a href="#cb27-1526" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post = np.reshape(theta_post, (n_samples_post,q))</span></span>
<span id="cb27-1527"><a href="#cb27-1527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1528"><a href="#cb27-1528" aria-hidden="true" tabindex="-1"></a><span class="in">    # Saves all relevant quantities</span></span>
<span id="cb27-1529"><a href="#cb27-1529" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1530"><a href="#cb27-1530" aria-hidden="true" tabindex="-1"></a><span class="in">    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI}</span></span>
<span id="cb27-1531"><a href="#cb27-1531" aria-hidden="true" tabindex="-1"></a><span class="in">    Prior = {'samples' : theta_sample, 'jeffreys' : jeffreys_sample, 'params' : all_params}</span></span>
<span id="cb27-1532"><a href="#cb27-1532" aria-hidden="true" tabindex="-1"></a><span class="in">    Posterior = {'samples' : theta_post, 'jeffreys' : jeffreys_post, 'Markov' : theta_MH}</span></span>
<span id="cb27-1533"><a href="#cb27-1533" aria-hidden="true" tabindex="-1"></a><span class="in">    Normal_results_nocstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}</span></span>
<span id="cb27-1534"><a href="#cb27-1534" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-1535"><a href="#cb27-1535" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(Normal_results_nocstr, file)</span></span>
<span id="cb27-1536"><a href="#cb27-1536" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1537"><a href="#cb27-1537" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-1538"><a href="#cb27-1538" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1539"><a href="#cb27-1539" aria-hidden="true" tabindex="-1"></a><span class="in">        res = pickle.load(file)</span></span>
<span id="cb27-1540"><a href="#cb27-1540" aria-hidden="true" tabindex="-1"></a><span class="in">        MI_dic = res['MI']</span></span>
<span id="cb27-1541"><a href="#cb27-1541" aria-hidden="true" tabindex="-1"></a><span class="in">        Prior = res['prior'] </span></span>
<span id="cb27-1542"><a href="#cb27-1542" aria-hidden="true" tabindex="-1"></a><span class="in">        Posterior = res['post']</span></span>
<span id="cb27-1543"><a href="#cb27-1543" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper']</span></span>
<span id="cb27-1544"><a href="#cb27-1544" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']</span></span>
<span id="cb27-1545"><a href="#cb27-1545" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post, jeffreys_post, theta_MH = Posterior['samples'], Posterior['jeffreys'], Posterior['Markov']</span></span>
<span id="cb27-1546"><a href="#cb27-1546" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1547"><a href="#cb27-1547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1548"><a href="#cb27-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1549"><a href="#cb27-1549" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1550"><a href="#cb27-1550" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-normal_prior</span></span>
<span id="cb27-1551"><a href="#cb27-1551" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Left : Monte Carlo estimation of the generalized mutual information with $\\alpha=0.5$ (from 200 samples) for $\\pi_{\\lambda_e}$ where $\\lambda_e$ is the parameter of the neural network at epoch $e$. The red curve is the mean value and the gray zone is the 95\\% confidence interval. Right : Histograms of the initial prior (at epoch 0) and the fitted prior (after training), each one is obtained from $10^5$ samples. The learning rate used in the optimization is $0.025$."</span></span>
<span id="cb27-1552"><a href="#cb27-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1553"><a href="#cb27-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1554"><a href="#cb27-1554" aria-hidden="true" tabindex="-1"></a><span class="in">fig, axs = plt.subplots(1, 2, figsize=(12, 3))</span></span>
<span id="cb27-1555"><a href="#cb27-1555" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].plot(range_MI, MI, '-', color=rougeCEA)</span></span>
<span id="cb27-1556"><a href="#cb27-1556" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].plot(range_MI, MI, '*', color=rougeCEA)</span></span>
<span id="cb27-1557"><a href="#cb27-1557" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].plot(range_MI, upper_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1558"><a href="#cb27-1558" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].plot(range_MI, lower_MI, '-', color='lightgrey')</span></span>
<span id="cb27-1559"><a href="#cb27-1559" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].fill_between(range_MI, lower_MI, upper_MI, color='lightgrey', alpha=0.5)</span></span>
<span id="cb27-1560"><a href="#cb27-1560" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].set_xlabel(r"Epochs")</span></span>
<span id="cb27-1561"><a href="#cb27-1561" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].set_ylabel(r"Generalized mutual information")</span></span>
<span id="cb27-1562"><a href="#cb27-1562" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].grid()</span></span>
<span id="cb27-1563"><a href="#cb27-1563" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].hist(theta_sample_init, density=True, bins="rice", color="red", label=r"Initial prior", alpha=0.4)</span></span>
<span id="cb27-1564"><a href="#cb27-1564" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].hist(theta_sample, density=True, bins="rice", label=r"Fitted prior", alpha=0.4)</span></span>
<span id="cb27-1565"><a href="#cb27-1565" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].set_xlabel(r"$\theta$")</span></span>
<span id="cb27-1566"><a href="#cb27-1566" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].legend(fontsize=12)</span></span>
<span id="cb27-1567"><a href="#cb27-1567" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].grid()</span></span>
<span id="cb27-1568"><a href="#cb27-1568" aria-hidden="true" tabindex="-1"></a><span class="in">#plt.tight_layout()</span></span>
<span id="cb27-1569"><a href="#cb27-1569" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1570"><a href="#cb27-1570" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1571"><a href="#cb27-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1572"><a href="#cb27-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1573"><a href="#cb27-1573" aria-hidden="true" tabindex="-1"></a>We retrieve close results to those of @gauchy_var_rp, even though we used the $\alpha$-divergence instead of the classic KL-divergence (@fig-normal_prior). The evolution of the mutual information seems to be more stable during training.  We can not however directly compare our result to the target Jeffrey prior since the latter is improper.</span>
<span id="cb27-1574"><a href="#cb27-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1575"><a href="#cb27-1575" aria-hidden="true" tabindex="-1"></a>For the posterior distribution, we sample 10 times from the normal distribution using $\theta_{true} = 1$.</span>
<span id="cb27-1576"><a href="#cb27-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1577"><a href="#cb27-1577" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1578"><a href="#cb27-1578" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-normal_post</span></span>
<span id="cb27-1579"><a href="#cb27-1579" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Left : Markov chain during the Metropolis-Hastings iterations. Right : Histograms of the fitted posterior and the Jeffreys posterior, each one is obtained from $5\\cdot10^4$ samples."</span></span>
<span id="cb27-1580"><a href="#cb27-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1581"><a href="#cb27-1581" aria-hidden="true" tabindex="-1"></a><span class="in">fig, axs = plt.subplots(1, 2, figsize=(12, 3))</span></span>
<span id="cb27-1582"><a href="#cb27-1582" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].plot(theta_MH)</span></span>
<span id="cb27-1583"><a href="#cb27-1583" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].set_xlabel('Iterations')</span></span>
<span id="cb27-1584"><a href="#cb27-1584" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].set_ylabel(r'$\theta$')</span></span>
<span id="cb27-1585"><a href="#cb27-1585" aria-hidden="true" tabindex="-1"></a><span class="in">axs[0].grid()</span></span>
<span id="cb27-1586"><a href="#cb27-1586" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].hist(theta_post, density=True, bins="rice", label=r"Fitted posterior", alpha=0.4)</span></span>
<span id="cb27-1587"><a href="#cb27-1587" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].hist(jeffreys_post, density=True, bins="rice", label=r"Jeffreys", alpha=0.4, color=vertCEA)</span></span>
<span id="cb27-1588"><a href="#cb27-1588" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].set_xlabel(r"$\theta$")</span></span>
<span id="cb27-1589"><a href="#cb27-1589" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].legend(fontsize=12)</span></span>
<span id="cb27-1590"><a href="#cb27-1590" aria-hidden="true" tabindex="-1"></a><span class="in">axs[1].grid()</span></span>
<span id="cb27-1591"><a href="#cb27-1591" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1592"><a href="#cb27-1592" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1593"><a href="#cb27-1593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1594"><a href="#cb27-1594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1595"><a href="#cb27-1595" aria-hidden="true" tabindex="-1"></a>As @fig-normal_post shows, we obtain a parametric posterior distribution which closely resembles the target distribution, even though the theoretical prior is improper.</span>
<span id="cb27-1596"><a href="#cb27-1596" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1597"><a href="#cb27-1597" aria-hidden="true" tabindex="-1"></a>In order to evaluate the performance of the algorithm for the prior, we have to add a constraint. The simplest kind of constraints are moment constraints with : $a(\theta) = \theta^{\beta}$, however, we can not use such a constraint here since the integrals for $\mathcal{K}$ and $c$ from section 2 would diverge either at $0$ or at $+\infty$.</span>
<span id="cb27-1598"><a href="#cb27-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1599"><a href="#cb27-1599" aria-hidden="true" tabindex="-1"></a>If we define : $a(\theta) = \displaystyle \frac{1}{\theta^{\beta}+\theta^{\tau}}$ with $\beta &lt; 0 &lt; \tau$,</span>
<span id="cb27-1600"><a href="#cb27-1600" aria-hidden="true" tabindex="-1"></a>then the integrals for $\mathcal{K}$ and $c$ are finite, because : </span>
<span id="cb27-1601"><a href="#cb27-1601" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1602"><a href="#cb27-1602" aria-hidden="true" tabindex="-1"></a> \forall \, \delta \geq 1, \quad \int_0^{+\infty} \frac{1}{\theta}\cdot \left(\frac{1}{\theta^{\beta}+\theta^{\tau}} \right)^{\delta} d\theta \leq \frac{1}{\delta}\left( \frac{1}{\tau} - \frac{1}{\beta}\right) . </span>
<span id="cb27-1603"><a href="#cb27-1603" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1604"><a href="#cb27-1604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1605"><a href="#cb27-1605" aria-hidden="true" tabindex="-1"></a>This function of constraint $a$ is preferable because it yields different asymptotic rates at $0$ and $+\infty$ :</span>
<span id="cb27-1606"><a href="#cb27-1606" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1607"><a href="#cb27-1607" aria-hidden="true" tabindex="-1"></a>   \begin{cases} \displaystyle</span>
<span id="cb27-1608"><a href="#cb27-1608" aria-hidden="true" tabindex="-1"></a>a(\theta) \sim \theta^{-\beta} \quad \text{as} \quad  \theta \longrightarrow 0 <span class="sc">\\</span></span>
<span id="cb27-1609"><a href="#cb27-1609" aria-hidden="true" tabindex="-1"></a>a(\theta) \sim \theta^{-\tau} \quad \text{as} \quad  \theta \longrightarrow +\infty.</span>
<span id="cb27-1610"><a href="#cb27-1610" aria-hidden="true" tabindex="-1"></a>\end{cases} </span>
<span id="cb27-1611"><a href="#cb27-1611" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1612"><a href="#cb27-1612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1613"><a href="#cb27-1613" aria-hidden="true" tabindex="-1"></a>In order to apply the algorithm, we are interested in finding : </span>
<span id="cb27-1614"><a href="#cb27-1614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb27-1615"><a href="#cb27-1615" aria-hidden="true" tabindex="-1"></a> \mathcal{K} = \int_0^{+\infty} \frac{1}{\theta}\cdot a(\theta)^{1/\alpha} d\theta \quad \text{and} \quad  c = \int_0^{+\infty} \frac{1}{\theta}\cdot a(\theta)^{1+(1/\alpha)} d\theta. </span>
<span id="cb27-1616"><a href="#cb27-1616" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb27-1617"><a href="#cb27-1617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1618"><a href="#cb27-1618" aria-hidden="true" tabindex="-1"></a>For instance, let $\alpha=1/2$. If $\beta=-1$, $\tau=1$, then $\mathcal{K} = 1/2$ and $c = \pi/16$. The constraint value is $c/\mathcal{K} = \pi/8$. Thus, for this example, we only have to apply the third step of the proposed method. We use in this case a one-layer neural network with $\exp$ as the activation function, the parametric set of priors corresponds to log-normal distributions.</span>
<span id="cb27-1619"><a href="#cb27-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1620"><a href="#cb27-1620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1621"><a href="#cb27-1621" aria-hidden="true" tabindex="-1"></a><span class="in">```{python python-code}</span></span>
<span id="cb27-1622"><a href="#cb27-1622" aria-hidden="true" tabindex="-1"></a><span class="in"># Parameters and classes</span></span>
<span id="cb27-1623"><a href="#cb27-1623" aria-hidden="true" tabindex="-1"></a><span class="in">p = 10     # latent space dimension</span></span>
<span id="cb27-1624"><a href="#cb27-1624" aria-hidden="true" tabindex="-1"></a><span class="in">q = 1      # theta dimension</span></span>
<span id="cb27-1625"><a href="#cb27-1625" aria-hidden="true" tabindex="-1"></a><span class="in">mu = 0     # normal mean parameter</span></span>
<span id="cb27-1626"><a href="#cb27-1626" aria-hidden="true" tabindex="-1"></a><span class="in">N = 10     # number of data samples</span></span>
<span id="cb27-1627"><a href="#cb27-1627" aria-hidden="true" tabindex="-1"></a><span class="in">J = 1000   # nb samples for MC estimation in MI</span></span>
<span id="cb27-1628"><a href="#cb27-1628" aria-hidden="true" tabindex="-1"></a><span class="in">T = 50     # nb samples MC marginal likelihood</span></span>
<span id="cb27-1629"><a href="#cb27-1629" aria-hidden="true" tabindex="-1"></a><span class="in">alpha = 0.5</span></span>
<span id="cb27-1630"><a href="#cb27-1630" aria-hidden="true" tabindex="-1"></a><span class="in">Normal = torch_NormalModel_variance(mu=mu)  </span></span>
<span id="cb27-1631"><a href="#cb27-1631" aria-hidden="true" tabindex="-1"></a><span class="in">input_size = p  </span></span>
<span id="cb27-1632"><a href="#cb27-1632" aria-hidden="true" tabindex="-1"></a><span class="in">output_size = 1</span></span>
<span id="cb27-1633"><a href="#cb27-1633" aria-hidden="true" tabindex="-1"></a><span class="in">low = 0.0001        # lower bound</span></span>
<span id="cb27-1634"><a href="#cb27-1634" aria-hidden="true" tabindex="-1"></a><span class="in">upp = 1 + low      # upper bound (not relevant)</span></span>
<span id="cb27-1635"><a href="#cb27-1635" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**6</span></span>
<span id="cb27-1636"><a href="#cb27-1636" aria-hidden="true" tabindex="-1"></a><span class="in">name_file = 'Normal_results_constrained.pkl'</span></span>
<span id="cb27-1637"><a href="#cb27-1637" aria-hidden="true" tabindex="-1"></a><span class="in">file_path = os.path.join(path_plot, name_file)</span></span>
<span id="cb27-1638"><a href="#cb27-1638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1639"><a href="#cb27-1639" aria-hidden="true" tabindex="-1"></a><span class="in"># Constraint function parameter</span></span>
<span id="cb27-1640"><a href="#cb27-1640" aria-hidden="true" tabindex="-1"></a><span class="in">beta = torch.tensor([-1])</span></span>
<span id="cb27-1641"><a href="#cb27-1641" aria-hidden="true" tabindex="-1"></a><span class="in">tau = torch.tensor([1])</span></span>
<span id="cb27-1642"><a href="#cb27-1642" aria-hidden="true" tabindex="-1"></a><span class="in"># beta = -tau = -1, yields K = 0.5</span></span>
<span id="cb27-1643"><a href="#cb27-1643" aria-hidden="true" tabindex="-1"></a><span class="in"># beta = -tau = -1/3, yields K = 1.5</span></span>
<span id="cb27-1644"><a href="#cb27-1644" aria-hidden="true" tabindex="-1"></a><span class="in">K = 0.5</span></span>
<span id="cb27-1645"><a href="#cb27-1645" aria-hidden="true" tabindex="-1"></a><span class="in">constr_val = np.pi / 8</span></span>
<span id="cb27-1646"><a href="#cb27-1646" aria-hidden="true" tabindex="-1"></a><span class="in">b = torch.tensor([[constr_val]]) </span></span>
<span id="cb27-1647"><a href="#cb27-1647" aria-hidden="true" tabindex="-1"></a><span class="in">T_cstr = 100000</span></span>
<span id="cb27-1648"><a href="#cb27-1648" aria-hidden="true" tabindex="-1"></a><span class="in">eta_augm = torch.tensor([[1.0]])</span></span>
<span id="cb27-1649"><a href="#cb27-1649" aria-hidden="true" tabindex="-1"></a><span class="in">eta = torch.tensor([[1.]])</span></span>
<span id="cb27-1650"><a href="#cb27-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1651"><a href="#cb27-1651" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(0)</span></span>
<span id="cb27-1652"><a href="#cb27-1652" aria-hidden="true" tabindex="-1"></a><span class="in">NN = SingleLinear(input_size, output_size, m1=0, s1=0.1, b1=0, act1=torch.exp)</span></span>
<span id="cb27-1653"><a href="#cb27-1653" aria-hidden="true" tabindex="-1"></a><span class="in">NN = AffineTransformation(NN, m_low=low, M_upp=upp)</span></span>
<span id="cb27-1654"><a href="#cb27-1654" aria-hidden="true" tabindex="-1"></a><span class="in">VA = VA_NeuralNet(neural_net=NN, model=Normal)</span></span>
<span id="cb27-1655"><a href="#cb27-1655" aria-hidden="true" tabindex="-1"></a><span class="in">#print(f'Number of parameters : {VA.nb_param}')</span></span>
<span id="cb27-1656"><a href="#cb27-1656" aria-hidden="true" tabindex="-1"></a><span class="in">#Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=False, use_log_lik=True)</span></span>
<span id="cb27-1657"><a href="#cb27-1657" aria-hidden="true" tabindex="-1"></a><span class="in">Div = DivMetric_NeuralNet(va=VA, T=T, use_alpha=True, alpha=alpha, use_log_lik=True)</span></span>
<span id="cb27-1658"><a href="#cb27-1658" aria-hidden="true" tabindex="-1"></a><span class="in">Constr = Constraints_NeuralNet(div=Div, betas=beta, b=b, T_cstr=T_cstr, </span></span>
<span id="cb27-1659"><a href="#cb27-1659" aria-hidden="true" tabindex="-1"></a><span class="in">                               objective='LB_MI', lag_method='augmented', eta_augm=eta_augm, rule='SGD',moment='alter', taus=tau)</span></span>
<span id="cb27-1660"><a href="#cb27-1660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1661"><a href="#cb27-1661" aria-hidden="true" tabindex="-1"></a><span class="in">num_epochs = 10000</span></span>
<span id="cb27-1662"><a href="#cb27-1662" aria-hidden="true" tabindex="-1"></a><span class="in">optimizer = torch_Adam</span></span>
<span id="cb27-1663"><a href="#cb27-1663" aria-hidden="true" tabindex="-1"></a><span class="in">num_samples_MI = 100</span></span>
<span id="cb27-1664"><a href="#cb27-1664" aria-hidden="true" tabindex="-1"></a><span class="in">freq_MI = 200</span></span>
<span id="cb27-1665"><a href="#cb27-1665" aria-hidden="true" tabindex="-1"></a><span class="in">save_best_param = False</span></span>
<span id="cb27-1666"><a href="#cb27-1666" aria-hidden="true" tabindex="-1"></a><span class="in">learning_rate = 0.0005</span></span>
<span id="cb27-1667"><a href="#cb27-1667" aria-hidden="true" tabindex="-1"></a><span class="in">freq_augm = 100</span></span>
<span id="cb27-1668"><a href="#cb27-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1669"><a href="#cb27-1669" aria-hidden="true" tabindex="-1"></a><span class="in">def target_prior(theta, beta, tau, K, alpha):</span></span>
<span id="cb27-1670"><a href="#cb27-1670" aria-hidden="true" tabindex="-1"></a><span class="in">    return (K**-1 / theta) * (theta**beta + theta**tau)**(-1/alpha)</span></span>
<span id="cb27-1671"><a href="#cb27-1671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1672"><a href="#cb27-1672" aria-hidden="true" tabindex="-1"></a><span class="in">if not load_results_Normal_cstr :</span></span>
<span id="cb27-1673"><a href="#cb27-1673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1674"><a href="#cb27-1674" aria-hidden="true" tabindex="-1"></a><span class="in">    ### Training loop</span></span>
<span id="cb27-1675"><a href="#cb27-1675" aria-hidden="true" tabindex="-1"></a><span class="in">    MI, constr_values, range_MI, lower_MI, upper_MI = Constr.Partial_autograd(J, N, eta, num_epochs, optimizer, num_samples_MI, </span></span>
<span id="cb27-1676"><a href="#cb27-1676" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            freq_MI, save_best_param, learning_rate, momentum=True,</span></span>
<span id="cb27-1677"><a href="#cb27-1677" aria-hidden="true" tabindex="-1"></a><span class="in">                                                            freq_augm=freq_augm, max_violation=0.005, update_eta_augm=2.)</span></span>
<span id="cb27-1678"><a href="#cb27-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1679"><a href="#cb27-1679" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1680"><a href="#cb27-1680" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample = Constr.va.implicit_prior_sampler(n_samples_prior).numpy()</span></span>
<span id="cb27-1681"><a href="#cb27-1681" aria-hidden="true" tabindex="-1"></a><span class="in">    print(f'Moment estimation : {np.mean((theta_sample**beta.item()+theta_sample**tau.item())**-1)}, wanted value : {b}')</span></span>
<span id="cb27-1682"><a href="#cb27-1682" aria-hidden="true" tabindex="-1"></a><span class="in">    all_params = torch.cat([param.view(-1) for param in NN.parameters()])</span></span>
<span id="cb27-1683"><a href="#cb27-1683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1684"><a href="#cb27-1684" aria-hidden="true" tabindex="-1"></a><span class="in">    seed_all(0)</span></span>
<span id="cb27-1685"><a href="#cb27-1685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1686"><a href="#cb27-1686" aria-hidden="true" tabindex="-1"></a><span class="in">    # Sample data from 'true' parameter</span></span>
<span id="cb27-1687"><a href="#cb27-1687" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_true = torch.tensor(1.)  </span></span>
<span id="cb27-1688"><a href="#cb27-1688" aria-hidden="true" tabindex="-1"></a><span class="in">    N = 10</span></span>
<span id="cb27-1689"><a href="#cb27-1689" aria-hidden="true" tabindex="-1"></a><span class="in">    D = Normal.sample(theta_true, N, 1)</span></span>
<span id="cb27-1690"><a href="#cb27-1690" aria-hidden="true" tabindex="-1"></a><span class="in">    X = D[:, 0]</span></span>
<span id="cb27-1691"><a href="#cb27-1691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1692"><a href="#cb27-1692" aria-hidden="true" tabindex="-1"></a><span class="in">    a_X = 0.5 * N</span></span>
<span id="cb27-1693"><a href="#cb27-1693" aria-hidden="true" tabindex="-1"></a><span class="in">    b_X = 0.5 * np.sum(X.numpy()**2)</span></span>
<span id="cb27-1694"><a href="#cb27-1694" aria-hidden="true" tabindex="-1"></a><span class="in">    Y = scipy.stats.invgamma.rvs(a_X, 0, b_X, 10**6)</span></span>
<span id="cb27-1695"><a href="#cb27-1695" aria-hidden="true" tabindex="-1"></a><span class="in">    cste_post = np.mean((Y**beta.item() + Y**tau.item())**(-1/alpha))</span></span>
<span id="cb27-1696"><a href="#cb27-1696" aria-hidden="true" tabindex="-1"></a><span class="in">    print(f'Normalizing constant : {cste_post}')</span></span>
<span id="cb27-1697"><a href="#cb27-1697" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1698"><a href="#cb27-1698" aria-hidden="true" tabindex="-1"></a><span class="in">    def target_posterior(theta, beta, tau, alpha, cste):</span></span>
<span id="cb27-1699"><a href="#cb27-1699" aria-hidden="true" tabindex="-1"></a><span class="in">        return scipy.stats.invgamma.pdf(theta, a_X, 0, b_X) * (theta**beta + theta**tau)**(-1/alpha) / cste</span></span>
<span id="cb27-1700"><a href="#cb27-1700" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1701"><a href="#cb27-1701" aria-hidden="true" tabindex="-1"></a><span class="in">    interv = np.linspace(0.01,10,10000)</span></span>
<span id="cb27-1702"><a href="#cb27-1702" aria-hidden="true" tabindex="-1"></a><span class="in">    target_post_values = target_posterior(interv,beta.item(),tau.item(),alpha,cste_post)</span></span>
<span id="cb27-1703"><a href="#cb27-1703" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1704"><a href="#cb27-1704" aria-hidden="true" tabindex="-1"></a><span class="in">    n_samples_post = 5*10**4</span></span>
<span id="cb27-1705"><a href="#cb27-1705" aria-hidden="true" tabindex="-1"></a><span class="in">    T_mcmc = 10**5 + 1</span></span>
<span id="cb27-1706"><a href="#cb27-1706" aria-hidden="true" tabindex="-1"></a><span class="in">    sigma2_0 = torch.tensor(1.)</span></span>
<span id="cb27-1707"><a href="#cb27-1707" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_0 = torch.randn(p)</span></span>
<span id="cb27-1708"><a href="#cb27-1708" aria-hidden="true" tabindex="-1"></a><span class="in">    eps_MH, batch_acc = VA.MH_posterior(eps_0, T_mcmc, sigma2_0, adap=True, Cov=False)</span></span>
<span id="cb27-1709"><a href="#cb27-1709" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_MH = NN(eps_MH)</span></span>
<span id="cb27-1710"><a href="#cb27-1710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1711"><a href="#cb27-1711" aria-hidden="true" tabindex="-1"></a><span class="in">    with torch.no_grad():</span></span>
<span id="cb27-1712"><a href="#cb27-1712" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_MH = theta_MH.numpy()</span></span>
<span id="cb27-1713"><a href="#cb27-1713" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post = theta_MH[-n_samples_post:]</span></span>
<span id="cb27-1714"><a href="#cb27-1714" aria-hidden="true" tabindex="-1"></a><span class="in">    theta_post = np.reshape(theta_post, n_samples_post)</span></span>
<span id="cb27-1715"><a href="#cb27-1715" aria-hidden="true" tabindex="-1"></a><span class="in">    # Saves all relevant quantities</span></span>
<span id="cb27-1716"><a href="#cb27-1716" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1717"><a href="#cb27-1717" aria-hidden="true" tabindex="-1"></a><span class="in">    Mutual_Info = {'values' : MI, 'range' : range_MI, 'lower' : lower_MI, 'upper' : upper_MI}</span></span>
<span id="cb27-1718"><a href="#cb27-1718" aria-hidden="true" tabindex="-1"></a><span class="in">    Prior = {'samples' : theta_sample, 'jeffreys' : jeffreys_sample, 'params' : all_params}</span></span>
<span id="cb27-1719"><a href="#cb27-1719" aria-hidden="true" tabindex="-1"></a><span class="in">    Posterior = {'samples' : theta_post, 'jeffreys' : jeffreys_post, 'target' : target_post_values}</span></span>
<span id="cb27-1720"><a href="#cb27-1720" aria-hidden="true" tabindex="-1"></a><span class="in">    Normal_results_cstr = {'MI' : Mutual_Info, 'prior' : Prior, 'post' : Posterior}</span></span>
<span id="cb27-1721"><a href="#cb27-1721" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'wb') as file:</span></span>
<span id="cb27-1722"><a href="#cb27-1722" aria-hidden="true" tabindex="-1"></a><span class="in">        pickle.dump(Normal_results_cstr, file)</span></span>
<span id="cb27-1723"><a href="#cb27-1723" aria-hidden="true" tabindex="-1"></a><span class="in">    </span></span>
<span id="cb27-1724"><a href="#cb27-1724" aria-hidden="true" tabindex="-1"></a><span class="in">else :</span></span>
<span id="cb27-1725"><a href="#cb27-1725" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1726"><a href="#cb27-1726" aria-hidden="true" tabindex="-1"></a><span class="in">        res = pickle.load(file)</span></span>
<span id="cb27-1727"><a href="#cb27-1727" aria-hidden="true" tabindex="-1"></a><span class="in">        MI_dic = res['MI']</span></span>
<span id="cb27-1728"><a href="#cb27-1728" aria-hidden="true" tabindex="-1"></a><span class="in">        Prior = res['prior'] </span></span>
<span id="cb27-1729"><a href="#cb27-1729" aria-hidden="true" tabindex="-1"></a><span class="in">        Posterior = res['post']</span></span>
<span id="cb27-1730"><a href="#cb27-1730" aria-hidden="true" tabindex="-1"></a><span class="in">        MI, range_MI, lower_MI, upper_MI = MI_dic['values'], MI_dic['range'], MI_dic['lower'], MI_dic['upper']</span></span>
<span id="cb27-1731"><a href="#cb27-1731" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_sample, jeffreys_sample, all_params = Prior['samples'], Prior['jeffreys'], Prior['params']</span></span>
<span id="cb27-1732"><a href="#cb27-1732" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post, jeffreys_post, target_post_values = Posterior['samples'], Posterior['jeffreys'], Posterior['target']</span></span>
<span id="cb27-1733"><a href="#cb27-1733" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1734"><a href="#cb27-1734" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1735"><a href="#cb27-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1736"><a href="#cb27-1736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1737"><a href="#cb27-1737" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1738"><a href="#cb27-1738" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-normal_prior_constr</span></span>
<span id="cb27-1739"><a href="#cb27-1739" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Histogram of the constrained fitted prior obtained from $10^5$ samples, and density function of the target prior. The learning rate used in the optimization is $0.0005$."</span></span>
<span id="cb27-1740"><a href="#cb27-1740" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1741"><a href="#cb27-1741" aria-hidden="true" tabindex="-1"></a><span class="in">interv = np.linspace(0.01,10,10000)</span></span>
<span id="cb27-1742"><a href="#cb27-1742" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1743"><a href="#cb27-1743" aria-hidden="true" tabindex="-1"></a><span class="in">plt.hist(theta_sample, density=True, bins=500, label=r"Fitted prior", alpha=0.4)</span></span>
<span id="cb27-1744"><a href="#cb27-1744" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(interv, target_prior(interv,beta.item(),tau.item(),K,alpha), label="Target prior",color=vertCEA)</span></span>
<span id="cb27-1745"><a href="#cb27-1745" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"$\theta$")</span></span>
<span id="cb27-1746"><a href="#cb27-1746" aria-hidden="true" tabindex="-1"></a><span class="in">plt.legend(fontsize=12)</span></span>
<span id="cb27-1747"><a href="#cb27-1747" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1748"><a href="#cb27-1748" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlim(-0.1,10)</span></span>
<span id="cb27-1749"><a href="#cb27-1749" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1750"><a href="#cb27-1750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1751"><a href="#cb27-1751" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1752"><a href="#cb27-1752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1753"><a href="#cb27-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1754"><a href="#cb27-1754" aria-hidden="true" tabindex="-1"></a>In this case we are able to compare prior distributions since both are proper, as @fig-normal_prior_constr shows, we recover a relevant result using our algorithm even with added constraints.</span>
<span id="cb27-1755"><a href="#cb27-1755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1756"><a href="#cb27-1756" aria-hidden="true" tabindex="-1"></a>The density function of the posterior is known up to a multiplicative constant, more precisely, it corresponds to the product of the constraint function and the density function of an inverse-gamma distribution. Hence, the constant can be estimated using Monte Carlo samples from the inverse-gamma distribution in question. We apply the same approach as before in order to obtain the posterior from the parametric prior.</span>
<span id="cb27-1757"><a href="#cb27-1757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1758"><a href="#cb27-1758" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1759"><a href="#cb27-1759" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1760"><a href="#cb27-1760" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-normal_post_constr</span></span>
<span id="cb27-1761"><a href="#cb27-1761" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Histogram of the fitted posterior obtained from $5\\cdot10^4$ samples, and density function of the target posterior. "</span></span>
<span id="cb27-1762"><a href="#cb27-1762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1763"><a href="#cb27-1763" aria-hidden="true" tabindex="-1"></a><span class="in">interv = np.linspace(0.01,10,10000)</span></span>
<span id="cb27-1764"><a href="#cb27-1764" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1765"><a href="#cb27-1765" aria-hidden="true" tabindex="-1"></a><span class="in">plt.hist(theta_post, density=True, bins=500, label=r"Fitted posterior", alpha=0.4)</span></span>
<span id="cb27-1766"><a href="#cb27-1766" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(interv, target_post_values, label="Target posterior",color=vertCEA)</span></span>
<span id="cb27-1767"><a href="#cb27-1767" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel(r"$\theta$")</span></span>
<span id="cb27-1768"><a href="#cb27-1768" aria-hidden="true" tabindex="-1"></a><span class="in">plt.legend(fontsize=12)</span></span>
<span id="cb27-1769"><a href="#cb27-1769" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1770"><a href="#cb27-1770" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlim(-0.1,6)</span></span>
<span id="cb27-1771"><a href="#cb27-1771" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb27-1772"><a href="#cb27-1772" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1773"><a href="#cb27-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1774"><a href="#cb27-1774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1775"><a href="#cb27-1775" aria-hidden="true" tabindex="-1"></a>As shown in @fig-normal_post_constr, the parametric posterior has a shape similar to the theoretical distribution. </span>
<span id="cb27-1776"><a href="#cb27-1776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1777"><a href="#cb27-1777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1778"><a href="#cb27-1778" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probit model and robustness</span></span>
<span id="cb27-1779"><a href="#cb27-1779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1780"><a href="#cb27-1780" aria-hidden="true" tabindex="-1"></a>As mentioned in @sec-probit_model regarding the probit model, we present several additional results.</span>
<span id="cb27-1781"><a href="#cb27-1781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1782"><a href="#cb27-1782" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1783"><a href="#cb27-1783" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-quad_error_post</span></span>
<span id="cb27-1784"><a href="#cb27-1784" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Mean norm difference as a function of the size $N$ of the dataset for the unconstrained fitted posterior and the Jeffreys posterior. For each value of $N$, 10 different datasets are considered from which we obtain 95\\% confidence intervals."</span></span>
<span id="cb27-1785"><a href="#cb27-1785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1786"><a href="#cb27-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1787"><a href="#cb27-1787" aria-hidden="true" tabindex="-1"></a><span class="in">num_M = 10</span></span>
<span id="cb27-1788"><a href="#cb27-1788" aria-hidden="true" tabindex="-1"></a><span class="in">N_max = 200</span></span>
<span id="cb27-1789"><a href="#cb27-1789" aria-hidden="true" tabindex="-1"></a><span class="in">N_init = 5</span></span>
<span id="cb27-1790"><a href="#cb27-1790" aria-hidden="true" tabindex="-1"></a><span class="in">tab_N = np.arange(N_init, N_max+N_init, 5)</span></span>
<span id="cb27-1791"><a href="#cb27-1791" aria-hidden="true" tabindex="-1"></a><span class="in">QE = np.zeros((len(tab_N), num_M))</span></span>
<span id="cb27-1792"><a href="#cb27-1792" aria-hidden="true" tabindex="-1"></a><span class="in">file_name = os.path.join(varp_probit_path, 'Quadratic_errors/Quad_error_post')</span></span>
<span id="cb27-1793"><a href="#cb27-1793" aria-hidden="true" tabindex="-1"></a><span class="in">first_file = file_name + '1.pkl'</span></span>
<span id="cb27-1794"><a href="#cb27-1794" aria-hidden="true" tabindex="-1"></a><span class="in">for M in range(num_M):</span></span>
<span id="cb27-1795"><a href="#cb27-1795" aria-hidden="true" tabindex="-1"></a><span class="in">    file = file_name + f'{M+1}.pkl'</span></span>
<span id="cb27-1796"><a href="#cb27-1796" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file, 'rb') as file:</span></span>
<span id="cb27-1797"><a href="#cb27-1797" aria-hidden="true" tabindex="-1"></a><span class="in">        QE[:,M] = pickle.load(file)</span></span>
<span id="cb27-1798"><a href="#cb27-1798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1799"><a href="#cb27-1799" aria-hidden="true" tabindex="-1"></a><span class="in">mean_vals = np.mean(QE, axis=1)</span></span>
<span id="cb27-1800"><a href="#cb27-1800" aria-hidden="true" tabindex="-1"></a><span class="in">lower_quantile = np.quantile(QE, 0.025, axis=1)</span></span>
<span id="cb27-1801"><a href="#cb27-1801" aria-hidden="true" tabindex="-1"></a><span class="in">upper_quantile = np.quantile(QE, 0.975, axis=1)</span></span>
<span id="cb27-1802"><a href="#cb27-1802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1803"><a href="#cb27-1803" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(1)</span></span>
<span id="cb27-1804"><a href="#cb27-1804" aria-hidden="true" tabindex="-1"></a><span class="in">theta_vrai = np.array([3.37610525, 0.43304097])</span></span>
<span id="cb27-1805"><a href="#cb27-1805" aria-hidden="true" tabindex="-1"></a><span class="in">N_max = 200</span></span>
<span id="cb27-1806"><a href="#cb27-1806" aria-hidden="true" tabindex="-1"></a><span class="in">N_init = 5</span></span>
<span id="cb27-1807"><a href="#cb27-1807" aria-hidden="true" tabindex="-1"></a><span class="in">tab_N2 = np.arange(N_init, N_max+N_init, 5)</span></span>
<span id="cb27-1808"><a href="#cb27-1808" aria-hidden="true" tabindex="-1"></a><span class="in">num_MC = 5000</span></span>
<span id="cb27-1809"><a href="#cb27-1809" aria-hidden="true" tabindex="-1"></a><span class="in">M = 10</span></span>
<span id="cb27-1810"><a href="#cb27-1810" aria-hidden="true" tabindex="-1"></a><span class="in">num_MC = 5000</span></span>
<span id="cb27-1811"><a href="#cb27-1811" aria-hidden="true" tabindex="-1"></a><span class="in">err_jeff = np.zeros((len(tab_N2), M))</span></span>
<span id="cb27-1812"><a href="#cb27-1812" aria-hidden="true" tabindex="-1"></a><span class="in">j = 0</span></span>
<span id="cb27-1813"><a href="#cb27-1813" aria-hidden="true" tabindex="-1"></a><span class="in">for N in tab_N2 : </span></span>
<span id="cb27-1814"><a href="#cb27-1814" aria-hidden="true" tabindex="-1"></a><span class="in">    resultats_theta_post_N = np.zeros((M, num_MC, 2))</span></span>
<span id="cb27-1815"><a href="#cb27-1815" aria-hidden="true" tabindex="-1"></a><span class="in">    for i in tqdm(range(M), desc=f'Iterations for N={N}', disable=True): </span></span>
<span id="cb27-1816"><a href="#cb27-1816" aria-hidden="true" tabindex="-1"></a><span class="in">        file_path = os.path.join(int_jeffreys_path, f'model_J_{i}')</span></span>
<span id="cb27-1817"><a href="#cb27-1817" aria-hidden="true" tabindex="-1"></a><span class="in">        with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1818"><a href="#cb27-1818" aria-hidden="true" tabindex="-1"></a><span class="in">            Jeffreys = pickle.load(file)</span></span>
<span id="cb27-1819"><a href="#cb27-1819" aria-hidden="true" tabindex="-1"></a><span class="in">        resultats_theta_post_N[i] = Jeffreys['logs']['post'][N]</span></span>
<span id="cb27-1820"><a href="#cb27-1820" aria-hidden="true" tabindex="-1"></a><span class="in">        err_jeff[j, i] = np.mean(np.linalg.norm(resultats_theta_post_N[i] - theta_vrai[np.newaxis], axis=1))</span></span>
<span id="cb27-1821"><a href="#cb27-1821" aria-hidden="true" tabindex="-1"></a><span class="in">    j = j + 1</span></span>
<span id="cb27-1822"><a href="#cb27-1822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1823"><a href="#cb27-1823" aria-hidden="true" tabindex="-1"></a><span class="in">mean_vals2 = np.mean(err_jeff, axis=1)</span></span>
<span id="cb27-1824"><a href="#cb27-1824" aria-hidden="true" tabindex="-1"></a><span class="in">lower_quantile2 = np.quantile(err_jeff, 0.025, axis=1)</span></span>
<span id="cb27-1825"><a href="#cb27-1825" aria-hidden="true" tabindex="-1"></a><span class="in">upper_quantile2 = np.quantile(err_jeff, 0.975, axis=1)</span></span>
<span id="cb27-1826"><a href="#cb27-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1827"><a href="#cb27-1827" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1828"><a href="#cb27-1828" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(tab_N, mean_vals, label='Mean Approx')</span></span>
<span id="cb27-1829"><a href="#cb27-1829" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(tab_N, lower_quantile, upper_quantile, color='b', alpha=0.2, label='95% CI Approx')</span></span>
<span id="cb27-1830"><a href="#cb27-1830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1831"><a href="#cb27-1831" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(tab_N2, mean_vals2, label='Mean Jeffreys')</span></span>
<span id="cb27-1832"><a href="#cb27-1832" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color='orange', alpha=0.2, label='95% CI Jeffreys')</span></span>
<span id="cb27-1833"><a href="#cb27-1833" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel('N')</span></span>
<span id="cb27-1834"><a href="#cb27-1834" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel('Mean Norm Difference')</span></span>
<span id="cb27-1835"><a href="#cb27-1835" aria-hidden="true" tabindex="-1"></a><span class="in">plt.legend(fontsize=12)</span></span>
<span id="cb27-1836"><a href="#cb27-1836" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1837"><a href="#cb27-1837" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1838"><a href="#cb27-1838" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1839"><a href="#cb27-1839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1840"><a href="#cb27-1840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1841"><a href="#cb27-1841" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1842"><a href="#cb27-1842" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-quad_error_post_constr</span></span>
<span id="cb27-1843"><a href="#cb27-1843" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Mean norm difference as a function of the size $N$ of the dataset for the constrained fitted posterior and the Jeffreys posterior. For each value of $N$, 10 different datasets are considered from which we obtain 95\\% confidence intervals."</span></span>
<span id="cb27-1844"><a href="#cb27-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1845"><a href="#cb27-1845" aria-hidden="true" tabindex="-1"></a><span class="in">num_M = 10</span></span>
<span id="cb27-1846"><a href="#cb27-1846" aria-hidden="true" tabindex="-1"></a><span class="in">N_max = 200</span></span>
<span id="cb27-1847"><a href="#cb27-1847" aria-hidden="true" tabindex="-1"></a><span class="in">N_init = 5</span></span>
<span id="cb27-1848"><a href="#cb27-1848" aria-hidden="true" tabindex="-1"></a><span class="in">tab_N = np.arange(N_init, N_max+N_init, 5)</span></span>
<span id="cb27-1849"><a href="#cb27-1849" aria-hidden="true" tabindex="-1"></a><span class="in">QE = np.zeros((len(tab_N), num_M))</span></span>
<span id="cb27-1850"><a href="#cb27-1850" aria-hidden="true" tabindex="-1"></a><span class="in">file_name = os.path.join(varp_probit_path, 'Quadratic_errors/Quad_error_post_constr')</span></span>
<span id="cb27-1851"><a href="#cb27-1851" aria-hidden="true" tabindex="-1"></a><span class="in">first_file = file_name + '1.pkl'</span></span>
<span id="cb27-1852"><a href="#cb27-1852" aria-hidden="true" tabindex="-1"></a><span class="in">for M in range(num_M):</span></span>
<span id="cb27-1853"><a href="#cb27-1853" aria-hidden="true" tabindex="-1"></a><span class="in">    file = file_name + f'{M+1}.pkl'</span></span>
<span id="cb27-1854"><a href="#cb27-1854" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file, 'rb') as file:</span></span>
<span id="cb27-1855"><a href="#cb27-1855" aria-hidden="true" tabindex="-1"></a><span class="in">        QE[:,M] = pickle.load(file)</span></span>
<span id="cb27-1856"><a href="#cb27-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1857"><a href="#cb27-1857" aria-hidden="true" tabindex="-1"></a><span class="in">mean_vals = np.mean(QE, axis=1)</span></span>
<span id="cb27-1858"><a href="#cb27-1858" aria-hidden="true" tabindex="-1"></a><span class="in">lower_quantile = np.quantile(QE, 0.025, axis=1)</span></span>
<span id="cb27-1859"><a href="#cb27-1859" aria-hidden="true" tabindex="-1"></a><span class="in">upper_quantile = np.quantile(QE, 0.975, axis=1)</span></span>
<span id="cb27-1860"><a href="#cb27-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1861"><a href="#cb27-1861" aria-hidden="true" tabindex="-1"></a><span class="in">seed_all(1)</span></span>
<span id="cb27-1862"><a href="#cb27-1862" aria-hidden="true" tabindex="-1"></a><span class="in">theta_vrai = np.array([3.37610525, 0.43304097])</span></span>
<span id="cb27-1863"><a href="#cb27-1863" aria-hidden="true" tabindex="-1"></a><span class="in">N_max = 200</span></span>
<span id="cb27-1864"><a href="#cb27-1864" aria-hidden="true" tabindex="-1"></a><span class="in">N_init = 5</span></span>
<span id="cb27-1865"><a href="#cb27-1865" aria-hidden="true" tabindex="-1"></a><span class="in">tab_N2 = np.arange(N_init, N_max+N_init, 5)</span></span>
<span id="cb27-1866"><a href="#cb27-1866" aria-hidden="true" tabindex="-1"></a><span class="in">num_MC = 5000</span></span>
<span id="cb27-1867"><a href="#cb27-1867" aria-hidden="true" tabindex="-1"></a><span class="in">M = 10</span></span>
<span id="cb27-1868"><a href="#cb27-1868" aria-hidden="true" tabindex="-1"></a><span class="in">num_MC = 5000</span></span>
<span id="cb27-1869"><a href="#cb27-1869" aria-hidden="true" tabindex="-1"></a><span class="in">err_jeff = np.zeros((len(tab_N2), M))</span></span>
<span id="cb27-1870"><a href="#cb27-1870" aria-hidden="true" tabindex="-1"></a><span class="in">j = 0</span></span>
<span id="cb27-1871"><a href="#cb27-1871" aria-hidden="true" tabindex="-1"></a><span class="in">for N in tab_N2 : </span></span>
<span id="cb27-1872"><a href="#cb27-1872" aria-hidden="true" tabindex="-1"></a><span class="in">    resultats_theta_post_N = np.zeros((M, num_MC, 2))</span></span>
<span id="cb27-1873"><a href="#cb27-1873" aria-hidden="true" tabindex="-1"></a><span class="in">    for i in tqdm(range(M), desc=f'Iterations for N={N}', disable=True):</span></span>
<span id="cb27-1874"><a href="#cb27-1874" aria-hidden="true" tabindex="-1"></a><span class="in">        file_path = os.path.join(int_jeffreys_path, f'model_J_constraint_{i}') </span></span>
<span id="cb27-1875"><a href="#cb27-1875" aria-hidden="true" tabindex="-1"></a><span class="in">        with open(file_path, 'rb') as file:</span></span>
<span id="cb27-1876"><a href="#cb27-1876" aria-hidden="true" tabindex="-1"></a><span class="in">            Jeffreys = pickle.load(file)</span></span>
<span id="cb27-1877"><a href="#cb27-1877" aria-hidden="true" tabindex="-1"></a><span class="in">        resultats_theta_post_N[i] = Jeffreys['logs']['post'][N]</span></span>
<span id="cb27-1878"><a href="#cb27-1878" aria-hidden="true" tabindex="-1"></a><span class="in">        err_jeff[j, i] = np.mean(np.linalg.norm(resultats_theta_post_N[i] - theta_vrai[np.newaxis], axis=1))</span></span>
<span id="cb27-1879"><a href="#cb27-1879" aria-hidden="true" tabindex="-1"></a><span class="in">    j = j + 1</span></span>
<span id="cb27-1880"><a href="#cb27-1880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1881"><a href="#cb27-1881" aria-hidden="true" tabindex="-1"></a><span class="in">mean_vals2 = np.mean(err_jeff, axis=1)</span></span>
<span id="cb27-1882"><a href="#cb27-1882" aria-hidden="true" tabindex="-1"></a><span class="in">lower_quantile2 = np.quantile(err_jeff, 0.025, axis=1)</span></span>
<span id="cb27-1883"><a href="#cb27-1883" aria-hidden="true" tabindex="-1"></a><span class="in">upper_quantile2 = np.quantile(err_jeff, 0.975, axis=1)</span></span>
<span id="cb27-1884"><a href="#cb27-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1885"><a href="#cb27-1885" aria-hidden="true" tabindex="-1"></a><span class="in">plt.figure(figsize=(5, 3))</span></span>
<span id="cb27-1886"><a href="#cb27-1886" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(tab_N, mean_vals, label='Mean Approx')</span></span>
<span id="cb27-1887"><a href="#cb27-1887" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(tab_N, lower_quantile, upper_quantile, color='b', alpha=0.2, label='95% CI Approx')</span></span>
<span id="cb27-1888"><a href="#cb27-1888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1889"><a href="#cb27-1889" aria-hidden="true" tabindex="-1"></a><span class="in">plt.plot(tab_N2, mean_vals2, label='Mean Jeffreys')</span></span>
<span id="cb27-1890"><a href="#cb27-1890" aria-hidden="true" tabindex="-1"></a><span class="in">plt.fill_between(tab_N2, lower_quantile2, upper_quantile2, color='orange', alpha=0.2, label='95% CI Jeffreys')</span></span>
<span id="cb27-1891"><a href="#cb27-1891" aria-hidden="true" tabindex="-1"></a><span class="in">plt.xlabel('N')</span></span>
<span id="cb27-1892"><a href="#cb27-1892" aria-hidden="true" tabindex="-1"></a><span class="in">plt.ylabel('Mean Norm Difference')</span></span>
<span id="cb27-1893"><a href="#cb27-1893" aria-hidden="true" tabindex="-1"></a><span class="in">plt.legend(fontsize=12)</span></span>
<span id="cb27-1894"><a href="#cb27-1894" aria-hidden="true" tabindex="-1"></a><span class="in">plt.grid()</span></span>
<span id="cb27-1895"><a href="#cb27-1895" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1896"><a href="#cb27-1896" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1897"><a href="#cb27-1897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1898"><a href="#cb27-1898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1899"><a href="#cb27-1899" aria-hidden="true" tabindex="-1"></a>@fig-quad_error_post and @fig-quad_error_post_constr show the evolution of the posterior mean norm difference as the size $N$ of the dataset considered for the posterior distribution increases. For each value of $N$, 10 different datasets are used in order to quantify the variability of said error. The proportion of degenerate datasets is rather high when $N=5$ or $N=10$, the consequence is that the approximation tends to be more unstable. The main observation is that the error is decreasing in all cases when $N$ increases, also, the behaviour of the error for the fitted distributions on one hand, and the behaviour for the Jeffreys distribution on the other hand are quite similar in terms of mean value and confidence intervals.</span>
<span id="cb27-1900"><a href="#cb27-1900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1901"><a href="#cb27-1901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1902"><a href="#cb27-1902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1903"><a href="#cb27-1903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1904"><a href="#cb27-1904" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1905"><a href="#cb27-1905" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_ecdf</span></span>
<span id="cb27-1906"><a href="#cb27-1906" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Empirical cumulative distribution functions for the unconstrained fitted posterior and the Jeffreys posterior using $5000$ samples. The band is obtained by computing the ECDFs over $100$ different seeds and monitoring the maximum and minimum ECDF values for each $\\theta$."</span></span>
<span id="cb27-1907"><a href="#cb27-1907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1908"><a href="#cb27-1908" aria-hidden="true" tabindex="-1"></a><span class="in">n_exp = 100   # Number of experiments</span></span>
<span id="cb27-1909"><a href="#cb27-1909" aria-hidden="true" tabindex="-1"></a><span class="in">model = os.path.join(varp_probit_path, 'probit_samples/probit')</span></span>
<span id="cb27-1910"><a href="#cb27-1910" aria-hidden="true" tabindex="-1"></a><span class="in">q = 2</span></span>
<span id="cb27-1911"><a href="#cb27-1911" aria-hidden="true" tabindex="-1"></a><span class="in">first_file = model + '_seed_1.pkl' </span></span>
<span id="cb27-1912"><a href="#cb27-1912" aria-hidden="true" tabindex="-1"></a><span class="in"> </span></span>
<span id="cb27-1913"><a href="#cb27-1913" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**6</span></span>
<span id="cb27-1914"><a href="#cb27-1914" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_post = 5000</span></span>
<span id="cb27-1915"><a href="#cb27-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1916"><a href="#cb27-1916" aria-hidden="true" tabindex="-1"></a><span class="in"># Concatenate results from all experiments</span></span>
<span id="cb27-1917"><a href="#cb27-1917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1918"><a href="#cb27-1918" aria-hidden="true" tabindex="-1"></a><span class="in">theta_prior = np.zeros((n_exp,n_samples_prior,q))</span></span>
<span id="cb27-1919"><a href="#cb27-1919" aria-hidden="true" tabindex="-1"></a><span class="in">jeffreys_prior = np.zeros((n_exp,n_samples_prior,q))</span></span>
<span id="cb27-1920"><a href="#cb27-1920" aria-hidden="true" tabindex="-1"></a><span class="in">theta_post = np.zeros((n_exp,n_samples_post,q))</span></span>
<span id="cb27-1921"><a href="#cb27-1921" aria-hidden="true" tabindex="-1"></a><span class="in">jeffreys_post = np.zeros((n_exp,n_samples_post,q))</span></span>
<span id="cb27-1922"><a href="#cb27-1922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1923"><a href="#cb27-1923" aria-hidden="true" tabindex="-1"></a><span class="in">for k in range(n_exp):</span></span>
<span id="cb27-1924"><a href="#cb27-1924" aria-hidden="true" tabindex="-1"></a><span class="in">    file_name = model + f'_seed{k+1}.pkl'</span></span>
<span id="cb27-1925"><a href="#cb27-1925" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_name, 'rb') as file:</span></span>
<span id="cb27-1926"><a href="#cb27-1926" aria-hidden="true" tabindex="-1"></a><span class="in">        data = pickle.load(file)</span></span>
<span id="cb27-1927"><a href="#cb27-1927" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_prior[k,:,:] = data['prior']['VA'] </span></span>
<span id="cb27-1928"><a href="#cb27-1928" aria-hidden="true" tabindex="-1"></a><span class="in">        #jeffreys_prior[k,:,:] = data['prior']['Jeffreys']</span></span>
<span id="cb27-1929"><a href="#cb27-1929" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post[k,:,:] = data['post']['VA']</span></span>
<span id="cb27-1930"><a href="#cb27-1930" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_post[k,:,:] = data['post']['Jeffreys']</span></span>
<span id="cb27-1931"><a href="#cb27-1931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1932"><a href="#cb27-1932" aria-hidden="true" tabindex="-1"></a><span class="in"># Post CDF computations </span></span>
<span id="cb27-1933"><a href="#cb27-1933" aria-hidden="true" tabindex="-1"></a><span class="in">num_x = 5000</span></span>
<span id="cb27-1934"><a href="#cb27-1934" aria-hidden="true" tabindex="-1"></a><span class="in">x_values_alpha = np.linspace(0., 8., num_x)</span></span>
<span id="cb27-1935"><a href="#cb27-1935" aria-hidden="true" tabindex="-1"></a><span class="in">x_values_beta = np.linspace(0., 2., num_x)</span></span>
<span id="cb27-1936"><a href="#cb27-1936" aria-hidden="true" tabindex="-1"></a><span class="in">cdf_post_alpha = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-1937"><a href="#cb27-1937" aria-hidden="true" tabindex="-1"></a><span class="in">cdf_post_beta = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-1938"><a href="#cb27-1938" aria-hidden="true" tabindex="-1"></a><span class="in">jeff_post_alpha = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-1939"><a href="#cb27-1939" aria-hidden="true" tabindex="-1"></a><span class="in">jeff_post_beta = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-1940"><a href="#cb27-1940" aria-hidden="true" tabindex="-1"></a><span class="in">for k in range(n_exp):</span></span>
<span id="cb27-1941"><a href="#cb27-1941" aria-hidden="true" tabindex="-1"></a><span class="in">    cdf_post_alpha[k,:] = compute_ecdf(theta_post[k,:,0], x_values_alpha)</span></span>
<span id="cb27-1942"><a href="#cb27-1942" aria-hidden="true" tabindex="-1"></a><span class="in">    cdf_post_beta[k,:] = compute_ecdf(theta_post[k,:,1], x_values_beta)</span></span>
<span id="cb27-1943"><a href="#cb27-1943" aria-hidden="true" tabindex="-1"></a><span class="in">    jeff_post_alpha[k,:] = compute_ecdf(jeffreys_post[k,:,0], x_values_alpha)</span></span>
<span id="cb27-1944"><a href="#cb27-1944" aria-hidden="true" tabindex="-1"></a><span class="in">    jeff_post_beta[k,:] = compute_ecdf(jeffreys_post[k,:,1], x_values_beta)</span></span>
<span id="cb27-1945"><a href="#cb27-1945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1946"><a href="#cb27-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1947"><a href="#cb27-1947" aria-hidden="true" tabindex="-1"></a><span class="in">lower_bound_alpha = np.quantile(cdf_post_alpha, 0., axis=0)</span></span>
<span id="cb27-1948"><a href="#cb27-1948" aria-hidden="true" tabindex="-1"></a><span class="in">upper_bound_alpha = np.quantile(cdf_post_alpha, 1.0, axis=0)</span></span>
<span id="cb27-1949"><a href="#cb27-1949" aria-hidden="true" tabindex="-1"></a><span class="in">median_cdf_alpha = np.quantile(cdf_post_alpha, 0.5, axis=0)</span></span>
<span id="cb27-1950"><a href="#cb27-1950" aria-hidden="true" tabindex="-1"></a><span class="in">median_jeff_alpha = np.quantile(jeff_post_alpha, 0.5, axis=0)</span></span>
<span id="cb27-1951"><a href="#cb27-1951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1952"><a href="#cb27-1952" aria-hidden="true" tabindex="-1"></a><span class="in">lower_bound_beta = np.quantile(cdf_post_beta, 0., axis=0)</span></span>
<span id="cb27-1953"><a href="#cb27-1953" aria-hidden="true" tabindex="-1"></a><span class="in">upper_bound_beta = np.quantile(cdf_post_beta, 1., axis=0)</span></span>
<span id="cb27-1954"><a href="#cb27-1954" aria-hidden="true" tabindex="-1"></a><span class="in">median_cdf_beta = np.quantile(cdf_post_beta, 0.5, axis=0)</span></span>
<span id="cb27-1955"><a href="#cb27-1955" aria-hidden="true" tabindex="-1"></a><span class="in">median_jeff_beta = np.quantile(jeff_post_beta, 0.5, axis=0)</span></span>
<span id="cb27-1956"><a href="#cb27-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1957"><a href="#cb27-1957" aria-hidden="true" tabindex="-1"></a><span class="in"># Create subplots for alpha and beta</span></span>
<span id="cb27-1958"><a href="#cb27-1958" aria-hidden="true" tabindex="-1"></a><span class="in">fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))</span></span>
<span id="cb27-1959"><a href="#cb27-1959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1960"><a href="#cb27-1960" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot for alpha</span></span>
<span id="cb27-1961"><a href="#cb27-1961" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.plot(x_values_alpha, median_cdf_alpha, label='Median fitted ECDF', color='blue', linewidth=2)</span></span>
<span id="cb27-1962"><a href="#cb27-1962" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.plot(x_values_alpha, median_jeff_alpha, label='Jeffreys ECDF', color='green', linewidth=2) </span></span>
<span id="cb27-1963"><a href="#cb27-1963" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color='lightblue', alpha=0.5, label='ECDF Band')</span></span>
<span id="cb27-1964"><a href="#cb27-1964" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.set_xlabel(r'$\theta_1$')</span></span>
<span id="cb27-1965"><a href="#cb27-1965" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.set_ylabel('CDF')</span></span>
<span id="cb27-1966"><a href="#cb27-1966" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.grid()</span></span>
<span id="cb27-1967"><a href="#cb27-1967" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.legend(loc='lower right', fontsize=12)</span></span>
<span id="cb27-1968"><a href="#cb27-1968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1969"><a href="#cb27-1969" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot for beta</span></span>
<span id="cb27-1970"><a href="#cb27-1970" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.plot(x_values_beta, median_cdf_beta, label='Median fitted ECDF', color='blue', linewidth=2)</span></span>
<span id="cb27-1971"><a href="#cb27-1971" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.plot(x_values_beta, median_jeff_beta, label='Jeffreys ECDF', color='green', linewidth=2)  </span></span>
<span id="cb27-1972"><a href="#cb27-1972" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color='lightblue', alpha=0.5, label='ECDF Band')</span></span>
<span id="cb27-1973"><a href="#cb27-1973" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.set_xlabel(r'$\theta_2$')</span></span>
<span id="cb27-1974"><a href="#cb27-1974" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.set_ylabel('CDF')</span></span>
<span id="cb27-1975"><a href="#cb27-1975" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.grid()</span></span>
<span id="cb27-1976"><a href="#cb27-1976" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.legend(loc='lower right', fontsize=12)</span></span>
<span id="cb27-1977"><a href="#cb27-1977" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()  </span></span>
<span id="cb27-1978"><a href="#cb27-1978" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-1979"><a href="#cb27-1979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1980"><a href="#cb27-1980" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-1981"><a href="#cb27-1981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1982"><a href="#cb27-1982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1983"><a href="#cb27-1983" aria-hidden="true" tabindex="-1"></a><span class="in">```{python stem-plot}</span></span>
<span id="cb27-1984"><a href="#cb27-1984" aria-hidden="true" tabindex="-1"></a><span class="in">#| label: fig-probit_ecdf_constr</span></span>
<span id="cb27-1985"><a href="#cb27-1985" aria-hidden="true" tabindex="-1"></a><span class="in">#| fig-cap: "Empirical cumulative distribution functions for the constrained fitted posterior and the Jeffreys posterior using $5000$ samples. The band is obtained by computing the ECDFs over $100$ different seeds and monitoring the maximum and minimum ECDF values for each $\\theta$."</span></span>
<span id="cb27-1986"><a href="#cb27-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1987"><a href="#cb27-1987" aria-hidden="true" tabindex="-1"></a><span class="in">n_exp = 100   # Number of experiments</span></span>
<span id="cb27-1988"><a href="#cb27-1988" aria-hidden="true" tabindex="-1"></a><span class="in">model = os.path.join(varp_probit_path, 'probit_samples/probit_constr')</span></span>
<span id="cb27-1989"><a href="#cb27-1989" aria-hidden="true" tabindex="-1"></a><span class="in">q = 2</span></span>
<span id="cb27-1990"><a href="#cb27-1990" aria-hidden="true" tabindex="-1"></a><span class="in">first_file = model + '_seed1.pkl' </span></span>
<span id="cb27-1991"><a href="#cb27-1991" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_prior = 10**6</span></span>
<span id="cb27-1992"><a href="#cb27-1992" aria-hidden="true" tabindex="-1"></a><span class="in">n_samples_post = 5000</span></span>
<span id="cb27-1993"><a href="#cb27-1993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1994"><a href="#cb27-1994" aria-hidden="true" tabindex="-1"></a><span class="in"># Concatenate results from all experiments</span></span>
<span id="cb27-1995"><a href="#cb27-1995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-1996"><a href="#cb27-1996" aria-hidden="true" tabindex="-1"></a><span class="in">theta_prior = np.zeros((n_exp,n_samples_prior,q))</span></span>
<span id="cb27-1997"><a href="#cb27-1997" aria-hidden="true" tabindex="-1"></a><span class="in">jeffreys_prior = np.zeros((n_exp,n_samples_prior,q))</span></span>
<span id="cb27-1998"><a href="#cb27-1998" aria-hidden="true" tabindex="-1"></a><span class="in">theta_post = np.zeros((n_exp,n_samples_post,q))</span></span>
<span id="cb27-1999"><a href="#cb27-1999" aria-hidden="true" tabindex="-1"></a><span class="in">jeffreys_post = np.zeros((n_exp,n_samples_post,q))</span></span>
<span id="cb27-2000"><a href="#cb27-2000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2001"><a href="#cb27-2001" aria-hidden="true" tabindex="-1"></a><span class="in">for k in range(n_exp):</span></span>
<span id="cb27-2002"><a href="#cb27-2002" aria-hidden="true" tabindex="-1"></a><span class="in">    file_name = model + f'_seed{k+1}.pkl'</span></span>
<span id="cb27-2003"><a href="#cb27-2003" aria-hidden="true" tabindex="-1"></a><span class="in">    #file_name = model + f'_seed_lognormal{k+1}.pkl'</span></span>
<span id="cb27-2004"><a href="#cb27-2004" aria-hidden="true" tabindex="-1"></a><span class="in">    with open(file_name, 'rb') as file:</span></span>
<span id="cb27-2005"><a href="#cb27-2005" aria-hidden="true" tabindex="-1"></a><span class="in">        data = pickle.load(file)</span></span>
<span id="cb27-2006"><a href="#cb27-2006" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_prior[k,:,:] = data['prior']['VA'] </span></span>
<span id="cb27-2007"><a href="#cb27-2007" aria-hidden="true" tabindex="-1"></a><span class="in">        #jeffreys_prior[k,:,:] = data['prior']['Jeffreys']</span></span>
<span id="cb27-2008"><a href="#cb27-2008" aria-hidden="true" tabindex="-1"></a><span class="in">        theta_post[k,:,:] = data['post']['VA']</span></span>
<span id="cb27-2009"><a href="#cb27-2009" aria-hidden="true" tabindex="-1"></a><span class="in">        jeffreys_post[k,:,:] = data['post']['Jeffreys']</span></span>
<span id="cb27-2010"><a href="#cb27-2010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2011"><a href="#cb27-2011" aria-hidden="true" tabindex="-1"></a><span class="in"># Post CDF computations </span></span>
<span id="cb27-2012"><a href="#cb27-2012" aria-hidden="true" tabindex="-1"></a><span class="in">num_x = 5000</span></span>
<span id="cb27-2013"><a href="#cb27-2013" aria-hidden="true" tabindex="-1"></a><span class="in">x_values_alpha = np.linspace(0., 8., num_x)</span></span>
<span id="cb27-2014"><a href="#cb27-2014" aria-hidden="true" tabindex="-1"></a><span class="in">x_values_beta = np.linspace(0., 2., num_x)</span></span>
<span id="cb27-2015"><a href="#cb27-2015" aria-hidden="true" tabindex="-1"></a><span class="in">cdf_post_alpha = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-2016"><a href="#cb27-2016" aria-hidden="true" tabindex="-1"></a><span class="in">cdf_post_beta = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-2017"><a href="#cb27-2017" aria-hidden="true" tabindex="-1"></a><span class="in">jeff_post_alpha = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-2018"><a href="#cb27-2018" aria-hidden="true" tabindex="-1"></a><span class="in">jeff_post_beta = np.zeros((n_exp, num_x))</span></span>
<span id="cb27-2019"><a href="#cb27-2019" aria-hidden="true" tabindex="-1"></a><span class="in">for k in range(n_exp):</span></span>
<span id="cb27-2020"><a href="#cb27-2020" aria-hidden="true" tabindex="-1"></a><span class="in">    cdf_post_alpha[k,:] = compute_ecdf(theta_post[k,:,0], x_values_alpha)</span></span>
<span id="cb27-2021"><a href="#cb27-2021" aria-hidden="true" tabindex="-1"></a><span class="in">    cdf_post_beta[k,:] = compute_ecdf(theta_post[k,:,1], x_values_beta)</span></span>
<span id="cb27-2022"><a href="#cb27-2022" aria-hidden="true" tabindex="-1"></a><span class="in">    jeff_post_alpha[k,:] = compute_ecdf(jeffreys_post[k,:,0], x_values_alpha)</span></span>
<span id="cb27-2023"><a href="#cb27-2023" aria-hidden="true" tabindex="-1"></a><span class="in">    jeff_post_beta[k,:] = compute_ecdf(jeffreys_post[k,:,1], x_values_beta)</span></span>
<span id="cb27-2024"><a href="#cb27-2024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2025"><a href="#cb27-2025" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2026"><a href="#cb27-2026" aria-hidden="true" tabindex="-1"></a><span class="in">lower_bound_alpha = np.quantile(cdf_post_alpha, 0., axis=0)</span></span>
<span id="cb27-2027"><a href="#cb27-2027" aria-hidden="true" tabindex="-1"></a><span class="in">upper_bound_alpha = np.quantile(cdf_post_alpha, 1.0, axis=0)</span></span>
<span id="cb27-2028"><a href="#cb27-2028" aria-hidden="true" tabindex="-1"></a><span class="in">median_cdf_alpha = np.quantile(cdf_post_alpha, 0.5, axis=0)</span></span>
<span id="cb27-2029"><a href="#cb27-2029" aria-hidden="true" tabindex="-1"></a><span class="in">median_jeff_alpha = np.quantile(jeff_post_alpha, 0.5, axis=0)</span></span>
<span id="cb27-2030"><a href="#cb27-2030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2031"><a href="#cb27-2031" aria-hidden="true" tabindex="-1"></a><span class="in">lower_bound_beta = np.quantile(cdf_post_beta, 0., axis=0)</span></span>
<span id="cb27-2032"><a href="#cb27-2032" aria-hidden="true" tabindex="-1"></a><span class="in">upper_bound_beta = np.quantile(cdf_post_beta, 1.0, axis=0)</span></span>
<span id="cb27-2033"><a href="#cb27-2033" aria-hidden="true" tabindex="-1"></a><span class="in">median_cdf_beta = np.quantile(cdf_post_beta, 0.5, axis=0)</span></span>
<span id="cb27-2034"><a href="#cb27-2034" aria-hidden="true" tabindex="-1"></a><span class="in">median_jeff_beta = np.quantile(jeff_post_beta, 0.5, axis=0)</span></span>
<span id="cb27-2035"><a href="#cb27-2035" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2036"><a href="#cb27-2036" aria-hidden="true" tabindex="-1"></a><span class="in"># Create subplots for alpha and beta</span></span>
<span id="cb27-2037"><a href="#cb27-2037" aria-hidden="true" tabindex="-1"></a><span class="in">fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))</span></span>
<span id="cb27-2038"><a href="#cb27-2038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2039"><a href="#cb27-2039" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot for alpha</span></span>
<span id="cb27-2040"><a href="#cb27-2040" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.plot(x_values_alpha, median_cdf_alpha, label='Median fitted ECDF', color='blue', linewidth=2)</span></span>
<span id="cb27-2041"><a href="#cb27-2041" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.plot(x_values_alpha, median_jeff_alpha, label='Jeffreys ECDF', color='green', linewidth=2) </span></span>
<span id="cb27-2042"><a href="#cb27-2042" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.fill_between(x_values_alpha, lower_bound_alpha, upper_bound_alpha, color='lightblue', alpha=0.5, label='ECDF Band')</span></span>
<span id="cb27-2043"><a href="#cb27-2043" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.set_xlabel(r'$\theta_1$')</span></span>
<span id="cb27-2044"><a href="#cb27-2044" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.set_ylabel('CDF')</span></span>
<span id="cb27-2045"><a href="#cb27-2045" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.grid()</span></span>
<span id="cb27-2046"><a href="#cb27-2046" aria-hidden="true" tabindex="-1"></a><span class="in">ax1.legend(loc='lower right', fontsize=12)</span></span>
<span id="cb27-2047"><a href="#cb27-2047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2048"><a href="#cb27-2048" aria-hidden="true" tabindex="-1"></a><span class="in"># Plot for beta</span></span>
<span id="cb27-2049"><a href="#cb27-2049" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.plot(x_values_beta, median_cdf_beta, label='Median fitted ECDF', color='blue', linewidth=2)</span></span>
<span id="cb27-2050"><a href="#cb27-2050" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.plot(x_values_beta, median_jeff_beta, label='Jeffreys ECDF', color='green', linewidth=2)  </span></span>
<span id="cb27-2051"><a href="#cb27-2051" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.fill_between(x_values_beta, lower_bound_beta, upper_bound_beta, color='lightblue', alpha=0.5, label='ECDF Band')</span></span>
<span id="cb27-2052"><a href="#cb27-2052" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.set_xlabel(r'$\theta_2$')</span></span>
<span id="cb27-2053"><a href="#cb27-2053" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.set_ylabel('CDF')</span></span>
<span id="cb27-2054"><a href="#cb27-2054" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.grid()</span></span>
<span id="cb27-2055"><a href="#cb27-2055" aria-hidden="true" tabindex="-1"></a><span class="in">ax2.legend(loc='lower right', fontsize=12)</span></span>
<span id="cb27-2056"><a href="#cb27-2056" aria-hidden="true" tabindex="-1"></a><span class="in">plt.tight_layout()  </span></span>
<span id="cb27-2057"><a href="#cb27-2057" aria-hidden="true" tabindex="-1"></a><span class="in">plt.show()</span></span>
<span id="cb27-2058"><a href="#cb27-2058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2059"><a href="#cb27-2059" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb27-2060"><a href="#cb27-2060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2061"><a href="#cb27-2061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2062"><a href="#cb27-2062" aria-hidden="true" tabindex="-1"></a>@fig-probit_ecdf and @fig-probit_ecdf_constr compare the empirical distribution functions of the fitted posterior and the Jeffreys posterior. In the unconstrained case, one can observe that the ECDFs are very close for $\theta_1$, whereas the variability is slightly higher for $\theta_2$ although still reasonable. When imposing a constraint on $\theta_2$, one remarks that the variability of the result is higher. The Jeffreys ECDF is contained in the band when $\theta_2$ is close to zero, but not when $\theta_2$ increases ($\theta_2 &gt; 0.5$). This is coherent with the previous scatter histograms where the Jeffreys posterior on $\theta_2$ tends to have a heavier tail than the variational approximation.</span>
<span id="cb27-2063"><a href="#cb27-2063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2064"><a href="#cb27-2064" aria-hidden="true" tabindex="-1"></a>Altogether, despite the stochastic nature of the developed algorithm, we consider that the result tends to be reasonably robust to the RNG seed for the optimization part, and robust to the dataset used for the posterior distribution for the MCMC part.</span>
<span id="cb27-2065"><a href="#cb27-2065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2066"><a href="#cb27-2066" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.unnumbered}</span></span>
<span id="cb27-2067"><a href="#cb27-2067" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-2068"><a href="#cb27-2068" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb27-2069"><a href="#cb27-2069" aria-hidden="true" tabindex="-1"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>